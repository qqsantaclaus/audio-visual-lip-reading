{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import sys\n",
    "# import skvideo.io\n",
    "import json\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import glob\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import cmudict\n",
    "import tensorflow as tf\n",
    "import wer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES']='3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phoneme_list = [] \n",
    "phoneme_dict = {}\n",
    "\n",
    "with open(\"/n/fs/scratch/jiaqis/cmudict-master/cmudict.phones\", 'r') as fp:\n",
    "    i = 0\n",
    "    line = fp.readline()\n",
    "    while line:\n",
    "        phoneme = line.split()[0].strip()\n",
    "        phoneme_property = line.split()[1].strip()\n",
    "        phoneme_list.append((phoneme, phoneme_property))\n",
    "        phoneme_dict[phoneme] = i+3\n",
    "        line = fp.readline()\n",
    "        i=i+1\n",
    "        \n",
    "phoneme_dict['START'] = 0\n",
    "phoneme_dict[\"END\"] = 1\n",
    "phoneme_dict[\"UNK\"] = 2\n",
    "print(phoneme_list, phoneme_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pron_dict = cmudict.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"/n/fs/scratch/jiaqis/LRS3-TED/\"\n",
    "SAVE_DIR = \"/n/fs/scratch/jiaqis/LRS3-TED-Extracted/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ID_list = json.load(open('test_ID_list.json', \"r\"))\n",
    "trainval_ID_list = json.load(open('trainval_ID_list.json', \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import layers\n",
    "\n",
    "\n",
    "GO_TOKEN = 0\n",
    "END_TOKEN = 1\n",
    "UNK_TOKEN = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_pron(pron):\n",
    "    \"\"\"Remove stress from pronunciations.\"\"\"\n",
    "    return re.sub(r\"\\d\", \"\", pron)\n",
    "\n",
    "def make_triphones(pron):\n",
    "    \"\"\"Output triphones from a word's pronunciation.\"\"\"\n",
    "    if len(pron) < 3:\n",
    "        return []\n",
    "    # Junk on end is to make word boundaries work\n",
    "    return ([((pron[idx - 2], pron[idx - 1]), pron[idx])\n",
    "             for idx in range(2, len(pron))] + [(('#', '#'), pron[0])] +\n",
    "            [((pron[-2], pron[-1]), '#')])\n",
    "                                                \n",
    "def triphone_probs(prons):\n",
    "    \"\"\"Calculate triphone probabilities for pronunciations.\"\"\"\n",
    "    context_counts = defaultdict(lambda: defaultdict(int))\n",
    "    for pron in prons:\n",
    "        for (context, phoneme) in make_triphones(pron):\n",
    "            context_counts[context][phoneme] += 1\n",
    "            \n",
    "    for (context, outcomes) in context_counts.items():\n",
    "        total_outcomes = sum(outcomes.values())\n",
    "        for outcome, count in outcomes.items():\n",
    "            context_counts[context][outcome] = float(count) / total_outcomes\n",
    "        \n",
    "    return context_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(filepath, img_size, keypoint_img_size, keypoint_size):\n",
    "    # images\n",
    "    # frames x rows x cols x channels\n",
    "    visual_cube = []\n",
    "    # keypoint features\n",
    "    feature_cube = []\n",
    "    features = json.load(open(filepath + \".json\", 'r'))\n",
    "    # Target Text/phonemes\n",
    "    labels = []\n",
    "    text = open(filepath+\".txt\", 'r').readline()\n",
    "    words = text[5:].lower().strip().split()\n",
    "    for word in words:\n",
    "        word_phonemes = pron_dict[word][0]\n",
    "        word_indices = [phoneme_dict[clean_pron(phon)] for phon in word_phonemes]\n",
    "        labels.extend(word_indices)\n",
    "        \n",
    "    acc = 0\n",
    "    for imgFilename in sorted(glob.glob(filepath + \"_*_mouth.jpg\")): # \n",
    "#         if 'mouth' in imgFilename: # \n",
    "#             continue               #\n",
    "        x = image.img_to_array(\n",
    "              image.load_img(imgFilename, target_size=img_size))/255.0\n",
    "#         x = np.expand_dims(x, axis=0)\n",
    "#         x = preprocess_input(x)\n",
    "        visual_cube.append(x)\n",
    "        \n",
    "        mask = np.zeros((keypoint_img_size[0], keypoint_img_size[1], keypoint_size))\n",
    "        framenum = str(int(imgFilename.split(\"_\")[-2].split(\".\")[0])) # \n",
    "        f_feature = features[framenum]['mouthCoords']\n",
    "        for ft_index in range(keypoint_size):\n",
    "            # TODO: check range of outputs\n",
    "            keypoint_x = min(f_feature[ft_index][0] - 1, 223)\n",
    "            keypoint_y = min(f_feature[ft_index][1] - 1, 223)\n",
    "            mask[keypoint_y, keypoint_x, ft_index] = 1.0\n",
    "        feature_cube.append(mask)\n",
    "        acc+=1\n",
    "    return np.array(visual_cube), np.array(feature_cube), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_wise_op(inputs, operation, **kwargs):\n",
    "    # batch x timesteps x w x h x c\n",
    "    # or\n",
    "    # batch x timesteps x c\n",
    "    inputs_flat = tf.reshape(inputs, tf.concat([tf.constant(-1, shape=(1,)), \n",
    "                                                tf.shape(inputs)[2:]], axis=0))\n",
    "    print(inputs, inputs_flat)\n",
    "    outputs_flat = operation(inputs_flat, **kwargs)\n",
    "\n",
    "    output_shape = tf.concat([tf.shape(inputs)[:2], tf.shape(outputs_flat)[1:]], 0)\n",
    "    outputs = tf.reshape(outputs_flat, output_shape)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq(mode, features, labels, params):\n",
    "    is_training = True\n",
    "    \n",
    "    vocab_size = params['vocab_size']\n",
    "    embed_dim = params['embed_dim']\n",
    "    num_units = params['num_units']\n",
    "    num_encoder_rnns = params['num_encoder_rnns']\n",
    "    num_decoder_rnns = params['num_decoder_rnns']\n",
    "    input_max_length = params['input_max_length']\n",
    "    output_max_length = params['output_max_length']\n",
    "    \n",
    "    inp = features['input']\n",
    "    output = features['output']\n",
    "    batch_size = tf.shape(inp)[0]\n",
    "    start_tokens = tf.zeros([batch_size], dtype=tf.int64)\n",
    "    print(inp, start_tokens)\n",
    "    train_output = tf.concat([tf.expand_dims(start_tokens, 1), output], 1)\n",
    "    # b x f x w x h x c\n",
    "    input_lengths = tf.reduce_sum(tf.to_int32(tf.reduce_sum(tf.to_int32(tf.not_equal(inp, 0.0)), \n",
    "                                                            axis=(2,3,4))>0), 1)\n",
    "    # b x seq\n",
    "    output_lengths = tf.reduce_sum(tf.to_int32(tf.not_equal(train_output, END_TOKEN)), 1)\n",
    "    print(train_output, input_lengths, output_lengths, output)\n",
    "\n",
    "    with tf.variable_scope('video_embed', reuse=tf.AUTO_REUSE):\n",
    "        # 224 x 224 x 64\n",
    "        conv1 = frame_wise_op(inp,\n",
    "                 operation=tf.layers.conv2d,\n",
    "                 filters=64,\n",
    "                 kernel_size=[3, 3],\n",
    "                 padding='same')\n",
    "        conv1 = tf.layers.batch_normalization(\n",
    "                                                conv1,\n",
    "                                                axis=-1,\n",
    "                                                training=is_training,\n",
    "                                                trainable=True\n",
    "                                            )\n",
    "        conv1 = tf.nn.relu(conv1)\n",
    "\n",
    "        # 112 x 112 x 64\n",
    "        down1 = frame_wise_op(conv1,\n",
    "                            operation=tf.layers.max_pooling2d,\n",
    "                            pool_size=[2, 2],\n",
    "                            strides=2)\n",
    "#         down1 = tf.layers.max_pooling3d(conv1,\n",
    "#                             pool_size=[2, 2, 2],\n",
    "#                             strides=2)\n",
    "        # 112 x 112 x 128\n",
    "        conv2 = frame_wise_op(down1,\n",
    "                     operation=tf.layers.conv2d,\n",
    "                     filters=128,\n",
    "                     kernel_size=[3, 3],\n",
    "                     padding='same',)\n",
    "        conv2 = tf.layers.batch_normalization(\n",
    "            conv2,\n",
    "            axis=-1,\n",
    "            training=is_training,\n",
    "            trainable=True\n",
    "        )\n",
    "        conv2 = tf.nn.relu(conv2)\n",
    "        \n",
    "        # 56 x 56 x 128\n",
    "        down2 = frame_wise_op(conv2,\n",
    "                            operation=tf.layers.max_pooling2d,\n",
    "                            pool_size=[2, 2],\n",
    "                            strides=2)\n",
    "        # 56 x 56 x 256\n",
    "        conv3 = frame_wise_op(down2,\n",
    "                     operation=tf.layers.conv2d,\n",
    "                     filters=256,\n",
    "                     kernel_size=[3, 3],\n",
    "                     padding='same')\n",
    "        conv3 = tf.layers.batch_normalization(\n",
    "            conv3,\n",
    "            axis=-1,\n",
    "            training=is_training,\n",
    "            trainable=True\n",
    "        )\n",
    "        conv3 = tf.nn.relu(conv3)\n",
    "        \n",
    "        # 28 x 28 x 256\n",
    "        down3 = frame_wise_op(conv3,\n",
    "                            operation=tf.layers.max_pooling2d,\n",
    "                            pool_size=[2, 2],\n",
    "                            strides=2)\n",
    "        # 28 x 28 x 256\n",
    "        conv4 = frame_wise_op(down3,\n",
    "                     operation=tf.layers.conv2d,\n",
    "                     filters=256,\n",
    "                     kernel_size=[3, 3],\n",
    "                     padding='same')\n",
    "        conv4 = tf.layers.batch_normalization(\n",
    "            conv4,\n",
    "            axis=-1,\n",
    "            training=is_training,\n",
    "            trainable=True\n",
    "        )\n",
    "        conv4 = tf.nn.relu(conv4)\n",
    "        \n",
    "        # 14 x 14 x 256\n",
    "        down4 = frame_wise_op(conv4,\n",
    "                            operation=tf.layers.max_pooling2d,\n",
    "                            pool_size=[2, 2],\n",
    "                            strides=2)\n",
    "        # 14 x 14 x 256\n",
    "        conv5 = frame_wise_op(down4,\n",
    "                     operation=tf.layers.conv2d,\n",
    "                     filters=256,\n",
    "                     kernel_size=[3, 3],\n",
    "                     padding='same')\n",
    "        conv5 = tf.layers.batch_normalization(\n",
    "            conv5,\n",
    "            axis=-1,\n",
    "            training=is_training,\n",
    "            trainable=True\n",
    "        )\n",
    "        conv5 = tf.nn.relu(conv5)\n",
    "        \n",
    "        # 7 x 7 x 256\n",
    "        down5 = frame_wise_op(conv5,\n",
    "                            operation=tf.layers.max_pooling2d,\n",
    "                            pool_size=[2, 2],\n",
    "                            strides=2)\n",
    "        # 256\n",
    "        down5_shape =down5.shape.as_list()\n",
    "        flattened = frame_wise_op(down5, \n",
    "                           operation=tf.reshape,\n",
    "                           shape=(-1, down5_shape[-1]*down5_shape[-2]*down5_shape[-3]))\n",
    "        fc6 = frame_wise_op(flattened, \n",
    "                           operation=tf.layers.dense,\n",
    "                           units=256,\n",
    "                           activation='relu')\n",
    "    \n",
    "    output_embed = layers.embed_sequence(\n",
    "        train_output, vocab_size=vocab_size, embed_dim=embed_dim, scope='embed')\n",
    "    with tf.variable_scope('embed', reuse=True):\n",
    "        embeddings = tf.get_variable('embeddings', dtype=tf.float32)\n",
    "\n",
    "    encoder_cells = [tf.contrib.rnn.GRUCell(num_units=num_units) for i in range(num_encoder_rnns)]\n",
    "    stacked_encoder_cell = tf.nn.rnn_cell.MultiRNNCell(encoder_cells)\n",
    "    \n",
    "    encoder_outputs, encoder_final_state = tf.nn.dynamic_rnn(stacked_encoder_cell, fc6, dtype=tf.float32)\n",
    "    print(encoder_outputs, encoder_final_state)\n",
    "    \n",
    "    tiled_context_vector = tf.tile(tf.reshape(encoder_final_state, (-1, 1, 256)),\n",
    "                                       multiples=[1,tf.shape(output_embed)[1],1]\n",
    "                                      )\n",
    "    print(embeddings, tiled_context_vector)\n",
    "    \n",
    "#     def re_embed(x):\n",
    "#         indices = tf.argmax(x, axis=-1)\n",
    "#         return tf.nn.embedding_lookup(embeddings, indices)\n",
    "    \n",
    "    train_helper = tf.contrib.seq2seq.TrainingHelper(output_embed, output_lengths)\n",
    "    \n",
    "    \n",
    "#     train_helper = tf.contrib.seq2seq.ScheduledOutputTrainingHelper(\n",
    "#         inputs = output_embed,\n",
    "#         sequence_length = output_lengths,\n",
    "#         next_inputs_fn = re_embed,\n",
    "#         auxiliary_inputs = tiled_context_vector, # condtional on inputs\n",
    "#         sampling_probability = 0.0, # for fullly inference\n",
    "#     )\n",
    "#     train_helper = tf.contrib.seq2seq.ScheduledEmbeddingTrainingHelper(\n",
    "#         output_embed, output_lengths, embeddings, 0.1\n",
    "#     )\n",
    "    \n",
    "#     def re_embed_inference(outputs):\n",
    "#         sample_ids = tf.argmax(x, axis=-1)\n",
    "#         finished = tf.equal(sample_ids, END_TOKEN)\n",
    "#         all_finished = tf.reduce_all(finished)\n",
    "#         next_inputs = tf.cond(\n",
    "#             all_finished,\n",
    "#             # If we're finished, the next_inputs value doesn't matter\n",
    "#             lambda: tf.nn.embedding_lookup(embeddings, start_tokens),\n",
    "#             lambda: tf.nn.embedding_lookup(embeddings, sample_ids))\n",
    "#         print(\"next_inputs:\", next_inputs)\n",
    "#         return tf.concat([tf.to_float32(next_inputs), encoder_final_state], axis=-1)\n",
    "    \n",
    "#     pred_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "#                         embeddings, start_tokens=tf.to_int32(start_tokens), end_token=1)\n",
    "    pred_helper = tf.contrib.seq2seq.SampleEmbeddingHelper(\n",
    "        embeddings,\n",
    "        start_tokens=tf.to_int32(start_tokens), end_token=1,\n",
    "    )\n",
    "#     pred_helper =tf.contrib.seq2seq.InferenceHelper(\n",
    "#                             sample_fn = (lambda outputs: outputs),\n",
    "#                             sample_shape = tf.TensorShape([vocab_size]),\n",
    "#                             sample_dtype = tf.float32,\n",
    "#                             start_inputs = tf.nn.embedding_lookup(embeddings, start_tokens),\n",
    "#                             end_fn = (lambda x: tf.argmax(x, axis=-1)==END_TOKEN),\n",
    "#                             next_inputs_fn = re_embed_inference,\n",
    "#                         )\n",
    "    print(\"here\")\n",
    "    beam_width = 2\n",
    "    \n",
    "    tiled_encoder_outputs = tf.contrib.seq2seq.tile_batch(\n",
    "        encoder_outputs, multiplier=beam_width)\n",
    "    tiled_encoder_final_state = tf.contrib.seq2seq.tile_batch(\n",
    "        encoder_final_state, multiplier=beam_width)\n",
    "    tiled_sequence_length = tf.contrib.seq2seq.tile_batch(\n",
    "        input_lengths, multiplier=beam_width)\n",
    "    \n",
    "    def decode(helper, scope, reuse=None):\n",
    "        with tf.variable_scope(scope, reuse=reuse):\n",
    "#             attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
    "#                 num_units=num_units, memory=tiled_inputs,\n",
    "#                 memory_sequence_length=tiled_sequence_length)\n",
    "            decoder_cells = [tf.contrib.rnn.GRUCell(num_units=num_units) for i in range(num_decoder_rnns)]\n",
    "            stacked_decoder_cell = tf.nn.rnn_cell.MultiRNNCell(decoder_cells)\n",
    "#             attn_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "#                 cell, attention_mechanism, attention_layer_size=num_units / 2)\n",
    "            out_cell = tf.contrib.rnn.OutputProjectionWrapper(\n",
    "                stacked_decoder_cell, vocab_size, reuse=reuse\n",
    "            )\n",
    "#             decoder_initial_state = attention_cell.zero_state(\n",
    "#                 dtype, batch_size=true_batch_size * beam_width)\n",
    "#             decoder_initial_state = decoder_initial_state.clone(\n",
    "#                 cell_state=tiled_encoder_final_state)\n",
    "            \n",
    "            decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
    "                        out_cell,\n",
    "                        embeddings,\n",
    "                        start_tokens = tf.to_int32(start_tokens),\n",
    "                        end_token = 1,\n",
    "                        initial_state = tiled_encoder_final_state ,\n",
    "                        beam_width = beam_width,\n",
    "                        flength_penalty_weight = 0.2,\n",
    "                    )\n",
    "    \n",
    "#             decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "#                 cell=out_cell, helper=helper,\n",
    "#                 initial_state=encoder_final_state)\n",
    "#                 initial_state=encoder_final_state)\n",
    "\n",
    "            outputs = tf.contrib.seq2seq.dynamic_decode(\n",
    "                decoder=decoder, output_time_major=False,\n",
    "                impute_finished=True, maximum_iterations=output_max_length\n",
    "            )\n",
    "            return outputs[0]\n",
    "    \n",
    "    train_outputs = decode(train_helper, 'decode')\n",
    "    print(train_outputs, train_outputs.rnn_output)\n",
    "    \n",
    "    pred_outputs = decode(pred_helper, 'decode', reuse=True)\n",
    "\n",
    "    tf.identity(train_outputs.sample_id[0], name='train_pred')\n",
    "    weights = tf.to_float(tf.not_equal(train_output[:, :-1], 1))\n",
    "    loss = tf.contrib.seq2seq.sequence_loss(\n",
    "        train_outputs.rnn_output, output, weights=weights)\n",
    "    train_op = layers.optimize_loss(\n",
    "        loss, tf.train.get_global_step(),\n",
    "        optimizer=params.get('optimizer', 'Adam'),\n",
    "        learning_rate=params.get('learning_rate', 0.001),\n",
    "        learning_rate_decay_fn = (lambda p1, p2: tf.train.exponential_decay(\n",
    "                                                        p1,\n",
    "                                                        p2,\n",
    "                                                        1000,\n",
    "                                                        0.9,\n",
    "                                                        staircase=False,\n",
    "                                                    )),\n",
    "        summaries=['loss', 'learning_rate'])\n",
    "\n",
    "    tf.identity(pred_outputs.sample_id[0], name='predictions')\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        predictions={'train_pred':train_outputs.sample_id, 'predictions':pred_outputs.sample_id},\n",
    "        loss=loss,\n",
    "        train_op=train_op\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_map(line, vocab):\n",
    "    return [vocab.get(token, UNK_TOKEN) for token in line.split(' ')]\n",
    "\n",
    "def make_input_fn(\n",
    "        data_dir, subset, list_IDs, prons, phonemes,\n",
    "        input_max_length, output_max_length,\n",
    "        img_size, keypoint_img_size,\n",
    "        keypoint_size,\n",
    "        batch_size=1,\n",
    "        input_process=tokenize_and_map, \n",
    "        output_process=tokenize_and_map):\n",
    "\n",
    "    def input_fn():\n",
    "        inp = tf.placeholder(tf.float32, shape=[None, None, img_size[0], img_size[1], img_size[2]], name='input')\n",
    "        output = tf.placeholder(tf.int64, shape=[None, None], name='output')\n",
    "        tf.identity(inp[0], 'input_0')\n",
    "        tf.identity(output[0], 'output_0')\n",
    "        return {\n",
    "            'input': inp,\n",
    "            'output': output,\n",
    "        }, None\n",
    "\n",
    "    def sampler():\n",
    "        while True:\n",
    "            rnd = random.randint(0, len(list_IDs)-1)\n",
    "            v_ID = list_IDs[rnd]\n",
    "            v_url, v_index = v_ID\n",
    "            filepath = os.path.join(data_dir, subset, v_url, v_index)\n",
    "            v_V, v_F, v_T = prepare_data(filepath, img_size, keypoint_img_size,\n",
    "                                           keypoint_size)\n",
    "            num_frames = v_V.shape[0]\n",
    "            yield {\n",
    "                    'input':  v_V[:min(num_frames,input_max_length), :, :, :], # v_F[:input_max_length]),\n",
    "                    'output': v_T[:output_max_length - 1] + [END_TOKEN]\n",
    "                }\n",
    "\n",
    "    sample_me = sampler()\n",
    "\n",
    "    def feed_fn():\n",
    "        inputs, outputs = [], []\n",
    "        input_length, output_length = 0, 0\n",
    "        for i in range(batch_size):\n",
    "            rec = sample_me.next()\n",
    "            inputs.append(rec['input'])\n",
    "            outputs.append(rec['output'])\n",
    "            num_frames = inputs[-1].shape[0] #\n",
    "#             print(inputs[-1].shape)\n",
    "            input_length = max(input_length, num_frames)\n",
    "            output_length = max(output_length, len(outputs[-1]))\n",
    "        # Pad me right with </S> token.\n",
    "        for i in range(batch_size):\n",
    "            num_frames = inputs[i].shape[0]\n",
    "            if input_length>num_frames:\n",
    "                new_input_V = np.pad(inputs[i], [(input_length - num_frames, 0), (0,0), (0,0), (0,0)], 'constant')\n",
    "                # new_input_F = np.pad(inputs[i][1], [output_length - num_frames, 0, 0, 0], 'constant')\n",
    "                inputs[i] = new_input_V\n",
    "            outputs[i] += [END_TOKEN] * (output_length - len(outputs[i]))\n",
    "        return {\n",
    "            'input:0': inputs,\n",
    "            'output:0': outputs\n",
    "        }\n",
    "\n",
    "    return input_fn, feed_fn\n",
    "\n",
    "\n",
    "def get_rev_vocab(vocab):\n",
    "    return {idx: key for key, idx in vocab.iteritems()}\n",
    "\n",
    "def get_formatter(keys, vocab):\n",
    "    rev_vocab = get_rev_vocab(vocab)\n",
    "    def to_str(sequence):\n",
    "        tokens = [\n",
    "            rev_vocab.get(x, \"<UNK>\") for x in sequence]\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    def format(values):\n",
    "        res = []\n",
    "        for key in keys:\n",
    "            res.append(\"%s = %s\" % (key, to_str(values[key])))\n",
    "        return '\\n'.join(res)\n",
    "    return format\n",
    "\n",
    "def get_editdist_formatter(keys, vocab):\n",
    "    rev_vocab = get_rev_vocab(vocab)\n",
    "    \n",
    "    def to_str(sequence):\n",
    "        tokens = [\n",
    "            rev_vocab.get(x, \"<UNK>\") for x in sequence]\n",
    "        return tokens\n",
    "\n",
    "    def format(values):\n",
    "        labels = values[keys[0]]  \n",
    "        clean_labels = []\n",
    "        for label in labels:\n",
    "            if label == END_TOKEN:\n",
    "                break\n",
    "            else:\n",
    "                clean_labels.append(label)\n",
    "        clean_labels = to_str(clean_labels)\n",
    "        \n",
    "        preds =  values[keys[1]]\n",
    "        clean_preds = []\n",
    "        for pred in preds:\n",
    "            if pred == END_TOKEN:\n",
    "                break\n",
    "            else:\n",
    "                clean_preds.append(pred)\n",
    "        clean_preds = to_str(clean_preds)\n",
    "        wer.wer(clean_labels, clean_preds)\n",
    "        return '\\n====== WER '+ keys[1]+' ======\\n'\n",
    "    \n",
    "    return format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_seq2seq(params, data_dir, subset, model_dir, list_IDs, prons, phonemes):\n",
    "    est = tf.estimator.Estimator(\n",
    "        model_fn=seq2seq,\n",
    "        model_dir=model_dir, params=params)\n",
    "\n",
    "    input_fn, feed_fn = make_input_fn(\n",
    "                            data_dir, subset, list_IDs, prons, phonemes,\n",
    "                            params['input_max_length'], params['output_max_length'],\n",
    "                            params['img_size'], params['keypoint_img_size'],\n",
    "                            params['keypoint_size'],\n",
    "                            params['batch_size'])\n",
    "\n",
    "    # Make hooks to print examples of inputs/predictions.\n",
    "    print_inputs = tf.train.LoggingTensorHook(\n",
    "        ['output_0'], every_n_iter=100,\n",
    "        formatter=get_formatter(['output_0'], phonemes))\n",
    "    print_predictions = tf.train.LoggingTensorHook(\n",
    "        ['predictions', 'train_pred'], every_n_iter=100,\n",
    "        formatter=get_formatter(['predictions', 'train_pred'], phonemes))\n",
    "    print_train_edit_distance = tf.train.LoggingTensorHook(\n",
    "        ['output_0', 'train_pred'], every_n_iter=100,\n",
    "        formatter=get_editdist_formatter(['output_0', 'train_pred'], phonemes))\n",
    "    print_pred_edit_distance = tf.train.LoggingTensorHook(\n",
    "        ['output_0', 'predictions'], every_n_iter=100,\n",
    "        formatter=get_editdist_formatter(['output_0', 'predictions'], phonemes))\n",
    "\n",
    "    est.train(\n",
    "        input_fn=input_fn,\n",
    "        hooks=[tf.train.FeedFnHook(feed_fn), print_inputs, print_predictions, \n",
    "                       print_train_edit_distance, print_pred_edit_distance],\n",
    "        steps=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_single_pass_input_fn(\n",
    "        data_dir, subset, list_IDs, prons, phonemes,\n",
    "        input_max_length, output_max_length,\n",
    "        img_size, keypoint_img_size,\n",
    "        keypoint_size,\n",
    "        batch_size=1,\n",
    "        input_process=tokenize_and_map, \n",
    "        output_process=tokenize_and_map):\n",
    "\n",
    "    def input_fn():\n",
    "        inp = tf.placeholder(tf.float32, shape=[None, None, img_size[0], img_size[1], img_size[2]], name='input')\n",
    "        output = tf.placeholder(tf.int64, shape=[None, None], name='output')\n",
    "        tf.identity(inp[0], 'input_0')\n",
    "        tf.identity(output[0], 'output_0')\n",
    "        return {\n",
    "            'input': inp,\n",
    "            'output': output,\n",
    "        }, None\n",
    "\n",
    "    def sampler():\n",
    "        for rnd in range(len(list_IDs)):\n",
    "            v_ID = list_IDs[rnd]\n",
    "            v_url, v_index = v_ID\n",
    "            filepath = os.path.join(data_dir, subset, v_url, v_index)\n",
    "            v_V, v_F, v_T = prepare_data(filepath, img_size, keypoint_img_size,\n",
    "                                           keypoint_size)\n",
    "            num_frames = v_V.shape[0]\n",
    "            yield {\n",
    "                    'input':  v_V[:min(num_frames,input_max_length), :, :, :], # v_F[:input_max_length]),\n",
    "                    'output': v_T[:output_max_length - 1] + [END_TOKEN]\n",
    "                }\n",
    "        return\n",
    "#         raise tf.errors.OutOfRangeError\n",
    "\n",
    "    sample_me = sampler()\n",
    "\n",
    "    def feed_fn():\n",
    "        inputs, outputs = [], []\n",
    "        input_length, output_length = 0, 0\n",
    "        for i in range(batch_size):\n",
    "            rec = sample_me.next()\n",
    "            inputs.append(rec['input'])\n",
    "            outputs.append(rec['output'])\n",
    "            num_frames = inputs[-1].shape[0] #\n",
    "#             print(inputs[-1].shape)\n",
    "            input_length = max(input_length, num_frames)\n",
    "            output_length = max(output_length, len(outputs[-1]))\n",
    "        # Pad me right with </S> token.\n",
    "        for i in range(batch_size):\n",
    "            num_frames = inputs[i].shape[0]\n",
    "            if input_length>num_frames:\n",
    "                new_input_V = np.pad(inputs[i], [(0, input_length - num_frames), (0,0), (0,0), (0,0)], 'constant')\n",
    "                # new_input_F = np.pad(inputs[i][1], [output_length - num_frames, 0, 0, 0], 'constant')\n",
    "                inputs[i] = new_input_V\n",
    "            outputs[i] += [END_TOKEN] * (output_length - len(outputs[i]))\n",
    "        return {\n",
    "            'input:0': inputs,\n",
    "            'output:0': outputs\n",
    "        }\n",
    "\n",
    "    return input_fn, feed_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_seq2seq(params, data_dir, subset, model_dir, list_IDs, prons, phonemes):\n",
    "    est = tf.estimator.Estimator(\n",
    "        model_fn=seq2seq,\n",
    "        model_dir=model_dir, params=params)\n",
    "\n",
    "    input_fn, feed_fn = make_single_pass_input_fn(\n",
    "                            data_dir, subset, list_IDs, prons, phonemes,\n",
    "                            params['input_max_length'], params['output_max_length'],\n",
    "                            params['img_size'], params['keypoint_img_size'],\n",
    "                            params['keypoint_size'],\n",
    "                            params['batch_size'])\n",
    "\n",
    "    # Make hooks to print examples of inputs/predictions.\n",
    "    print_inputs = tf.train.LoggingTensorHook(\n",
    "        ['output_0'], every_n_iter=100,\n",
    "        formatter=get_formatter(['output_0'], phonemes))\n",
    "    print_predictions = tf.train.LoggingTensorHook(\n",
    "        ['predictions', 'train_pred'], every_n_iter=100,\n",
    "        formatter=get_formatter(['predictions', 'train_pred'], phonemes))\n",
    "    print_train_edit_distance = tf.train.LoggingTensorHook(\n",
    "        ['output_0', 'train_pred'], every_n_iter=100,\n",
    "        formatter=get_editdist_formatter(['output_0', 'train_pred'], phonemes))\n",
    "    print_pred_edit_distance = tf.train.LoggingTensorHook(\n",
    "        ['output_0', 'predictions'], every_n_iter=100,\n",
    "        formatter=get_editdist_formatter(['output_0', 'predictions'], phonemes))\n",
    "    \n",
    "    eval_predictions = est.predict(\n",
    "        input_fn = input_fn,\n",
    "        hooks=[tf.train.FeedFnHook(feed_fn)], \n",
    "#                print_inputs, print_predictions, \n",
    "#                        print_train_edit_distance, print_pred_edit_distance],\n",
    "    )\n",
    "    return eval_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'vocab_size': 42,\n",
    "    'batch_size': 8,\n",
    "    'input_max_length': 100,\n",
    "    'output_max_length': 100,\n",
    "    'embed_dim': 100,\n",
    "    'num_units': 256,\n",
    "    'img_size': (120, 120, 3), \n",
    "    'keypoint_img_size': (224, 224),\n",
    "    'keypoint_size': 20,\n",
    "    'n_classes':39,\n",
    "    'num_tokens': 42,\n",
    "    'num_encoder_rnns':2,\n",
    "    'num_decoder_rnns':2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_editdist(labels, preds, vocab):\n",
    "    rev_vocab = get_rev_vocab(vocab)\n",
    "    \n",
    "    def to_str(sequence):\n",
    "        tokens = [\n",
    "            rev_vocab.get(x, \"<UNK>\") for x in sequence]\n",
    "        return tokens\n",
    "\n",
    "    clean_labels = []\n",
    "    for label in labels:\n",
    "        if label == END_TOKEN:\n",
    "            break\n",
    "        else:\n",
    "            clean_labels.append(label)\n",
    "    str_clean_labels = to_str(clean_labels)\n",
    "\n",
    "    clean_preds = []\n",
    "    for pred in preds:\n",
    "        if pred == END_TOKEN:\n",
    "            break\n",
    "        else:\n",
    "            clean_preds.append(pred)\n",
    "    str_clean_preds = to_str(clean_preds)        \n",
    "    score = wer.wer(str_clean_labels, str_clean_preds)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#     tf.logging.logger.setLevel(logging.INFO)\n",
    "train_seq2seq(params, SAVE_DIR, 'trainval', './model/seq2seq_copiedstates_no_attentions_bn_2rnn', trainval_ID_list, pron_dict, phoneme_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#     tf.logging.logger.setLevel(logging.INFO)\n",
    "eval_predictions = eval_seq2seq(params, SAVE_DIR, 'trainval', './model/seq2seq_copiedstates_no_attentions', trainval_ID_list, pron_dict, phoneme_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(eval_predictions, subset, ID_list, phoneme_dict):\n",
    "    my_results = {}\n",
    "    i=0\n",
    "    for item in eval_predictions:\n",
    "        print(item, ID_list[i])\n",
    "        item_id =  ID_list[i][0]+\" \"+ ID_list[i][1]\n",
    "        my_results[item_id] = item\n",
    "        i+=1\n",
    "    \n",
    "    acc_edit_distance = []\n",
    "    per_accuracy = []\n",
    "    per_predictions = [] # for confusion matrix\n",
    "    per_gt = []\n",
    "    \n",
    "    for key in my_results:\n",
    "        vs = key.split()\n",
    "        v_url = vs[0]\n",
    "        v_index = vs[1]\n",
    "        train_pred = my_results[key]['train_pred']\n",
    "        predictions = my_results[key]['predictions']\n",
    "        filepath = os.path.join(SAVE_DIR, subset, v_url, v_index)\n",
    "        v_V, v_F, v_T = prepare_data(filepath,  params['img_size'], params['keypoint_img_size'], params['keypoint_size'])\n",
    "    #     print(v_T, my_results[key])\n",
    "        ed = get_editdist(list(v_T), predictions, phoneme_dict)\n",
    "        acc_edit_distance.append(ed)\n",
    "        \n",
    "        end_idx = len(v_T)\n",
    "        \n",
    "        accuracy = float(np.sum(np.equal(train_pred[2:end_idx], np.array(v_T[2:]))))/end_idx\n",
    "        per_accuracy.append(accuracy)\n",
    "        \n",
    "        per_predictions.append(train_pred[2:end_idx])\n",
    "        per_gt.append(v_T[2:end_idx])\n",
    "    return np.mean(acc_edit_distance), \\\n",
    "            np.mean(accuracy), per_predictions, per_gt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_test_predictions = eval_seq2seq(params, SAVE_DIR, 'test', './model/seq2seq_copiedstates_no_attentions', test_ID_list, pron_dict, phoneme_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ed, test_accuracies, test_predictions, test_gt = calculate_accuracy(eval_test_predictions,'test', test_ID_list, phoneme_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "#     print(unique_labels(y_true, y_pred))\n",
    "#     classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "    fig, ax = plt.subplots(figsize=(20, 20))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "#     fmt = '.2f' if normalize else 'd'\n",
    "#     thresh = cm.max() / 2.\n",
    "#     for i in range(cm.shape[0]):\n",
    "#         for j in range(cm.shape[1]):\n",
    "#             ax.text(j, i, format(cm[i, j], fmt),\n",
    "#                     ha=\"center\", va=\"center\",\n",
    "#                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "flattened_test_gt = []\n",
    "for item in test_gt:\n",
    "    flattened_test_gt.extend(item)\n",
    "\n",
    "flattened_test_predictions = []\n",
    "for item in test_predictions:\n",
    "    flattened_test_predictions.extend(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_mat=sklearn.metrics.confusion_matrix(flattened_test_gt, flattened_test_predictions, labels=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(flattened_test_gt, flattened_test_predictions, phoneme_re_list,\n",
    "                          normalize=True,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phoneme_re_list = sorted(phoneme_dict.iteritems(), key=lambda x:x[1]) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-audio-3.6",
   "language": "python",
   "name": "conda-audio-3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
