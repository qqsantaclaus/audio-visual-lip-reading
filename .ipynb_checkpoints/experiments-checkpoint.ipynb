{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import pickle\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import sys\n",
    "import dlib\n",
    "# import skvideo.io\n",
    "import json\n",
    "from keras.preprocessing import image\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([('AA', 'vowel'), ('AE', 'vowel'), ('AH', 'vowel'), ('AO', 'vowel'), ('AW', 'vowel'), ('AY', 'vowel'), ('B', 'stop'), ('CH', 'affricate'), ('D', 'stop'), ('DH', 'fricative'), ('EH', 'vowel'), ('ER', 'vowel'), ('EY', 'vowel'), ('F', 'fricative'), ('G', 'stop'), ('HH', 'aspirate'), ('IH', 'vowel'), ('IY', 'vowel'), ('JH', 'affricate'), ('K', 'stop'), ('L', 'liquid'), ('M', 'nasal'), ('N', 'nasal'), ('NG', 'nasal'), ('OW', 'vowel'), ('OY', 'vowel'), ('P', 'stop'), ('R', 'liquid'), ('S', 'fricative'), ('SH', 'fricative'), ('T', 'stop'), ('TH', 'fricative'), ('UH', 'vowel'), ('UW', 'vowel'), ('V', 'fricative'), ('W', 'semivowel'), ('Y', 'semivowel'), ('Z', 'fricative'), ('ZH', 'fricative')], {'IY': 17, 'W': 35, 'DH': 9, 'Y': 36, 'HH': 15, 'CH': 7, 'JH': 18, 'ZH': 38, 'EH': 10, 'NG': 23, 'TH': 31, 'AA': 0, 'B': 6, 'AE': 1, 'D': 8, 'G': 14, 'F': 13, 'AH': 2, 'K': 19, 'M': 21, 'L': 20, 'AO': 3, 'N': 22, 'IH': 16, 'S': 28, 'R': 27, 'EY': 12, 'T': 30, 'AW': 4, 'V': 34, 'AY': 5, 'Z': 37, 'ER': 11, 'P': 26, 'UW': 33, 'SH': 29, 'UH': 32, 'OY': 25, 'OW': 24})\n"
     ]
    }
   ],
   "source": [
    "phoneme_list = [] \n",
    "phoneme_dict = {}\n",
    "\n",
    "with open(\"/n/fs/scratch/jiaqis/cmudict-master/cmudict.phones\", 'r') as fp:\n",
    "    i = 0\n",
    "    line = fp.readline()\n",
    "    while line:\n",
    "        phoneme = line.split()[0].strip()\n",
    "        phoneme_property = line.split()[1].strip()\n",
    "        phoneme_list.append((phoneme, phoneme_property))\n",
    "        phoneme_dict[phoneme] = i\n",
    "        line = fp.readline()\n",
    "        i=i+1\n",
    "\n",
    "print(phoneme_list, phoneme_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pron_dict = cmudict.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import cmudict\n",
    "\n",
    "def clean_pron(pron):\n",
    "    \"\"\"Remove stress from pronunciations.\"\"\"\n",
    "    return re.sub(r\"\\d\", \"\", pron)\n",
    "\n",
    "def make_triphones(pron):\n",
    "    \"\"\"Output triphones from a word's pronunciation.\"\"\"\n",
    "    if len(pron) < 3:\n",
    "        return []\n",
    "    # Junk on end is to make word boundaries work\n",
    "    return ([((pron[idx - 2], pron[idx - 1]), pron[idx])\n",
    "             for idx in range(2, len(pron))] + [(('#', '#'), pron[0])] +\n",
    "            [((pron[-2], pron[-1]), '#')])\n",
    "                                                \n",
    "def triphone_probs(prons):\n",
    "    \"\"\"Calculate triphone probabilities for pronunciations.\"\"\"\n",
    "    context_counts = defaultdict(lambda: defaultdict(int))\n",
    "    for pron in prons:\n",
    "        for (context, phoneme) in make_triphones(pron):\n",
    "            context_counts[context][phoneme] += 1\n",
    "            \n",
    "    for (context, outcomes) in context_counts.items():\n",
    "        total_outcomes = sum(outcomes.values())\n",
    "        for outcome, count in outcomes.items():\n",
    "            context_counts[context][outcome] = float(count) / total_outcomes\n",
    "        \n",
    "    return context_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Volume and Facial Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"/n/fs/scratch/jiaqis/LRS3-TED/\"\n",
    "SAVE_DIR = \"/n/fs/scratch/jiaqis/LRS3-TED-Extracted/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_list(dataDir, setName):\n",
    "    # Images, facial/mouth features, text-> phonetic\n",
    "    data_list = []\n",
    "    for urlDir in glob.glob(os.path.join(dataDir, setName, \"*/\")):\n",
    "        url = urlDir.split('/')[-2]\n",
    "        for idFilename in glob.glob(os.path.join(urlDir, '*.txt')):\n",
    "            ID = idFilename.split('/')[-1]\n",
    "            data_list.append((url, ID))\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ID_list = get_dataset_list(SAVE_DIR, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1320\n"
     ]
    }
   ],
   "source": [
    "print(len(test_ID_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "FPS = 25\n",
    "FRAME_ROWS = 120\n",
    "FRAME_COLS = 120\n",
    "NFRAMES = 5 # size of input volume of frames\n",
    "MARGIN = NFRAMES/2\n",
    "COLORS = 1 # grayscale\n",
    "CHANNELS = COLORS*NFRAMES\n",
    "MAX_FRAMES_COUNT= 250 # corresponding to 10 seconds, 25Hz*10\n",
    "\n",
    "EXAMPLE_FILEPATH = \"/n/fs/scratch/jiaqis/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(filepath, video_tensor_size, keypoint_size, label_seq_size):\n",
    "    # images\n",
    "    # frames x rows x cols x channels\n",
    "    visual_cube = np.zeros(video_target_size, dtype=\"float16\")\n",
    "    # keypoint features\n",
    "    feature_cube = np.zeros((video_target_size[0], video_target_size[1], video_target_size[2], keypoint_size), dtype=\"float16\")\n",
    "    features = json.load(open(filepath + \".json\", 'r'))\n",
    "    # Target Text/phonemes\n",
    "    labels = []\n",
    "    text = open(filepath+\".txt\", 'r').readline()\n",
    "    words = text.strip().split()\n",
    "    for i in range(words):\n",
    "        word_phonemes = pron_dict[word]\n",
    "        labels.extend([phoneme_dict[phon] for phon in word_phonemes])\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    acc = 0\n",
    "    for imgFilename in sorted(glob.glob(filepath + \"_*.jpg\")):\n",
    "        img = image.img_to_array(\n",
    "              image.load_img(imgFilename, target_size=img_target_size))\n",
    "        img = preprocess_input(img)\n",
    "        visual_cube[acc,:,:,:] = img\n",
    "        \n",
    "        framenum = int(imgFilename.split(\"_\")[-1].split(\".\")[0])\n",
    "        f_feature = features[framenum]\n",
    "        for ft_index in range(keypoint_size):\n",
    "            # TODO: check range of outputs\n",
    "            keypoint_x = math.floor(f_feature[ft_index, 0]) \n",
    "            keypoint_y = math.floor(f_feature[ft_index, 1])\n",
    "            feature_cube[acc, keypoint_y, keypoint_x, ft_index] = 1.0 \n",
    "        acc+=1\n",
    "    return visual_cube, feature_cube, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, vqa, features, answers, vocabulary, batch_size=32,\n",
    "                       n_classes=1000, shuffle=True, \n",
    "                       max_seq_len=30, feature_size=2048):\n",
    "        'Initialization'\n",
    "        self.batch_size = batch_size\n",
    "        self.feature_size = feature_size\n",
    "        self.features = features\n",
    "        self.answers = answers\n",
    "        self.vocabulary = vocabulary\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.vqa = vqa\n",
    "        self.list_IDs = vqa.getQuesIds()\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.ceil(float(len(self.list_IDs)) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        # Generate data\n",
    "        X, Y = self.data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, Y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        Q = np.zeros((len(list_IDs_temp), self.max_seq_len))\n",
    "        I = np.zeros((len(list_IDs_temp), self.feature_size))\n",
    "        A = np.zeros((len(list_IDs_temp), self.n_classes))\n",
    "        for it, quesID in enumerate(list_IDs_temp):\n",
    "          # Add question as a index sequence\n",
    "          question = self.vqa.qqa[quesID]['question']\n",
    "          words = question.split()\n",
    "          for i, word in enumerate(words):\n",
    "            new_word = preprocess_word(word)\n",
    "            if new_word in self.vocabulary:\n",
    "              Q[it, i] = self.vocabulary[new_word]\n",
    "          # Add image feature\n",
    "          ann = self.vqa.qa[quesID]\n",
    "          imgId = ann['image_id']\n",
    "          I[it, :] = self.features[imgId]\n",
    "          # Majority vote for answer\n",
    "          for ans in ann['answers']:\n",
    "            if ans['answer'] in self.answers:\n",
    "              ans_index = self.answers[ans['answer']]\n",
    "              A[it, ans_index] = A[it, ans_index] + 1.0\n",
    "          A[it, :] = A[it, :] / 10.0\n",
    "        \n",
    "        return [Q, I], A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator =  DataGenerator(train_vqa, train_features, answers_1000_dict, \n",
    "                              vocabulary_dict, batch_size=batch_size,\n",
    "                              n_classes=n_classes, shuffle=True, \n",
    "                              max_seq_len=max_seq_len, feature_size=feature_size)\n",
    "val_generator = DataGenerator(val_vqa, val_features, answers_1000_dict, \n",
    "                              vocabulary_dict, batch_size=batch_size,\n",
    "                              n_classes=n_classes, shuffle=True, \n",
    "                              max_seq_len=max_seq_len, feature_size=feature_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, BatchNormalization,ZeroPadding2D, Embedding, LSTM, Bidirectional, Add, Multiply, Activation, Masking, Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# Baseline Model #\n",
    "##################\n",
    "input_Q_tensor = Input(shape=(max_seq_len,), name=\"Q\")\n",
    "input_I_tensor = Input(shape=(feature_size,), name=\"I\")\n",
    "\n",
    "# Question Branch\n",
    "q_embed_tensor = Embedding(len(vocabulary)+1, 300, mask_zero=True, input_length=None)(input_Q_tensor)\n",
    "print(q_embed_tensor)\n",
    "masked_q_embed_tensor = Masking(mask_value=0)(q_embed_tensor)\n",
    "print(masked_q_embed_tensor)\n",
    "q_embed_act = Activation(\"tanh\")(masked_q_embed_tensor)\n",
    "print(q_embed_act)\n",
    "\n",
    "q_tensor_seq = LSTM(lstm_hidden_units, dropout=0.2, recurrent_dropout=0.5, \n",
    "                              return_sequences=False, return_state=False)(q_embed_act)\n",
    "q_feature = LSTM(lstm_hidden_units, dropout=0.2, recurrent_dropout=0.5, \n",
    "                              return_sequences=False, return_state=False)(q_tensor_seq)\n",
    "\n",
    "# Image Branch\n",
    "i_feature = Dense(lstm_hidden_units, activation='relu')(input_I_tensor)\n",
    "\n",
    "# Merge\n",
    "combo_feature = Add()([i_feature, q_feature])\n",
    "scores = Dense(n_classes, activation=\"softmax\")(combo_feature)\n",
    "\n",
    "model = Model(inputs=[input_Q_tensor, input_I_tensor], outputs=scores)\n",
    "\n",
    "print(model.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-audio-3.6",
   "language": "python",
   "name": "conda-audio-3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
