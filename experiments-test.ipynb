{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import pickle\n",
    "import numpy as np\n",
    "# import cv2\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import sys\n",
    "# import dlib\n",
    "# import skvideo.io\n",
    "import json\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import glob\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import cmudict\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['CUDA_VISIBLE_DEVICES']='2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([('AA', 'vowel'), ('AE', 'vowel'), ('AH', 'vowel'), ('AO', 'vowel'), ('AW', 'vowel'), ('AY', 'vowel'), ('B', 'stop'), ('CH', 'affricate'), ('D', 'stop'), ('DH', 'fricative'), ('EH', 'vowel'), ('ER', 'vowel'), ('EY', 'vowel'), ('F', 'fricative'), ('G', 'stop'), ('HH', 'aspirate'), ('IH', 'vowel'), ('IY', 'vowel'), ('JH', 'affricate'), ('K', 'stop'), ('L', 'liquid'), ('M', 'nasal'), ('N', 'nasal'), ('NG', 'nasal'), ('OW', 'vowel'), ('OY', 'vowel'), ('P', 'stop'), ('R', 'liquid'), ('S', 'fricative'), ('SH', 'fricative'), ('T', 'stop'), ('TH', 'fricative'), ('UH', 'vowel'), ('UW', 'vowel'), ('V', 'fricative'), ('W', 'semivowel'), ('Y', 'semivowel'), ('Z', 'fricative'), ('ZH', 'fricative')], {'IY': 17, 'W': 35, 'DH': 9, 'Y': 36, 'HH': 15, 'CH': 7, 'JH': 18, 'ZH': 38, 'EH': 10, 'NG': 23, 'TH': 31, 'AA': 0, 'B': 6, 'AE': 1, 'D': 8, 'G': 14, 'F': 13, 'AH': 2, 'K': 19, 'M': 21, 'L': 20, 'AO': 3, 'N': 22, 'IH': 16, 'S': 28, 'R': 27, 'EY': 12, 'T': 30, 'AW': 4, 'V': 34, 'AY': 5, 'Z': 37, 'ER': 11, 'P': 26, 'UW': 33, 'SH': 29, 'UH': 32, 'OY': 25, 'OW': 24})\n"
     ]
    }
   ],
   "source": [
    "phoneme_list = [] \n",
    "phoneme_dict = {}\n",
    "\n",
    "with open(\"/n/fs/scratch/jiaqis/cmudict-master/cmudict.phones\", 'r') as fp:\n",
    "    i = 0\n",
    "    line = fp.readline()\n",
    "    while line:\n",
    "        phoneme = line.split()[0].strip()\n",
    "        phoneme_property = line.split()[1].strip()\n",
    "        phoneme_list.append((phoneme, phoneme_property))\n",
    "        phoneme_dict[phoneme] = i\n",
    "        line = fp.readline()\n",
    "        i=i+1\n",
    "\n",
    "print(phoneme_list, phoneme_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pron_dict = cmudict.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_pron(pron):\n",
    "    \"\"\"Remove stress from pronunciations.\"\"\"\n",
    "    return re.sub(r\"\\d\", \"\", pron)\n",
    "\n",
    "def make_triphones(pron):\n",
    "    \"\"\"Output triphones from a word's pronunciation.\"\"\"\n",
    "    if len(pron) < 3:\n",
    "        return []\n",
    "    # Junk on end is to make word boundaries work\n",
    "    return ([((pron[idx - 2], pron[idx - 1]), pron[idx])\n",
    "             for idx in range(2, len(pron))] + [(('#', '#'), pron[0])] +\n",
    "            [((pron[-2], pron[-1]), '#')])\n",
    "                                                \n",
    "def triphone_probs(prons):\n",
    "    \"\"\"Calculate triphone probabilities for pronunciations.\"\"\"\n",
    "    context_counts = defaultdict(lambda: defaultdict(int))\n",
    "    for pron in prons:\n",
    "        for (context, phoneme) in make_triphones(pron):\n",
    "            context_counts[context][phoneme] += 1\n",
    "            \n",
    "    for (context, outcomes) in context_counts.items():\n",
    "        total_outcomes = sum(outcomes.values())\n",
    "        for outcome, count in outcomes.items():\n",
    "            context_counts[context][outcome] = float(count) / total_outcomes\n",
    "        \n",
    "    return context_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Volume and Facial Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"/n/fs/scratch/jiaqis/LRS3-TED/\"\n",
    "SAVE_DIR = \"/n/fs/scratch/jiaqis/LRS3-TED-Extracted/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_limit = 80\n",
    "def get_dataset_list(dataDir, setName):\n",
    "    # Images, facial/mouth features, text-> phonetic\n",
    "    data_list = []\n",
    "    for urlDir in glob.glob(os.path.join(dataDir, setName, \"*/\")):\n",
    "        url = urlDir.split('/')[-2]\n",
    "        for idFilename in glob.glob(os.path.join(urlDir, '*.txt')):\n",
    "            index = idFilename.split('/')[-1].split('.')[0]\n",
    "            filepath = os.path.join(dataDir, setName, url, index)\n",
    "            \n",
    "            text = open(filepath+\".txt\", 'r').readline()\n",
    "            words = text[5:].lower().strip().split()\n",
    "            flag = False\n",
    "            for word in words:\n",
    "                if word not in pron_dict:\n",
    "                    flag=True\n",
    "                    break\n",
    "            if flag:\n",
    "                continue\n",
    "            imgfiles = sorted(glob.glob(filepath + \"_*.jpg\"))\n",
    "            if len(imgfiles) > frame_limit:\n",
    "                continue\n",
    "            \n",
    "            ID = idFilename.split('/')[-1].split('.')[0]\n",
    "            data_list.append((url, ID))\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ID_list = get_dataset_list(SAVE_DIR, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainval_ID_list = get_dataset_list(SAVE_DIR, \"trainval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(test_ID_list, open('test_ID_list_'+str(frame_limit)+'.json', \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(trainval_ID_list, open('trainval_ID_list_'+str(frame_limit)+'.json', \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ID_list = json.load(open('test_ID_list.json', \"r\"))\n",
    "trainval_ID_list = json.load(open('trainval_ID_list.json', \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(508, 6231)\n"
     ]
    }
   ],
   "source": [
    "print(len(test_ID_list), len(trainval_ID_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "FPS = 25\n",
    "FRAME_ROWS = 120\n",
    "FRAME_COLS = 120\n",
    "NFRAMES = 5 # size of input volume of frames\n",
    "MARGIN = NFRAMES/2\n",
    "COLORS = 1 # grayscale\n",
    "CHANNELS = COLORS*NFRAMES\n",
    "MAX_FRAMES_COUNT= 250 # corresponding to 10 seconds, 25Hz*10\n",
    "\n",
    "EXAMPLE_FILEPATH = \"/n/fs/scratch/jiaqis/LRS3-TED-Extracted/test/0Fi83BHQsMA/00002\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_tensor_size = (100, 224, 224, 3) \n",
    "keypoint_size=20\n",
    "label_seq_size=100\n",
    "n_classes=39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(filepath, img_size, keypoint_size, label_seq_size):\n",
    "    # images\n",
    "    # frames x rows x cols x channels\n",
    "    visual_cube = []\n",
    "    # keypoint features\n",
    "    feature_cube = []\n",
    "    features = json.load(open(filepath + \".json\", 'r'))\n",
    "    # Target Text/phonemes\n",
    "    labels = []\n",
    "    text = open(filepath+\".txt\", 'r').readline()\n",
    "    words = text[5:].lower().strip().split()\n",
    "    for word in words:\n",
    "        word_phonemes = pron_dict[word][0]\n",
    "        word_indices = [phoneme_dict[clean_pron(phon)] for phon in word_phonemes]\n",
    "        labels.extend(word_indices)\n",
    "            \n",
    "    acc = 0\n",
    "    for imgFilename in sorted(glob.glob(filepath + \"_*.jpg\")):\n",
    "        if 'mouth' in imgFilename:\n",
    "            continue\n",
    "        x = image.img_to_array(\n",
    "              image.load_img(imgFilename, target_size=img_size))/255.0\n",
    "#         x = np.expand_dims(x, axis=0)\n",
    "#         x = preprocess_input(x)\n",
    "        visual_cube.append(x)\n",
    "        \n",
    "        mask = np.zeros((img_size[0], img_size[1], keypoint_size))\n",
    "        framenum = str(int(imgFilename.split(\"_\")[-1].split(\".\")[0]))\n",
    "        f_feature = features[framenum]['mouthCoords']\n",
    "        for ft_index in range(keypoint_size):\n",
    "            # TODO: check range of outputs\n",
    "            keypoint_x = f_feature[ft_index][0] - 1\n",
    "            keypoint_y = f_feature[ft_index][1] - 1\n",
    "            mask[keypoint_y, keypoint_x, ft_index] = 1.0\n",
    "        feature_cube.append(mask)\n",
    "        acc+=1\n",
    "    return np.array(visual_cube), np.array(feature_cube), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_cube, feature_cube, labels = prepare_data(EXAMPLE_FILEPATH, (224, 224, 3), 20, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41, 224, 224, 3)\n",
      "(41, 224, 224, 20)\n",
      "(19,)\n"
     ]
    }
   ],
   "source": [
    "print(visual_cube[:, :, :, :].shape)\n",
    "print(feature_cube.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/jpeg": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCADgAOABAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiipIBC1xEtxJJHAXAkeNA7KueSFJAJx2yM+oqw1tbFL2aO4kMEThLcuiK8pLHG5N+VG1WJK7wDtU/eBroLLwZBd3+x9etLfT4kZrq+eCV44jGkJmVWRWRyGl2Lh8OQuCPMj3cnRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRX/9k=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## TODO: Not working\n",
    "import PIL.Image\n",
    "from cStringIO import StringIO\n",
    "import IPython.display\n",
    "import numpy as np\n",
    "def showarray(a, fmt='jpeg'):\n",
    "    a = np.uint8(a * 255.0)\n",
    "    f = StringIO()\n",
    "    PIL.Image.fromarray(a).save(f, fmt)\n",
    "    IPython.display.display(IPython.display.Image(data=f.getvalue()))\n",
    "\n",
    "sub_cube = np.sum(feature_cube[0, :, :, :], axis=-1)\n",
    "mix_cube = np.minimum((visual_cube[0, :, :, :]) * 0.5\n",
    "                + np.sum(feature_cube[0, :, :, :], axis=-1, keepdims=True), 1.0)\n",
    "showarray(mix_cube)\n",
    "showarray(sub_cube)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, data_dir, subset, list_IDs, prons, phonemes,\n",
    "                       video_tensor_size=(200, 224, 224, 3), keypoint_size=20, label_seq_size=90, \n",
    "                       batch_size=32,\n",
    "                       n_classes=39, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.data_dir = data_dir\n",
    "        self.subset = subset\n",
    "        self.video_tensor_size = video_tensor_size\n",
    "        self.img_size = (video_tensor_size[1], video_tensor_size[2], video_tensor_size[3])\n",
    "        self.keypoint_size =keypoint_size\n",
    "        self.label_seq_size = label_seq_size\n",
    "        self.batch_size = batch_size\n",
    "        self.list_IDs = list_IDs\n",
    "        self.prons = prons\n",
    "        self.phonemes = phonemes\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.ceil(float(len(self.list_IDs)) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        # Generate data\n",
    "        X, Y = self.data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, Y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        V = np.zeros((len(list_IDs_temp), \n",
    "                      self.video_tensor_size[0],\n",
    "                      self.video_tensor_size[1],\n",
    "                      self.video_tensor_size[2],\n",
    "                      self.video_tensor_size[3]))\n",
    "        F = np.zeros((len(list_IDs_temp), \n",
    "                      self.video_tensor_size[0], \n",
    "                      self.video_tensor_size[1],\n",
    "                      self.video_tensor_size[2],\n",
    "                      self.keypoint_size))\n",
    "        # null = self.n_classes\n",
    "        T = self.n_classes * np.ones((len(list_IDs_temp), self.label_seq_size))\n",
    "        T_LEN = np.ones((len(list_IDs_temp), 1))\n",
    "        \n",
    "        acc = 0\n",
    "        for it, v_ID in enumerate(list_IDs_temp):\n",
    "            try:\n",
    "#                 print(v_ID)\n",
    "                v_url, v_index = v_ID\n",
    "                filepath = os.path.join(self.data_dir, self.subset, v_url, v_index)\n",
    "                v_V, v_F, v_T = prepare_data(filepath, self.img_size, \n",
    "                                               self.keypoint_size, self.label_seq_size)\n",
    "                if v_V.shape[0] > self.video_tensor_size[0]:\n",
    "                    continue\n",
    "            except Exception as e:\n",
    "#                 print(e)\n",
    "                continue\n",
    "                \n",
    "            num_frames = v_V.shape[0]\n",
    "            V[acc, -1*num_frames:, :, :, :] = v_V \n",
    "            F[acc, -1*num_frames:, :, :, :] = v_F \n",
    "            T[acc, :v_T.shape[0]] = v_T\n",
    "            T_LEN[acc, 0] = len(v_T)\n",
    "            acc+=1\n",
    "        \n",
    "        return [V[:acc], F[:acc], T[:acc], T_LEN[:acc]], \\\n",
    "                [np.zeros_like(T_LEN[:acc]), T[:acc, :, np.newaxis]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator =  DataGenerator(SAVE_DIR, 'trainval', trainval_ID_list, pron_dict, phoneme_dict,\n",
    "                       video_tensor_size=video_tensor_size, keypoint_size=keypoint_size, \n",
    "                       label_seq_size=label_seq_size, batch_size=2,\n",
    "                       n_classes=n_classes, shuffle=True)\n",
    "\n",
    "val_generator = DataGenerator(SAVE_DIR, 'test', test_ID_list, pron_dict, phoneme_dict,\n",
    "                       video_tensor_size=video_tensor_size, keypoint_size=keypoint_size, \n",
    "                       label_seq_size=label_seq_size, batch_size=2,\n",
    "                       n_classes=n_classes, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try_inputs, try_output = train_generator.data_generation(train_generator.list_IDs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try_V = try_inputs[0]\n",
    "try_F = try_inputs[1]\n",
    "try_T = try_inputs[2]\n",
    "try_T_LEN = try_inputs[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(try_V[2,90, :, :, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, BatchNormalization,ZeroPadding2D, Embedding, LSTM, Bidirectional, Add, Multiply, Activation, Masking, Concatenate\n",
    "from keras.layers import TimeDistributed, GlobalAveragePooling3D, Conv2D, Flatten, Permute, RepeatVector, Lambda, GlobalAveragePooling2D, MaxPooling2D\n",
    "import seq2seq\n",
    "from seq2seq.models import AttentionSeq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_block(inputs):\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    input_dim = int(inputs.shape[2])\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Dense(video_tensor_size[0], activation='softmax')(a)\n",
    "    a = Lambda(lambda x: keras.backend.mean(x, axis=1), name='dim_reduction')(a)\n",
    "    a = RepeatVector(input_dim)(a)\n",
    "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "    output_attention_mul = Multiply(name='attention_mul')([inputs, a_probs])\n",
    "    return output_attention_mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visual_conv_net(inputs, maxpool=True):\n",
    "    # 224 x 224 x 64\n",
    "    conv1 = TimeDistributed(Conv2D(32, kernel_size=(3,3), padding='same', \n",
    "                                   activation=\"relu\"))(inputs)\n",
    "    # 112 x 112 x 64\n",
    "    if maxpool:\n",
    "        down1 = TimeDistributed(MaxPooling2D(pool_size=(2, 2)))(conv1)\n",
    "    else:\n",
    "        down1 = TimeDistributed(Conv2D(32, kernel_size=(2,2), \n",
    "                                      strides=(2,2), \n",
    "                                      padding='same', \n",
    "                                      activation=None))(conv1)\n",
    "    # 112 x 112 x 128\n",
    "    conv2 = TimeDistributed(Conv2D(64, (3,3), padding='same', activation=\"relu\"))(down1)\n",
    "    # 56 x 56 x 128\n",
    "    if maxpool:\n",
    "        down2 = TimeDistributed(MaxPooling2D(pool_size=(2, 2)))(conv2)\n",
    "    else:\n",
    "        down2 = TimeDistributed(Conv2D(64, kernel_size=(2,2), \n",
    "                                      strides=(2,2), \n",
    "                                      padding='same', \n",
    "                                      activation=None))(conv2)\n",
    "    # 56 x 56 x 256\n",
    "    conv3 = TimeDistributed(Conv2D(128, (3,3), padding='same', activation=\"relu\"))(down2)\n",
    "    # 28 x 28 x 256\n",
    "    if maxpool:\n",
    "        down3 = TimeDistributed(MaxPooling2D(pool_size=(2, 2)))(conv3)\n",
    "    else:\n",
    "        down3 = TimeDistributed(Conv2D(128, kernel_size=(2, 2), \n",
    "                                      strides=(2,2), \n",
    "                                      padding='same', \n",
    "                                      activation=None))(conv3)\n",
    "    # 28 x 28 x 256\n",
    "    conv4 = TimeDistributed(Conv2D(128, (3,3), padding='same', activation=\"relu\"))(down3)\n",
    "    # 14 x 14 x 256\n",
    "    if maxpool:\n",
    "        down4 = TimeDistributed(MaxPooling2D(pool_size=(2, 2)))(conv4)\n",
    "    else:\n",
    "        down4 = TimeDistributed(Conv2D(128, kernel_size=(2,2), \n",
    "                                      strides=(2,2), \n",
    "                                      padding='same', \n",
    "                                      activation=None))(conv4)\n",
    "    # 14 x 14 x 256\n",
    "    conv5 = TimeDistributed(Conv2D(128, (3,3), padding='same', activation=\"relu\"))(down4)\n",
    "    # 7 x 7 x 256\n",
    "    if maxpool:\n",
    "        down5 = TimeDistributed(MaxPooling2D(pool_size=(2, 2)))(conv5)\n",
    "    else:\n",
    "        down5 = TimeDistributed(Conv2D(128, kernel_size=(2,2), \n",
    "                                      strides=(2,2), \n",
    "                                      padding='same', \n",
    "                                      activation=None))(conv5)\n",
    "    return down5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ctc_lambda_func(args):\n",
    "#     y_pred, labels, input_length, label_length = args\n",
    "#     # the 2 is critical here since the first couple outputs of the RNN\n",
    "#     # tend to be garbage:\n",
    "#     y_pred = y_pred[:, 2:, :]\n",
    "#     return keras.backend.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
    "\n",
    "def ctc_lambda_func(args):\n",
    "    base_output, labels, label_length = args \n",
    "    base_output_shape = tf.shape(base_output)\n",
    "    sequence_length = tf.fill([base_output_shape[0], 1], base_output_shape[1])\n",
    "    print(labels)\n",
    "    print(base_output)\n",
    "    print(sequence_length)\n",
    "    print(label_length)\n",
    "    \n",
    "    return keras.backend.ctc_batch_cost(labels, base_output, sequence_length, label_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_crossentropy_func(args):  \n",
    "    target, output = args\n",
    "    print(target)\n",
    "#     target_dense = keras.utils.to_categorical(\n",
    "#                         target,\n",
    "#                         num_classes=n_classes+1\n",
    "#                     )\n",
    "#     # Compute cross entropy for each frame.\n",
    "#     cross_entropy = target_dense * tf.log(output)\n",
    "#     cross_entropy = -tf.reduce_sum(cross_entropy, 2)\n",
    "    cross_entropy = keras.backend.sparse_categorical_crossentropy(\n",
    "                        target,\n",
    "                        output,\n",
    "                        from_logits=False,\n",
    "                        axis=-1\n",
    "                    )\n",
    "    print(cross_entropy)\n",
    "    mask = tf.cast(target < n_classes, dtype=tf.float32)\n",
    "    print(mask)\n",
    "    cross_entropy *= mask\n",
    "    # Average over actual sequence lengths.\n",
    "    cross_entropy = tf.reduce_sum(cross_entropy, 1)\n",
    "    cross_entropy /= tf.reduce_sum(mask, 1)\n",
    "    return cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# Baseline Model #\n",
    "##################\n",
    "input_V_tensor = Input(shape=video_tensor_size, name=\"V\")\n",
    "input_F_tensor = Input(shape=(video_tensor_size[0], \n",
    "                              video_tensor_size[1], \n",
    "                              video_tensor_size[2], \n",
    "                              keypoint_size), name=\"F\")\n",
    "\n",
    "labels = Input(shape=(label_seq_size,), name=\"labels\")\n",
    "label_length = Input(shape=(1,), name=\"label_length\")\n",
    "\n",
    "# 224 x 224 x 23\n",
    "input_tensor = Concatenate(axis=-1)([input_V_tensor, input_F_tensor])\n",
    "# input_tensor = input_V_tensor\n",
    "\n",
    "conv_output_tensor = visual_conv_net(input_tensor)\n",
    "\n",
    "# fc_out = TimeDistributed(GlobalAveragePooling2D())(conv_output_tensor)\n",
    "fc_in = TimeDistributed(Flatten())(conv_output_tensor)\n",
    "fc_out = TimeDistributed(Dense(128, activation=\"relu\"))(fc_in)\n",
    "\n",
    "print(conv_output_tensor)\n",
    "print(fc_out)\n",
    "\n",
    "att_seq2seq = AttentionSeq2Seq(input_dim=128, input_length=video_tensor_size[0], \n",
    "                         hidden_dim=128, \n",
    "                         output_length=label_seq_size, \n",
    "                         output_dim=n_classes+1,\n",
    "                         depth=1, dropout=0.3)\n",
    "decoded = att_seq2seq(fc_out)\n",
    "\n",
    "# # LSTM Encoder\n",
    "# encoder_lstm = LSTM(128, return_sequences=True, return_state=True)\n",
    "# encoder_outputs, state_h, state_c = encoder_lstm(fc_out)\n",
    "# encoder_states = [state_h, state_c]\n",
    "\n",
    "# # Sequence Placeholder\n",
    "# # decoder_inputs = Input(shape=(None, n_classes+2))\n",
    "\n",
    "# # LSTM Decoder\n",
    "# decoder_lstm = LSTM(128, return_sequences=True, return_state=True)\n",
    "# decoded, _, _ = decoder_lstm(encoder_outputs, initial_state=encoder_states)\n",
    "\n",
    "decoder_outputs = Dense(n_classes+1, activation='softmax', name='output_sequence')(decoded)\n",
    "# decoder_outputs = Lambda(lambda x:x, name='output_sequence')(decoded)\n",
    "\n",
    "loss = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([decoder_outputs, labels, label_length])\n",
    "ce_loss = Lambda(masked_crossentropy_func, output_shape=(1,), name='masked_ce')([labels, decoder_outputs])\n",
    "\n",
    "model = Model(inputs=[input_V_tensor, input_F_tensor, labels, label_length], outputs=[ce_loss, decoder_outputs])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./sessions/seq2seq\"\n",
    "checkpoints_path = os.path.join(path, 'checkpoints')\n",
    "history_filename = 'history_' + path[path.rindex('/') + 1:] + '.csv'\n",
    "early_stopping_patience = 10\n",
    "\n",
    "if not os.path.exists(\"./sessions\"):\n",
    "    os.mkdir(\"./sessions\")\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)\n",
    "\n",
    "if not os.path.exists(checkpoints_path):\n",
    "    os.mkdir(checkpoints_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(y_true, y_pred):\n",
    "    return y_pred\n",
    "\n",
    "def dummy_loss_func(y_true, y_pred):\n",
    "    return tf.fill([tf.shape(y_true)[0], 1], 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_loss(y_true, y_pred):\n",
    "    y_true_sparse = tf.argmax(y_true, axis=-1)\n",
    "    y_true_valid = tf.cast(y_true_sparse < n_classes, tf.float32)\n",
    "    print(y_true_valid)\n",
    "    print(y_pred)\n",
    "    label_length = tf.reduce_sum(y_true_valid, axis=-1, keepdims=True)\n",
    "    print(label_length)\n",
    "    input_length = label_seq_size * tf.ones((tf.shape(y_pred)[0], 1))\n",
    "    print(input_length)\n",
    "    loss_tensor = tf.keras.backend.ctc_batch_cost(\n",
    "                        y_true_sparse,\n",
    "                        y_pred,\n",
    "                        input_length,\n",
    "                        label_length\n",
    "                    )\n",
    "    return tf.reduce_mean(loss_tensor)\n",
    "\n",
    "def ctc_loss_2(y_true, y_pred):\n",
    "    sparse = tf.contrib.layers.dense_to_sparse(y_true)\n",
    "    input_length = label_seq_size * tf.ones((tf.shape(y_pred)[0], 1))\n",
    "    y_true_valid = tf.cast(y_true_sparse<n_classes, tf.float32)\n",
    "    label_length = tf.reduce_sum(y_true_valid, axis=-1, keepdims=True)\n",
    "    loss_tensor = tf.nn.ctc_loss(\n",
    "            sparse,\n",
    "            y_pred,\n",
    "            label_length,\n",
    "            preprocess_collapse_repeated=False,\n",
    "            ctc_merge_repeated=True,\n",
    "            ignore_longer_outputs_than_inputs=True,\n",
    "            time_major=False\n",
    "        )\n",
    "    return tf.reduce_mean(loss_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_edit_distance(truth, hyp):\n",
    "    truth = tf.reshape(truth, (-1, label_seq_size))\n",
    "    truth = tf.cast(truth, dtype=tf.int64)\n",
    "    truth_idx = tf.where(tf.not_equal(truth, n_classes))\n",
    "    # Use tf.shape(a_t, out_type=tf.int64) instead of a_t.get_shape() if tensor shape is dynamic\n",
    "    truth_sparse = tf.SparseTensor(truth_idx, tf.gather_nd(truth, truth_idx), tf.shape(truth, out_type=tf.int64))\n",
    "    \n",
    "    hyp_dense = tf.argmax(hyp, axis=-1)\n",
    "    hyp_idx = tf.where(tf.not_equal(hyp_dense, n_classes))\n",
    "    # Use tf.shape(a_t, out_type=tf.int64) instead of a_t.get_shape() if tensor shape is dynamic\n",
    "    hyp_sparse = tf.SparseTensor(hyp_idx, tf.gather_nd(hyp_dense, hyp_idx), tf.shape(hyp_dense, out_type=tf.int64))\n",
    "#     tf.contrib.layers.dense_to_sparse(\n",
    "#     tensor,\n",
    "#     eos_token=0,\n",
    "#     outputs_collections=None,\n",
    "#     scope=None\n",
    "# )\n",
    "    print(truth, hyp_dense)\n",
    "    editDist = tf.edit_distance(hyp_sparse, truth_sparse, normalize=True)\n",
    "    return editDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_accuracy(y_true, y_pred):\n",
    "    y_pred_sparse = tf.argmax(y_pred, axis=-1)\n",
    "    return tf.reduce_mean(tf.cast(y_pred_sparse==y_true, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "# model.compile(optimizer=Adam(lr=0.001), \n",
    "#               loss='categorical_crossentropy', metrics=[ctc_loss])\n",
    "\n",
    "model.compile(optimizer=Adam(lr=0.001), loss={'masked_ce': loss_func, \n",
    "                                              'output_sequence':'sparse_categorical_crossentropy'},\n",
    "                                          loss_weights={'masked_ce': 1.0, 'output_sequence':0.0},\n",
    "                                          metrics={'output_sequence':sparse_accuracy, 'output_sequence':test_edit_distance})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks():\n",
    "    return [\n",
    "        keras.callbacks.ReduceLROnPlateau(patience=early_stopping_patience / 2,\n",
    "                                              cooldown=early_stopping_patience / 4,\n",
    "                                              verbose=1),\n",
    "        keras.callbacks.EarlyStopping(patience=early_stopping_patience, verbose=1,\n",
    "                                          monitor='val_loss'),\n",
    "        keras.callbacks.ModelCheckpoint(os.path.join(checkpoints_path, 'checkpoint.{epoch:05d}-{val_loss:.3f}.hdf5')),\n",
    "        keras.callbacks.CSVLogger(os.path.join(path, history_filename), append=True)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(train_generator, \n",
    "          epochs=100, \n",
    "          verbose=1, \n",
    "          callbacks=get_callbacks(), \n",
    "          validation_data=val_generator, \n",
    "          shuffle=True, \n",
    "          initial_epoch=0, \n",
    "          steps_per_epoch=1000, \n",
    "          validation_steps=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('./sessions/seq2seq-ce/checkpoints/checkpoint.00002-0.613.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs, test_output = val_generator.data_generation(val_generator.list_IDs[5:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stats = model.evaluate_generator(val_generator, steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = model.predict_generator(val_generator, steps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = np.argmax(test_results[1], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_preds, test_output[1][:, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-audio-3.6",
   "language": "python",
   "name": "conda-audio-3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
