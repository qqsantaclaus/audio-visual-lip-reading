{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/fs/daps/anaconda2/envs/conda-audio-3.6/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import pickle\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import sys\n",
    "import dlib\n",
    "# import skvideo.io\n",
    "import json\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import glob\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import cmudict\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES']='0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([('AA', 'vowel'), ('AE', 'vowel'), ('AH', 'vowel'), ('AO', 'vowel'), ('AW', 'vowel'), ('AY', 'vowel'), ('B', 'stop'), ('CH', 'affricate'), ('D', 'stop'), ('DH', 'fricative'), ('EH', 'vowel'), ('ER', 'vowel'), ('EY', 'vowel'), ('F', 'fricative'), ('G', 'stop'), ('HH', 'aspirate'), ('IH', 'vowel'), ('IY', 'vowel'), ('JH', 'affricate'), ('K', 'stop'), ('L', 'liquid'), ('M', 'nasal'), ('N', 'nasal'), ('NG', 'nasal'), ('OW', 'vowel'), ('OY', 'vowel'), ('P', 'stop'), ('R', 'liquid'), ('S', 'fricative'), ('SH', 'fricative'), ('T', 'stop'), ('TH', 'fricative'), ('UH', 'vowel'), ('UW', 'vowel'), ('V', 'fricative'), ('W', 'semivowel'), ('Y', 'semivowel'), ('Z', 'fricative'), ('ZH', 'fricative')], {'IY': 17, 'W': 35, 'DH': 9, 'Y': 36, 'HH': 15, 'CH': 7, 'JH': 18, 'ZH': 38, 'EH': 10, 'NG': 23, 'TH': 31, 'AA': 0, 'B': 6, 'AE': 1, 'D': 8, 'G': 14, 'F': 13, 'AH': 2, 'K': 19, 'M': 21, 'L': 20, 'AO': 3, 'N': 22, 'IH': 16, 'S': 28, 'R': 27, 'EY': 12, 'T': 30, 'AW': 4, 'V': 34, 'AY': 5, 'Z': 37, 'ER': 11, 'P': 26, 'UW': 33, 'SH': 29, 'UH': 32, 'OY': 25, 'OW': 24})\n"
     ]
    }
   ],
   "source": [
    "phoneme_list = [] \n",
    "phoneme_dict = {}\n",
    "\n",
    "with open(\"/n/fs/scratch/jiaqis/cmudict-master/cmudict.phones\", 'r') as fp:\n",
    "    i = 0\n",
    "    line = fp.readline()\n",
    "    while line:\n",
    "        phoneme = line.split()[0].strip()\n",
    "        phoneme_property = line.split()[1].strip()\n",
    "        phoneme_list.append((phoneme, phoneme_property))\n",
    "        phoneme_dict[phoneme] = i\n",
    "        line = fp.readline()\n",
    "        i=i+1\n",
    "\n",
    "print(phoneme_list, phoneme_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pron_dict = cmudict.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_pron(pron):\n",
    "    \"\"\"Remove stress from pronunciations.\"\"\"\n",
    "    return re.sub(r\"\\d\", \"\", pron)\n",
    "\n",
    "def make_triphones(pron):\n",
    "    \"\"\"Output triphones from a word's pronunciation.\"\"\"\n",
    "    if len(pron) < 3:\n",
    "        return []\n",
    "    # Junk on end is to make word boundaries work\n",
    "    return ([((pron[idx - 2], pron[idx - 1]), pron[idx])\n",
    "             for idx in range(2, len(pron))] + [(('#', '#'), pron[0])] +\n",
    "            [((pron[-2], pron[-1]), '#')])\n",
    "                                                \n",
    "def triphone_probs(prons):\n",
    "    \"\"\"Calculate triphone probabilities for pronunciations.\"\"\"\n",
    "    context_counts = defaultdict(lambda: defaultdict(int))\n",
    "    for pron in prons:\n",
    "        for (context, phoneme) in make_triphones(pron):\n",
    "            context_counts[context][phoneme] += 1\n",
    "            \n",
    "    for (context, outcomes) in context_counts.items():\n",
    "        total_outcomes = sum(outcomes.values())\n",
    "        for outcome, count in outcomes.items():\n",
    "            context_counts[context][outcome] = float(count) / total_outcomes\n",
    "        \n",
    "    return context_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Volume and Facial Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"/n/fs/scratch/jiaqis/LRS3-TED/\"\n",
    "SAVE_DIR = \"/n/fs/scratch/jiaqis/LRS3-TED-Extracted/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_list(dataDir, setName):\n",
    "    # Images, facial/mouth features, text-> phonetic\n",
    "    data_list = []\n",
    "    for urlDir in glob.glob(os.path.join(dataDir, setName, \"*/\")):\n",
    "        url = urlDir.split('/')[-2]\n",
    "        for idFilename in glob.glob(os.path.join(urlDir, '*.txt')):\n",
    "            index = idFilename.split('/')[-1].split('.')[0]\n",
    "            filepath = os.path.join(dataDir, setName, url, index)\n",
    "            \n",
    "            text = open(filepath+\".txt\", 'r').readline()\n",
    "            words = text[5:].lower().strip().split()\n",
    "            flag = False\n",
    "            for word in words:\n",
    "                if word not in pron_dict:\n",
    "                    flag=True\n",
    "                    break\n",
    "            if flag:\n",
    "                continue\n",
    "            imgfiles = sorted(glob.glob(filepath + \"_*_mouth.jpg\"))\n",
    "            if len(imgfiles) > 100:\n",
    "                continue\n",
    "            \n",
    "            ID = idFilename.split('/')[-1].split('.')[0]\n",
    "            data_list.append((url, ID))\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_ID_list = get_dataset_list(SAVE_DIR, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainval_ID_list = get_dataset_list(SAVE_DIR, \"trainval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json.dump(test_ID_list, open('test_ID_list.json', \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json.dump(trainval_ID_list, open('trainval_ID_list.json', \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ID_list = json.load(open('test_ID_list.json', \"r\"))\n",
    "trainval_ID_list = json.load(open('trainval_ID_list.json', \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(730, 3360)\n"
     ]
    }
   ],
   "source": [
    "print(len(test_ID_list), len(trainval_ID_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "FPS = 25\n",
    "FRAME_ROWS = 120\n",
    "FRAME_COLS = 120\n",
    "NFRAMES = 5 # size of input volume of frames\n",
    "MARGIN = NFRAMES/2\n",
    "COLORS = 1 # grayscale\n",
    "CHANNELS = COLORS*NFRAMES\n",
    "MAX_FRAMES_COUNT= 250 # corresponding to 10 seconds, 25Hz*10\n",
    "\n",
    "EXAMPLE_FILEPATH = \"/n/fs/scratch/jiaqis/LRS3-TED-Extracted/test/0Fi83BHQsMA/00002\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_tensor_size = (100, 120, 120, 3) \n",
    "keypoint_img_size = (224, 224)\n",
    "keypoint_size=20\n",
    "label_seq_size=100\n",
    "n_classes=39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(filepath, img_size, keypoint_img_size, keypoint_size, label_seq_size):\n",
    "    # images\n",
    "    # frames x rows x cols x channels\n",
    "    visual_cube = []\n",
    "    # keypoint features\n",
    "    feature_cube = []\n",
    "    features = json.load(open(filepath + \".json\", 'r'))\n",
    "    # Target Text/phonemes\n",
    "    labels = []\n",
    "    text = open(filepath+\".txt\", 'r').readline()\n",
    "    words = text[5:].lower().strip().split()\n",
    "    for word in words:\n",
    "        word_phonemes = pron_dict[word][0]\n",
    "        word_indices = [phoneme_dict[clean_pron(phon)] for phon in word_phonemes]\n",
    "        labels.extend(word_indices)\n",
    "            \n",
    "    acc = 0\n",
    "    for imgFilename in sorted(glob.glob(filepath + \"_*_mouth.jpg\")):\n",
    "#         if 'mouth' in imgFilename:\n",
    "#             continue\n",
    "        x = image.img_to_array(\n",
    "              image.load_img(imgFilename, target_size=img_size))/255.0\n",
    "#         x = np.expand_dims(x, axis=0)\n",
    "#         x = preprocess_input(x)\n",
    "        visual_cube.append(x)\n",
    "        \n",
    "        mask = np.zeros((keypoint_img_size[0], keypoint_img_size[1], keypoint_size))\n",
    "        framenum = str(int(imgFilename.split(\"_\")[-2].split(\".\")[0]))\n",
    "        f_feature = features[framenum]['mouthCoords']\n",
    "        for ft_index in range(keypoint_size):\n",
    "            # TODO: check range of outputs\n",
    "            keypoint_x = f_feature[ft_index][0] - 1\n",
    "            keypoint_y = f_feature[ft_index][1] - 1\n",
    "            mask[keypoint_y, keypoint_x, ft_index] = 1.0\n",
    "        feature_cube.append(mask)\n",
    "        acc+=1\n",
    "    return np.array(visual_cube), np.array(feature_cube), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_cube, feature_cube, labels = prepare_data(EXAMPLE_FILEPATH, (120, 120, 3), (224, 224), 20, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41, 120, 120, 3)\n",
      "(41, 224, 224, 20)\n",
      "(19,)\n"
     ]
    }
   ],
   "source": [
    "print(visual_cube[:, :, :, :].shape)\n",
    "print(feature_cube.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ## TODO: Not working\n",
    "# import cv2\n",
    "# cv2.imshow( \"Display window\", visual_cube[0, :, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, data_dir, subset, list_IDs, prons, phonemes,\n",
    "                       video_tensor_size=(200, 224, 224, 3), \n",
    "                       keypoint_img_size = (224, 224),\n",
    "                       keypoint_size=20, label_seq_size=90, \n",
    "                       batch_size=32,\n",
    "                       n_classes=39, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.data_dir = data_dir\n",
    "        self.subset = subset\n",
    "        self.video_tensor_size = video_tensor_size\n",
    "        self.img_size = (video_tensor_size[1], video_tensor_size[2], video_tensor_size[3])\n",
    "        self.keypoint_img_size = keypoint_img_size\n",
    "        self.keypoint_size =keypoint_size\n",
    "        self.label_seq_size = label_seq_size\n",
    "        self.batch_size = batch_size\n",
    "        self.list_IDs = list_IDs\n",
    "        self.prons = prons\n",
    "        self.phonemes = phonemes\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.ceil(float(len(self.list_IDs)) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        # Generate data\n",
    "        X, Y = self.data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, Y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        V = np.zeros((len(list_IDs_temp), \n",
    "                      self.video_tensor_size[0],\n",
    "                      self.video_tensor_size[1],\n",
    "                      self.video_tensor_size[2],\n",
    "                      self.video_tensor_size[3]))\n",
    "        F = np.zeros((len(list_IDs_temp), \n",
    "                      self.video_tensor_size[0], \n",
    "                      self.keypoint_img_size[0],\n",
    "                      self.keypoint_img_size[1],\n",
    "                      self.keypoint_size))\n",
    "        # null = self.n_classes\n",
    "        T = self.n_classes * np.ones((len(list_IDs_temp), self.label_seq_size))\n",
    "        T_LEN = np.ones((len(list_IDs_temp), 1))\n",
    "        \n",
    "        acc = 0\n",
    "        for it, v_ID in enumerate(list_IDs_temp):\n",
    "            try:\n",
    "#                 print(v_ID)\n",
    "                v_url, v_index = v_ID\n",
    "                filepath = os.path.join(self.data_dir, self.subset, v_url, v_index)\n",
    "                v_V, v_F, v_T = prepare_data(filepath, self.img_size, self.keypoint_img_size,\n",
    "                                               self.keypoint_size, self.label_seq_size)\n",
    "                if v_V.shape[0] > self.video_tensor_size[0]:\n",
    "                    continue\n",
    "            except Exception as e:\n",
    "#                 print(e)\n",
    "                continue\n",
    "                \n",
    "            num_frames = v_V.shape[0]\n",
    "            V[acc, -1*num_frames:, :, :, :] = v_V \n",
    "            F[acc, -1*num_frames:, :, :, :] = v_F \n",
    "            T[acc, :v_T.shape[0]] = v_T\n",
    "            T_LEN[acc, 0] = len(v_T)\n",
    "            acc+=1\n",
    "        \n",
    "        return [V[:acc], F[:acc], T[:acc], T_LEN[:acc]], \\\n",
    "                [np.zeros_like(T_LEN[:acc]), T[:acc, :, np.newaxis]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator =  DataGenerator(SAVE_DIR, 'trainval', trainval_ID_list, pron_dict, phoneme_dict,\n",
    "                       video_tensor_size=video_tensor_size, \n",
    "                       keypoint_img_size=keypoint_img_size,\n",
    "                       keypoint_size=keypoint_size, \n",
    "                       label_seq_size=label_seq_size, batch_size=1,\n",
    "                       n_classes=n_classes, shuffle=True)\n",
    "\n",
    "val_generator = DataGenerator(SAVE_DIR, 'test', test_ID_list, pron_dict, phoneme_dict,\n",
    "                       video_tensor_size=video_tensor_size, \n",
    "                       keypoint_img_size=keypoint_img_size,\n",
    "                       keypoint_size=keypoint_size, \n",
    "                       label_seq_size=label_seq_size, batch_size=1,\n",
    "                       n_classes=n_classes, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "try_inputs, try_output = train_generator.data_generation(train_generator.list_IDs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "try_V = try_inputs[0]\n",
    "try_F = try_inputs[1]\n",
    "try_T = try_inputs[2]\n",
    "try_T_LEN = try_inputs[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.42156863 -0.4254902  -0.4254902  ...  0.13529414  0.1156863\n",
      "   0.10000002]\n",
      " [-0.42156863 -0.42156863 -0.4254902  ...  0.13921571  0.11960787\n",
      "   0.10392159]\n",
      " [-0.42156863 -0.42156863 -0.4254902  ...  0.13137257  0.1156863\n",
      "   0.10000002]\n",
      " ...\n",
      " [-0.26078433 -0.26078433 -0.26078433 ... -0.42941177 -0.42941177\n",
      "  -0.42941177]\n",
      " [-0.26078433 -0.26078433 -0.26078433 ... -0.42941177 -0.42941177\n",
      "  -0.42941177]\n",
      " [-0.26078433 -0.26078433 -0.26078433 ... -0.42941177 -0.42941177\n",
      "  -0.42941177]]\n"
     ]
    }
   ],
   "source": [
    "print(try_V[2,90, :, :, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, BatchNormalization,ZeroPadding2D, Embedding, LSTM, Bidirectional, Add, Multiply, Activation, Masking, Concatenate\n",
    "from keras.layers import TimeDistributed, GlobalAveragePooling3D, Conv2D, Flatten, Permute, RepeatVector, Lambda, GlobalAveragePooling2D, MaxPooling2D\n",
    "import seq2seq\n",
    "from seq2seq.models import AttentionSeq2Seq, Seq2Seq, SimpleSeq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_block(inputs):\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    input_dim = int(inputs.shape[2])\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Dense(video_tensor_size[0], activation='softmax')(a)\n",
    "    a = Lambda(lambda x: keras.backend.mean(x, axis=1), name='dim_reduction')(a)\n",
    "    a = RepeatVector(input_dim)(a)\n",
    "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "    output_attention_mul = Multiply(name='attention_mul')([inputs, a_probs])\n",
    "    return output_attention_mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visual_conv_net(inputs, maxpool=True):\n",
    "    # 224 x 224 x 64\n",
    "    conv1 = TimeDistributed(Conv2D(64, kernel_size=(3,3), padding='same', \n",
    "                                   activation=\"relu\"))(inputs)\n",
    "    # 112 x 112 x 64\n",
    "    if maxpool:\n",
    "        down1 = TimeDistributed(MaxPooling2D(pool_size=(2, 2)))(conv1)\n",
    "    else:\n",
    "        down1 = TimeDistributed(Conv2D(64, kernel_size=(2,2), \n",
    "                                      strides=(2,2), \n",
    "                                      padding='same', \n",
    "                                      activation=None))(conv1)\n",
    "    # 112 x 112 x 128\n",
    "    conv2 = TimeDistributed(Conv2D(128, (3,3), padding='same', activation=\"relu\"))(down1)\n",
    "    # 56 x 56 x 128\n",
    "    if maxpool:\n",
    "        down2 = TimeDistributed(MaxPooling2D(pool_size=(2, 2)))(conv2)\n",
    "    else:\n",
    "        down2 = TimeDistributed(Conv2D(128, kernel_size=(2,2), \n",
    "                                      strides=(2,2), \n",
    "                                      padding='same', \n",
    "                                      activation=None))(conv2)\n",
    "    # 56 x 56 x 256\n",
    "    conv3 = TimeDistributed(Conv2D(256, (3,3), padding='same', activation=\"relu\"))(down2)\n",
    "    # 28 x 28 x 256\n",
    "    if maxpool:\n",
    "        down3 = TimeDistributed(MaxPooling2D(pool_size=(2, 2)))(conv3)\n",
    "    else:\n",
    "        down3 = TimeDistributed(Conv2D(256, kernel_size=(2, 2), \n",
    "                                      strides=(2,2), \n",
    "                                      padding='same', \n",
    "                                      activation=None))(conv3)\n",
    "    # 28 x 28 x 256\n",
    "    conv4 = TimeDistributed(Conv2D(256, (3,3), padding='same', activation=\"relu\"))(down3)\n",
    "    # 14 x 14 x 256\n",
    "    if maxpool:\n",
    "        down4 = TimeDistributed(MaxPooling2D(pool_size=(2, 2)))(conv4)\n",
    "    else:\n",
    "        down4 = TimeDistributed(Conv2D(256, kernel_size=(2,2), \n",
    "                                      strides=(2,2), \n",
    "                                      padding='same', \n",
    "                                      activation=None))(conv4)\n",
    "    # 14 x 14 x 256\n",
    "    conv5 = TimeDistributed(Conv2D(256, (3,3), padding='same', activation=\"relu\"))(down4)\n",
    "    # 7 x 7 x 256\n",
    "    if maxpool:\n",
    "        down5 = TimeDistributed(MaxPooling2D(pool_size=(2, 2)))(conv5)\n",
    "    else:\n",
    "        down5 = TimeDistributed(Conv2D(256, kernel_size=(2,2), \n",
    "                                      strides=(2,2), \n",
    "                                      padding='same', \n",
    "                                      activation=None))(conv5)\n",
    "    return down5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ctc_lambda_func(args):\n",
    "#     y_pred, labels, input_length, label_length = args\n",
    "#     # the 2 is critical here since the first couple outputs of the RNN\n",
    "#     # tend to be garbage:\n",
    "#     y_pred = y_pred[:, 2:, :]\n",
    "#     return keras.backend.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
    "\n",
    "def ctc_lambda_func(args):\n",
    "    base_output, labels, label_length = args \n",
    "    base_output_shape = tf.shape(base_output)\n",
    "    sequence_length = tf.fill([base_output_shape[0], 1], base_output_shape[1])\n",
    "    print(labels)\n",
    "    print(base_output)\n",
    "    print(sequence_length)\n",
    "    print(label_length)\n",
    "    \n",
    "    return keras.backend.ctc_batch_cost(labels, base_output, sequence_length, label_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_crossentropy_func(args):  \n",
    "    target, output = args\n",
    "    print(target)\n",
    "#     target_dense = keras.utils.to_categorical(\n",
    "#                         target,\n",
    "#                         num_classes=n_classes+1\n",
    "#                     )\n",
    "#     # Compute cross entropy for each frame.\n",
    "#     cross_entropy = target_dense * tf.log(output)\n",
    "#     cross_entropy = -tf.reduce_sum(cross_entropy, 2)\n",
    "    cross_entropy = keras.backend.sparse_categorical_crossentropy(\n",
    "                        target,\n",
    "                        output,\n",
    "                        from_logits=False,\n",
    "                        axis=-1\n",
    "                    )\n",
    "    print(cross_entropy)\n",
    "    mask = tf.cast(target < n_classes, dtype=tf.float32)\n",
    "    print(mask)\n",
    "    cross_entropy *= mask\n",
    "    # Average over actual sequence lengths.\n",
    "    cross_entropy = tf.reduce_sum(cross_entropy, 1)\n",
    "    cross_entropy /= tf.reduce_sum(mask, 1)\n",
    "    return cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an input sequence and process it.\n",
    "\n",
    "input_V_tensor = Input(shape=(None, \n",
    "                              video_tensor_size[1], \n",
    "                              video_tensor_size[2], \n",
    "                              video_tensor_size[3]), name=\"V\")\n",
    "input_F_tensor = Input(shape=(None, \n",
    "                              keypoint_img_size[0], \n",
    "                              keypoint_img_size[1], \n",
    "                              keypoint_size), name=\"F\")\n",
    "\n",
    "labels = Input(shape=(None,), name=\"labels\")\n",
    "\n",
    "label_length = Input(shape=(1,), name=\"label_length\")\n",
    "\n",
    "# 224 x 224 x 23\n",
    "# input_tensor = Concatenate(axis=-1)([input_V_tensor, input_F_tensor])\n",
    "input_tensor = input_V_tensor\n",
    "\n",
    "conv_output_tensor = visual_conv_net(input_tensor)\n",
    "\n",
    "# fc_out = TimeDistributed(GlobalAveragePooling2D())(conv_output_tensor)\n",
    "fc_in = TimeDistributed(Flatten())(conv_output_tensor)\n",
    "fc_out = TimeDistributed(Dense(256, activation=\"relu\"))(fc_in)\n",
    "\n",
    "\n",
    "encoder = LSTM(256, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "decoder_inputs_dense = Lambda(lambda x: keras.utils.to_categorical(\n",
    "                                                x,\n",
    "                                                num_classes=n_classes+1))(decoder_inputs)\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(n_classes+1, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, labels, label_length, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)\n",
    "# Save model\n",
    "model.save('s2s.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"time_distributed_498/Reshape_1:0\", shape=(?, 100, 3, 3, 256), dtype=float32)\n",
      "Tensor(\"time_distributed_500/Reshape_1:0\", shape=(?, 100, 256), dtype=float32)\n",
      "Tensor(\"labels_38:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"output_sequence_27/Reshape_1:0\", shape=(?, 100, 40), dtype=float32)\n",
      "Tensor(\"ctc_33/Fill:0\", shape=(?, 1), dtype=int32)\n",
      "Tensor(\"label_length_38:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"labels_38:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"masked_ce/Reshape_2:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"masked_ce/Cast_1:0\", shape=(?, 100), dtype=float32)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "V (InputLayer)                  (None, 100, 120, 120 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_489 (TimeDistr (None, 100, 120, 120 1792        V[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_490 (TimeDistr (None, 100, 60, 60,  0           time_distributed_489[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_491 (TimeDistr (None, 100, 60, 60,  73856       time_distributed_490[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_492 (TimeDistr (None, 100, 30, 30,  0           time_distributed_491[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_493 (TimeDistr (None, 100, 30, 30,  295168      time_distributed_492[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_494 (TimeDistr (None, 100, 15, 15,  0           time_distributed_493[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_495 (TimeDistr (None, 100, 15, 15,  590080      time_distributed_494[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_496 (TimeDistr (None, 100, 7, 7, 25 0           time_distributed_495[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_497 (TimeDistr (None, 100, 7, 7, 25 590080      time_distributed_496[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_498 (TimeDistr (None, 100, 3, 3, 25 0           time_distributed_497[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_499 (TimeDistr (None, 100, 2304)    0           time_distributed_498[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_500 (TimeDistr (None, 100, 256)     590080      time_distributed_499[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "masking_5 (Masking)             (None, 100, 256)     0           time_distributed_500[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "model_517 (Model)               (None, 100, 40)      1623456     masking_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "labels (InputLayer)             (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "output_sequence (TimeDistribute (None, 100, 40)      1640        model_517[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "masked_ce (Lambda)              (None, 1)            0           labels[0][0]                     \n",
      "                                                                 output_sequence[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 3,766,152\n",
      "Trainable params: 3,766,152\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "# Baseline Model #\n",
    "##################\n",
    "input_V_tensor = Input(shape=video_tensor_size, name=\"V\")\n",
    "input_F_tensor = Input(shape=(video_tensor_size[0], \n",
    "                              keypoint_img_size[0], \n",
    "                              keypoint_img_size[1], \n",
    "                              keypoint_size), name=\"F\")\n",
    "\n",
    "labels = Input(shape=(label_seq_size,), name=\"labels\")\n",
    "label_length = Input(shape=(1,), name=\"label_length\")\n",
    "\n",
    "# 224 x 224 x 23\n",
    "# input_tensor = Concatenate(axis=-1)([input_V_tensor, input_F_tensor])\n",
    "input_tensor = input_V_tensor\n",
    "\n",
    "conv_output_tensor = visual_conv_net(input_tensor)\n",
    "\n",
    "# fc_out = TimeDistributed(GlobalAveragePooling2D())(conv_output_tensor)\n",
    "fc_in = TimeDistributed(Flatten())(conv_output_tensor)\n",
    "fc_out = TimeDistributed(Dense(256, activation=\"relu\"))(fc_in)\n",
    "\n",
    "print(conv_output_tensor)\n",
    "print(fc_out)\n",
    "\n",
    "# att_seq2seq = AttentionSeq2Seq(input_dim=128, input_length=video_tensor_size[0], \n",
    "#                          hidden_dim=128, \n",
    "#                          output_length=label_seq_size, \n",
    "#                          output_dim=n_classes+1,\n",
    "#                          depth=2)\n",
    "# decoded = att_seq2seq(fc_out)\n",
    "\n",
    "# # LSTM Encoder\n",
    "# encoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "# encoder_outputs, state_h, state_c = encoder_lstm(fc_out)\n",
    "# encoder_states = [state_h, state_c]\n",
    "\n",
    "# # Sequence Placeholder\n",
    "# # decoder_inputs = Input(shape=(None, n_classes+2))\n",
    "\n",
    "# # LSTM Decoder\n",
    "# decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "# decoded, _, _ = decoder_lstm(encoder_outputs, initial_state=encoder_states)\n",
    "\n",
    "fc_out = Masking(mask_value=0.0)(fc_out)\n",
    "\n",
    "seq2seq = SimpleSeq2Seq(output_dim=n_classes+1, output_length=label_seq_size,\n",
    "            input_dim=256, input_length=video_tensor_size[0],\n",
    "            hidden_dim=256, depth=2, unroll=False,\n",
    "            stateful=False, dropout=0.3)\n",
    "\n",
    "decoded = seq2seq(fc_out)\n",
    "\n",
    "decoder_outputs = TimeDistributed(Dense(n_classes+1, activation='softmax'), name='output_sequence')(decoded)\n",
    "# decoder_outputs = Lambda(lambda x:x, name='output_sequence')(decoded)\n",
    "\n",
    "loss = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([decoder_outputs, labels, label_length])\n",
    "\n",
    "ce_loss = Lambda(masked_crossentropy_func, output_shape=(1,), name='masked_ce')([labels, decoder_outputs])\n",
    "\n",
    "model = Model(inputs=[input_V_tensor, input_F_tensor, labels, label_length], outputs=[ce_loss, decoder_outputs])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./sessions/simpleseq2seq-mouth-ce\"\n",
    "checkpoints_path = os.path.join(path, 'checkpoints')\n",
    "history_filename = 'history_' + path[path.rindex('/') + 1:] + '.csv'\n",
    "early_stopping_patience = 10\n",
    "\n",
    "if not os.path.exists(\"./sessions\"):\n",
    "    os.mkdir(\"./sessions\")\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)\n",
    "\n",
    "if not os.path.exists(checkpoints_path):\n",
    "    os.mkdir(checkpoints_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(y_true, y_pred):\n",
    "    return y_pred\n",
    "\n",
    "def dummy_loss_func(y_true, y_pred):\n",
    "    return tf.fill([tf.shape(y_true)[0], 1], 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ctc_loss(y_true, y_pred):\n",
    "#     y_true_sparse = tf.argmax(y_true, axis=-1)\n",
    "#     y_true_valid = tf.cast(y_true_sparse < n_classes, tf.float32)\n",
    "#     print(y_true_valid)\n",
    "#     print(y_pred)\n",
    "#     label_length = tf.reduce_sum(y_true_valid, axis=-1, keepdims=True)\n",
    "#     print(label_length)\n",
    "#     input_length = label_seq_size * tf.ones((tf.shape(y_pred)[0], 1))\n",
    "#     print(input_length)\n",
    "#     loss_tensor = tf.keras.backend.ctc_batch_cost(\n",
    "#                         y_true_sparse,\n",
    "#                         y_pred,\n",
    "#                         input_length,\n",
    "#                         label_length\n",
    "#                     )\n",
    "#     return tf.reduce_mean(loss_tensor)\n",
    "\n",
    "# def ctc_loss_2(y_true, y_pred):\n",
    "#     sparse = tf.contrib.layers.dense_to_sparse(y_true)\n",
    "#     input_length = label_seq_size * tf.ones((tf.shape(y_pred)[0], 1))\n",
    "#     y_true_valid = tf.cast(y_true_sparse<n_classes, tf.float32)\n",
    "#     label_length = tf.reduce_sum(y_true_valid, axis=-1, keepdims=True)\n",
    "#     loss_tensor = tf.nn.ctc_loss(\n",
    "#             sparse,\n",
    "#             y_pred,\n",
    "#             label_length,\n",
    "#             preprocess_collapse_repeated=False,\n",
    "#             ctc_merge_repeated=True,\n",
    "#             ignore_longer_outputs_than_inputs=True,\n",
    "#             time_major=False\n",
    "#         )\n",
    "#     return tf.reduce_mean(loss_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_edit_distance(truth, hyp):\n",
    "    truth = tf.reshape(truth, (-1, label_seq_size))\n",
    "    truth = tf.cast(truth, dtype=tf.int64)\n",
    "    truth_idx = tf.where(tf.not_equal(truth, n_classes))\n",
    "    # Use tf.shape(a_t, out_type=tf.int64) instead of a_t.get_shape() if tensor shape is dynamic\n",
    "    truth_sparse = tf.SparseTensor(truth_idx, tf.gather_nd(truth, truth_idx), tf.shape(truth, out_type=tf.int64))\n",
    "    \n",
    "    hyp_dense = tf.argmax(hyp, axis=-1)\n",
    "    hyp_idx = tf.where(tf.not_equal(hyp_dense, n_classes))\n",
    "    # Use tf.shape(a_t, out_type=tf.int64) instead of a_t.get_shape() if tensor shape is dynamic\n",
    "    hyp_sparse = tf.SparseTensor(hyp_idx, tf.gather_nd(hyp_dense, hyp_idx), tf.shape(hyp_dense, out_type=tf.int64))\n",
    "#     tf.contrib.layers.dense_to_sparse(\n",
    "#     tensor,\n",
    "#     eos_token=0,\n",
    "#     outputs_collections=None,\n",
    "#     scope=None\n",
    "# )\n",
    "    print(truth, hyp_dense)\n",
    "    editDist = tf.edit_distance(hyp_sparse, truth_sparse, normalize=True)\n",
    "    return editDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_accuracy(y_true, y_pred):\n",
    "    y_pred_sparse = tf.argmax(y_pred, axis=-1)\n",
    "    return tf.reduce_mean(tf.cast(y_pred_sparse==y_true, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('./sessions/lstm/checkpoints/checkpoint.00002-55.906.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor 'metrics_45/test_edit_distance/Cast:0' shape=(?, 100) dtype=int64>, <tf.Tensor 'metrics_45/test_edit_distance/ArgMax:0' shape=(?, 100) dtype=int64>)\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "# model.compile(optimizer=Adam(lr=0.001), \n",
    "#               loss='categorical_crossentropy', metrics=[ctc_loss])\n",
    "\n",
    "model.compile(optimizer=RMSprop(lr=0.001, clipnorm=200), loss={'masked_ce': loss_func, \n",
    "                                                               'output_sequence':'sparse_categorical_crossentropy'},\n",
    "                                          loss_weights={'masked_ce': 1.0, 'output_sequence':0.0},\n",
    "                                          metrics={'output_sequence':sparse_accuracy, 'output_sequence':test_edit_distance})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks():\n",
    "    return [\n",
    "        keras.callbacks.ReduceLROnPlateau(patience=early_stopping_patience / 2,\n",
    "                                              cooldown=early_stopping_patience / 4,\n",
    "                                              verbose=1),\n",
    "        keras.callbacks.EarlyStopping(patience=early_stopping_patience, verbose=1,\n",
    "                                          monitor='val_loss'),\n",
    "        keras.callbacks.ModelCheckpoint(os.path.join(checkpoints_path, 'checkpoint.{epoch:05d}-{val_loss:.3f}.hdf5')),\n",
    "        keras.callbacks.CSVLogger(os.path.join(path, history_filename), append=True)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "400/400 [==============================] - 1330s 3s/step - loss: 3.3438 - masked_ce_loss: 3.3438 - output_sequence_loss: 13.8255 - output_sequence_test_edit_distance: 6.1400 - val_loss: 3.2909 - val_masked_ce_loss: 3.2909 - val_output_sequence_loss: 13.9575 - val_output_sequence_test_edit_distance: 6.4250\n",
      "Epoch 2/100\n",
      "400/400 [==============================] - 1293s 3s/step - loss: 3.3441 - masked_ce_loss: 3.3441 - output_sequence_loss: 13.9195 - output_sequence_test_edit_distance: 6.4044 - val_loss: 3.3530 - val_masked_ce_loss: 3.3530 - val_output_sequence_loss: 14.1351 - val_output_sequence_test_edit_distance: 6.9783\n",
      "Epoch 3/100\n",
      "400/400 [==============================] - 1303s 3s/step - loss: 3.3624 - masked_ce_loss: 3.3624 - output_sequence_loss: 13.8702 - output_sequence_test_edit_distance: 6.1261 - val_loss: 3.3178 - val_masked_ce_loss: 3.3178 - val_output_sequence_loss: 14.0117 - val_output_sequence_test_edit_distance: 6.7386\n",
      "Epoch 4/100\n",
      "400/400 [==============================] - 1314s 3s/step - loss: 3.3645 - masked_ce_loss: 3.3645 - output_sequence_loss: 13.8836 - output_sequence_test_edit_distance: 6.2748 - val_loss: 3.3453 - val_masked_ce_loss: 3.3453 - val_output_sequence_loss: 14.0214 - val_output_sequence_test_edit_distance: 6.6804\n",
      "Epoch 5/100\n",
      "400/400 [==============================] - 1329s 3s/step - loss: 3.3395 - masked_ce_loss: 3.3395 - output_sequence_loss: 13.9160 - output_sequence_test_edit_distance: 6.4198 - val_loss: 3.3478 - val_masked_ce_loss: 3.3478 - val_output_sequence_loss: 14.0780 - val_output_sequence_test_edit_distance: 6.9404\n",
      "Epoch 6/100\n",
      "400/400 [==============================] - 1305s 3s/step - loss: 3.3352 - masked_ce_loss: 3.3352 - output_sequence_loss: 13.8963 - output_sequence_test_edit_distance: 6.2843 - val_loss: 3.3542 - val_masked_ce_loss: 3.3542 - val_output_sequence_loss: 14.1230 - val_output_sequence_test_edit_distance: 7.0812\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.000999999977648.\n",
      "Epoch 7/100\n",
      "400/400 [==============================] - 1315s 3s/step - loss: 3.3636 - masked_ce_loss: 3.3636 - output_sequence_loss: 13.9045 - output_sequence_test_edit_distance: 6.3046 - val_loss: 3.3169 - val_masked_ce_loss: 3.3169 - val_output_sequence_loss: 14.0215 - val_output_sequence_test_edit_distance: 6.6379\n",
      "Epoch 8/100\n",
      "400/400 [==============================] - 1293s 3s/step - loss: 3.3308 - masked_ce_loss: 3.3308 - output_sequence_loss: 13.9043 - output_sequence_test_edit_distance: 6.4118 - val_loss: 3.3009 - val_masked_ce_loss: 3.3009 - val_output_sequence_loss: 14.0531 - val_output_sequence_test_edit_distance: 6.8080\n",
      "Epoch 9/100\n",
      "400/400 [==============================] - 1303s 3s/step - loss: 3.3328 - masked_ce_loss: 3.3328 - output_sequence_loss: 13.9046 - output_sequence_test_edit_distance: 6.3204 - val_loss: 3.3305 - val_masked_ce_loss: 3.3305 - val_output_sequence_loss: 14.0435 - val_output_sequence_test_edit_distance: 6.6832\n",
      "Epoch 10/100\n",
      "400/400 [==============================] - 1304s 3s/step - loss: 3.3402 - masked_ce_loss: 3.3402 - output_sequence_loss: 13.8680 - output_sequence_test_edit_distance: 6.2241 - val_loss: 3.3391 - val_masked_ce_loss: 3.3391 - val_output_sequence_loss: 14.0230 - val_output_sequence_test_edit_distance: 6.7994\n",
      "Epoch 11/100\n",
      "400/400 [==============================] - 1300s 3s/step - loss: 3.3565 - masked_ce_loss: 3.3565 - output_sequence_loss: 13.8539 - output_sequence_test_edit_distance: 6.1777 - val_loss: 3.3091 - val_masked_ce_loss: 3.3091 - val_output_sequence_loss: 14.1070 - val_output_sequence_test_edit_distance: 6.8797\n",
      "Epoch 00011: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcdf511e310>"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, \n",
    "          epochs=100, \n",
    "          verbose=1, \n",
    "          callbacks=get_callbacks(), \n",
    "          validation_data=val_generator, \n",
    "          shuffle=True, \n",
    "          initial_epoch=0, \n",
    "          steps_per_epoch=400, \n",
    "          validation_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-audio-3.6",
   "language": "python",
   "name": "conda-audio-3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
