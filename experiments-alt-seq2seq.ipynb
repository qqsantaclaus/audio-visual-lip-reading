{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import pickle\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import sys\n",
    "import dlib\n",
    "# import skvideo.io\n",
    "import json\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import glob\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import cmudict\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES']='2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([('AA', 'vowel'), ('AE', 'vowel'), ('AH', 'vowel'), ('AO', 'vowel'), ('AW', 'vowel'), ('AY', 'vowel'), ('B', 'stop'), ('CH', 'affricate'), ('D', 'stop'), ('DH', 'fricative'), ('EH', 'vowel'), ('ER', 'vowel'), ('EY', 'vowel'), ('F', 'fricative'), ('G', 'stop'), ('HH', 'aspirate'), ('IH', 'vowel'), ('IY', 'vowel'), ('JH', 'affricate'), ('K', 'stop'), ('L', 'liquid'), ('M', 'nasal'), ('N', 'nasal'), ('NG', 'nasal'), ('OW', 'vowel'), ('OY', 'vowel'), ('P', 'stop'), ('R', 'liquid'), ('S', 'fricative'), ('SH', 'fricative'), ('T', 'stop'), ('TH', 'fricative'), ('UH', 'vowel'), ('UW', 'vowel'), ('V', 'fricative'), ('W', 'semivowel'), ('Y', 'semivowel'), ('Z', 'fricative'), ('ZH', 'fricative')], {'IY': 17, 'START': 39, 'W': 35, 'DH': 9, 'Y': 36, 'HH': 15, 'CH': 7, 'JH': 18, 'ZH': 38, 'END': 40, 'EH': 10, 'NG': 23, 'TH': 31, 'BLANK': 41, 'AA': 0, 'B': 6, 'AE': 1, 'D': 8, 'G': 14, 'F': 13, 'AH': 2, 'K': 19, 'M': 21, 'L': 20, 'AO': 3, 'N': 22, 'IH': 16, 'S': 28, 'R': 27, 'EY': 12, 'T': 30, 'AW': 4, 'V': 34, 'AY': 5, 'Z': 37, 'ER': 11, 'P': 26, 'UW': 33, 'SH': 29, 'UH': 32, 'OY': 25, 'OW': 24})\n"
     ]
    }
   ],
   "source": [
    "phoneme_list = [] \n",
    "phoneme_dict = {}\n",
    "\n",
    "with open(\"/n/fs/scratch/jiaqis/cmudict-master/cmudict.phones\", 'r') as fp:\n",
    "    i = 0\n",
    "    line = fp.readline()\n",
    "    while line:\n",
    "        phoneme = line.split()[0].strip()\n",
    "        phoneme_property = line.split()[1].strip()\n",
    "        phoneme_list.append((phoneme, phoneme_property))\n",
    "        phoneme_dict[phoneme] = i\n",
    "        line = fp.readline()\n",
    "        i=i+1\n",
    "        \n",
    "phoneme_dict['START'] = 39\n",
    "phoneme_dict[\"END\"] = 40\n",
    "phoneme_dict[\"BLANK\"] = 41\n",
    "print(phoneme_list, phoneme_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pron_dict = cmudict.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_pron(pron):\n",
    "    \"\"\"Remove stress from pronunciations.\"\"\"\n",
    "    return re.sub(r\"\\d\", \"\", pron)\n",
    "\n",
    "def make_triphones(pron):\n",
    "    \"\"\"Output triphones from a word's pronunciation.\"\"\"\n",
    "    if len(pron) < 3:\n",
    "        return []\n",
    "    # Junk on end is to make word boundaries work\n",
    "    return ([((pron[idx - 2], pron[idx - 1]), pron[idx])\n",
    "             for idx in range(2, len(pron))] + [(('#', '#'), pron[0])] +\n",
    "            [((pron[-2], pron[-1]), '#')])\n",
    "                                                \n",
    "def triphone_probs(prons):\n",
    "    \"\"\"Calculate triphone probabilities for pronunciations.\"\"\"\n",
    "    context_counts = defaultdict(lambda: defaultdict(int))\n",
    "    for pron in prons:\n",
    "        for (context, phoneme) in make_triphones(pron):\n",
    "            context_counts[context][phoneme] += 1\n",
    "            \n",
    "    for (context, outcomes) in context_counts.items():\n",
    "        total_outcomes = sum(outcomes.values())\n",
    "        for outcome, count in outcomes.items():\n",
    "            context_counts[context][outcome] = float(count) / total_outcomes\n",
    "        \n",
    "    return context_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Volume and Facial Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"/n/fs/scratch/jiaqis/LRS3-TED/\"\n",
    "SAVE_DIR = \"/n/fs/scratch/jiaqis/LRS3-TED-Extracted/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_list(dataDir, setName):\n",
    "    # Images, facial/mouth features, text-> phonetic\n",
    "    data_list = []\n",
    "    for urlDir in glob.glob(os.path.join(dataDir, setName, \"*/\")):\n",
    "        url = urlDir.split('/')[-2]\n",
    "        for idFilename in glob.glob(os.path.join(urlDir, '*.txt')):\n",
    "            index = idFilename.split('/')[-1].split('.')[0]\n",
    "            filepath = os.path.join(dataDir, setName, url, index)\n",
    "            \n",
    "            text = open(filepath+\".txt\", 'r').readline()\n",
    "            words = text[5:].lower().strip().split()\n",
    "            flag = False\n",
    "            for word in words:\n",
    "                if word not in pron_dict:\n",
    "                    flag=True\n",
    "                    break\n",
    "            if flag:\n",
    "                continue\n",
    "            imgfiles = sorted(glob.glob(filepath + \"_*.jpg\"))\n",
    "            if len(imgfiles) > 100:\n",
    "                continue\n",
    "            \n",
    "            ID = idFilename.split('/')[-1].split('.')[0]\n",
    "            data_list.append((url, ID))\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_ID_list = get_dataset_list(SAVE_DIR, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainval_ID_list = get_dataset_list(SAVE_DIR, \"trainval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json.dump(test_ID_list, open('test_ID_list.json', \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json.dump(trainval_ID_list, open('trainval_ID_list.json', \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ID_list = json.load(open('test_ID_list.json', \"r\"))\n",
    "trainval_ID_list = json.load(open('trainval_ID_list.json', \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(730, 3360)\n"
     ]
    }
   ],
   "source": [
    "print(len(test_ID_list), len(trainval_ID_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "FPS = 25\n",
    "FRAME_ROWS = 120\n",
    "FRAME_COLS = 120\n",
    "NFRAMES = 5 # size of input volume of frames\n",
    "MARGIN = NFRAMES/2\n",
    "COLORS = 1 # grayscale\n",
    "CHANNELS = COLORS*NFRAMES\n",
    "MAX_FRAMES_COUNT= 250 # corresponding to 10 seconds, 25Hz*10\n",
    "\n",
    "EXAMPLE_FILEPATH = \"/n/fs/scratch/jiaqis/LRS3-TED-Extracted/test/0Fi83BHQsMA/00002\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_tensor_size = (100, 120, 120, 3) \n",
    "keypoint_img_size = (224, 224)\n",
    "keypoint_size=20\n",
    "label_seq_size=100\n",
    "n_classes=39\n",
    "num_tokens=n_classes+3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(filepath, img_size, keypoint_img_size, keypoint_size, label_seq_size):\n",
    "    # images\n",
    "    # frames x rows x cols x channels\n",
    "    visual_cube = []\n",
    "    # keypoint features\n",
    "    feature_cube = []\n",
    "    features = json.load(open(filepath + \".json\", 'r'))\n",
    "    # Target Text/phonemes\n",
    "    labels = [phoneme_dict[\"START\"]]\n",
    "    text = open(filepath+\".txt\", 'r').readline()\n",
    "    words = text[5:].lower().strip().split()\n",
    "    for word in words:\n",
    "        word_phonemes = pron_dict[word][0]\n",
    "        word_indices = [phoneme_dict[clean_pron(phon)] for phon in word_phonemes]\n",
    "        labels.extend(word_indices)\n",
    "    labels.append(phoneme_dict[\"END\"])        \n",
    "    acc = 0\n",
    "    for imgFilename in sorted(glob.glob(filepath + \"_*_mouth.jpg\")):\n",
    "#         if 'mouth' in imgFilename:\n",
    "#             continue\n",
    "        x = image.img_to_array(\n",
    "              image.load_img(imgFilename, target_size=img_size))/255.0\n",
    "#         x = np.expand_dims(x, axis=0)\n",
    "#         x = preprocess_input(x)\n",
    "        visual_cube.append(x)\n",
    "        \n",
    "        mask = np.zeros((keypoint_img_size[0], keypoint_img_size[1], keypoint_size))\n",
    "        framenum = str(int(imgFilename.split(\"_\")[-2].split(\".\")[0]))\n",
    "        f_feature = features[framenum]['mouthCoords']\n",
    "        for ft_index in range(keypoint_size):\n",
    "            # TODO: check range of outputs\n",
    "            keypoint_x = f_feature[ft_index][0] - 1\n",
    "            keypoint_y = f_feature[ft_index][1] - 1\n",
    "            mask[keypoint_y, keypoint_x, ft_index] = 1.0\n",
    "        feature_cube.append(mask)\n",
    "        acc+=1\n",
    "    return np.array(visual_cube), np.array(feature_cube), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_cube, feature_cube, labels = prepare_data(EXAMPLE_FILEPATH, (120, 120, 3), (224, 224), 20, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(visual_cube[:, :, :, :].shape)\n",
    "print(feature_cube.shape)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ## TODO: Not working\n",
    "# import cv2\n",
    "# cv2.imshow( \"Display window\", visual_cube[0, :, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, data_dir, subset, list_IDs, prons, phonemes,\n",
    "                       video_tensor_size=(200, 224, 224, 3), \n",
    "                       keypoint_img_size = (224, 224),\n",
    "                       keypoint_size=20, label_seq_size=90, \n",
    "                       batch_size=32,\n",
    "                       n_classes=39, num_tokens=42, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.data_dir = data_dir\n",
    "        self.subset = subset\n",
    "        self.video_tensor_size = video_tensor_size\n",
    "        self.img_size = (video_tensor_size[1], video_tensor_size[2], video_tensor_size[3])\n",
    "        self.keypoint_img_size = keypoint_img_size\n",
    "        self.keypoint_size =keypoint_size\n",
    "        self.label_seq_size = label_seq_size\n",
    "        self.batch_size = batch_size\n",
    "        self.list_IDs = list_IDs\n",
    "        self.prons = prons\n",
    "        self.phonemes = phonemes\n",
    "        self.n_classes = n_classes\n",
    "        self.num_tokens = num_tokens\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.ceil(float(len(self.list_IDs)) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        # Generate data\n",
    "        X, Y = self.data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, Y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        v_ID = list_IDs_temp[0]\n",
    "\n",
    "        v_url, v_index = v_ID\n",
    "        filepath = os.path.join(self.data_dir, self.subset, v_url, v_index)\n",
    "        v_V, v_F, v_T = prepare_data(filepath, self.img_size, self.keypoint_img_size,\n",
    "                                       self.keypoint_size, self.label_seq_size)\n",
    "        num_frames = v_V.shape[0]\n",
    "        \n",
    "        T_LEN = np.ones((1, 1))\n",
    "        T_LEN[0, 0] = len(v_T)-1\n",
    "        \n",
    "#         decoder_T = np.zeros((1, len(v_T)-1, self.num_tokens))\n",
    "#         for i in range(len(v_T)-1):\n",
    "#             decoder_T[0, i, v_T[i]] = 1.0\n",
    "        \n",
    "        return [v_V[np.newaxis,], v_F[np.newaxis,], v_T[np.newaxis, :-1], v_T[np.newaxis,1:], T_LEN], \\\n",
    "                [np.zeros_like(T_LEN), v_T[np.newaxis, 1:, np.newaxis]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator =  DataGenerator(SAVE_DIR, 'trainval', trainval_ID_list, pron_dict, phoneme_dict,\n",
    "                       video_tensor_size=video_tensor_size, \n",
    "                       keypoint_img_size=keypoint_img_size,\n",
    "                       keypoint_size=keypoint_size, \n",
    "                       label_seq_size=label_seq_size, batch_size=1,\n",
    "                       n_classes=n_classes, shuffle=True)\n",
    "\n",
    "val_generator = DataGenerator(SAVE_DIR, 'test', test_ID_list, pron_dict, phoneme_dict,\n",
    "                       video_tensor_size=video_tensor_size, \n",
    "                       keypoint_img_size=keypoint_img_size,\n",
    "                       keypoint_size=keypoint_size, \n",
    "                       label_seq_size=label_seq_size, batch_size=1,\n",
    "                       n_classes=n_classes, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "try_inputs, try_output = train_generator.data_generation(train_generator.list_IDs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try_V = try_inputs[0]\n",
    "try_F = try_inputs[1]\n",
    "try_dT = try_inputs[2]\n",
    "try_T = try_inputs[3]\n",
    "try_T_LEN = try_inputs[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(try_V.shape, try_F.shape, try_dT.shape, try_T.shape, try_T_LEN.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, BatchNormalization,ZeroPadding2D, Embedding, LSTM, Bidirectional, Add, Multiply, Activation, Masking, Concatenate\n",
    "from keras.layers import TimeDistributed, GlobalAveragePooling3D, Conv2D, Flatten, Permute, RepeatVector, Lambda, GlobalAveragePooling2D, MaxPooling2D\n",
    "from keras.layers import Dot\n",
    "import seq2seq\n",
    "from seq2seq.models import AttentionSeq2Seq, Seq2Seq, SimpleSeq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_block(inputs):\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    input_dim = int(inputs.shape[2])\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Dense(video_tensor_size[0], activation='softmax')(a)\n",
    "    a = Lambda(lambda x: keras.backend.mean(x, axis=1), name='dim_reduction')(a)\n",
    "    a = RepeatVector(input_dim)(a)\n",
    "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "    output_attention_mul = Multiply(name='attention_mul')([inputs, a_probs])\n",
    "    return output_attention_mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visual_conv_net(inputs, maxpool=True):\n",
    "    # 224 x 224 x 64\n",
    "    conv1 = TimeDistributed(Conv2D(64, kernel_size=(3,3), padding='same', \n",
    "                                   activation=\"relu\"))(inputs)\n",
    "    # 112 x 112 x 64\n",
    "    if maxpool:\n",
    "        down1 = TimeDistributed(MaxPooling2D(pool_size=(2, 2)))(conv1)\n",
    "    else:\n",
    "        down1 = TimeDistributed(Conv2D(64, kernel_size=(2,2), \n",
    "                                      strides=(2,2), \n",
    "                                      padding='same', \n",
    "                                      activation=None))(conv1)\n",
    "    # 112 x 112 x 128\n",
    "    conv2 = TimeDistributed(Conv2D(128, (3,3), padding='same', activation=\"relu\"))(down1)\n",
    "    # 56 x 56 x 128\n",
    "    if maxpool:\n",
    "        down2 = TimeDistributed(MaxPooling2D(pool_size=(2, 2)))(conv2)\n",
    "    else:\n",
    "        down2 = TimeDistributed(Conv2D(128, kernel_size=(2,2), \n",
    "                                      strides=(2,2), \n",
    "                                      padding='same', \n",
    "                                      activation=None))(conv2)\n",
    "    # 56 x 56 x 256\n",
    "    conv3 = TimeDistributed(Conv2D(256, (3,3), padding='same', activation=\"relu\"))(down2)\n",
    "    # 28 x 28 x 256\n",
    "    if maxpool:\n",
    "        down3 = TimeDistributed(MaxPooling2D(pool_size=(2, 2)))(conv3)\n",
    "    else:\n",
    "        down3 = TimeDistributed(Conv2D(256, kernel_size=(2, 2), \n",
    "                                      strides=(2,2), \n",
    "                                      padding='same', \n",
    "                                      activation=None))(conv3)\n",
    "    # 28 x 28 x 256\n",
    "    conv4 = TimeDistributed(Conv2D(256, (3,3), padding='same', activation=\"relu\"))(down3)\n",
    "    # 14 x 14 x 256\n",
    "    if maxpool:\n",
    "        down4 = TimeDistributed(MaxPooling2D(pool_size=(2, 2)))(conv4)\n",
    "    else:\n",
    "        down4 = TimeDistributed(Conv2D(256, kernel_size=(2,2), \n",
    "                                      strides=(2,2), \n",
    "                                      padding='same', \n",
    "                                      activation=None))(conv4)\n",
    "    # 14 x 14 x 256\n",
    "    conv5 = TimeDistributed(Conv2D(256, (3,3), padding='same', activation=\"relu\"))(down4)\n",
    "    # 7 x 7 x 256\n",
    "    if maxpool:\n",
    "        down5 = TimeDistributed(MaxPooling2D(pool_size=(2, 2)))(conv5)\n",
    "    else:\n",
    "        down5 = TimeDistributed(Conv2D(256, kernel_size=(2,2), \n",
    "                                      strides=(2,2), \n",
    "                                      padding='same', \n",
    "                                      activation=None))(conv5)\n",
    "    return down5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ctc_lambda_func(args):\n",
    "#     y_pred, labels, input_length, label_length = args\n",
    "#     # the 2 is critical here since the first couple outputs of the RNN\n",
    "#     # tend to be garbage:\n",
    "#     y_pred = y_pred[:, 2:, :]\n",
    "#     return keras.backend.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
    "\n",
    "def ctc_lambda_func(args):\n",
    "    base_output, labels, label_length = args \n",
    "    base_output_shape = tf.shape(base_output)\n",
    "    sequence_length = tf.fill([base_output_shape[0], 1], base_output_shape[1])\n",
    "    print(labels)\n",
    "    print(base_output)\n",
    "    print(sequence_length)\n",
    "    print(label_length)\n",
    "    \n",
    "    return keras.backend.ctc_batch_cost(labels, base_output, sequence_length, label_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_crossentropy_func(args):  \n",
    "    target, output = args\n",
    "    print(target)\n",
    "#     target_dense = keras.utils.to_categorical(\n",
    "#                         target,\n",
    "#                         num_classes=n_classes+1\n",
    "#                     )\n",
    "#     # Compute cross entropy for each frame.\n",
    "#     cross_entropy = target_dense * tf.log(output)\n",
    "#     cross_entropy = -tf.reduce_sum(cross_entropy, 2)\n",
    "    cross_entropy = keras.backend.sparse_categorical_crossentropy(\n",
    "                        target,\n",
    "                        output,\n",
    "                        from_logits=False,\n",
    "                        axis=-1\n",
    "                    )\n",
    "    print(cross_entropy)\n",
    "    mask = tf.cast(target < n_classes, dtype=tf.float32)\n",
    "    print(mask)\n",
    "    cross_entropy *= mask\n",
    "    # Average over actual sequence lengths.\n",
    "    cross_entropy = tf.reduce_sum(cross_entropy, 1)\n",
    "    cross_entropy /= tf.reduce_sum(mask, 1)\n",
    "    return cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"time_distributed_139/Reshape_1:0\", shape=(?, ?, 256), dtype=float32)\n",
      "(<keras.layers.wrappers.TimeDistributed object at 0x7f2b3fa44a10>, <tf.Tensor 'time_distributed_140/Reshape_1:0' shape=(?, ?, 256) dtype=float32>)\n",
      "Tensor(\"labels_10:0\", shape=(?, ?), dtype=float32)\n",
      "Tensor(\"output_sequence_9/truediv:0\", shape=(?, ?, 42), dtype=float32)\n",
      "Tensor(\"ctc_6/Fill:0\", shape=(?, 1), dtype=int32)\n",
      "Tensor(\"label_length_10:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"labels_10:0\", shape=(?, ?), dtype=float32)\n",
      "Tensor(\"masked_ce_6/Reshape_2:0\", shape=(?, ?), dtype=float32)\n",
      "Tensor(\"masked_ce_6/Cast_1:0\", shape=(?, ?), dtype=float32)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "V (InputLayer)                  (None, None, 120, 12 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_128 (TimeDistr (None, None, 120, 12 1792        V[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_129 (TimeDistr (None, None, 60, 60, 0           time_distributed_128[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_130 (TimeDistr (None, None, 60, 60, 73856       time_distributed_129[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_131 (TimeDistr (None, None, 30, 30, 0           time_distributed_130[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_132 (TimeDistr (None, None, 30, 30, 295168      time_distributed_131[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_133 (TimeDistr (None, None, 15, 15, 0           time_distributed_132[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_134 (TimeDistr (None, None, 15, 15, 590080      time_distributed_133[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_135 (TimeDistr (None, None, 7, 7, 2 0           time_distributed_134[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_136 (TimeDistr (None, None, 7, 7, 2 590080      time_distributed_135[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_137 (TimeDistr (None, None, 3, 3, 2 0           time_distributed_136[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_138 (TimeDistr (None, None, 2304)   0           time_distributed_137[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "input_27 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_139 (TimeDistr (None, None, 256)    590080      time_distributed_138[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_14 (Embedding)        (None, None, 256)    11008       input_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_19 (LSTM)                  [(None, None, 256),  525312      time_distributed_139[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "lstm_20 (LSTM)                  [(None, None, 256),  525312      embedding_14[0][0]               \n",
      "                                                                 lstm_19[0][1]                    \n",
      "                                                                 lstm_19[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_29 (Dot)                    (None, None, None)   0           lstm_20[0][0]                    \n",
      "                                                                 lstm_19[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, None, None)   0           dot_29[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dot_30 (Dot)                    (None, None, 256)    0           activation_15[0][0]              \n",
      "                                                                 lstm_19[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, None, 512)    0           dot_30[0][0]                     \n",
      "                                                                 lstm_20[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_140 (TimeDistr (None, None, 256)    131328      concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "labels (InputLayer)             (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "output_sequence (Dense)         (None, None, 42)     10794       time_distributed_140[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "masked_ce (Lambda)              (None, 1)            0           labels[0][0]                     \n",
      "                                                                 output_sequence[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 3,344,810\n",
      "Trainable params: 3,344,810\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define an input sequence and process it.\n",
    "input_V_tensor = Input(shape=(None, \n",
    "                              video_tensor_size[1], \n",
    "                              video_tensor_size[2], \n",
    "                              video_tensor_size[3]), name=\"V\")\n",
    "input_F_tensor = Input(shape=(None, \n",
    "                              keypoint_img_size[0], \n",
    "                              keypoint_img_size[1], \n",
    "                              keypoint_size), name=\"F\")\n",
    "\n",
    "labels = Input(shape=(None,), name=\"labels\")\n",
    "\n",
    "label_length = Input(shape=(1,), name=\"label_length\")\n",
    "\n",
    "# 224 x 224 x 23\n",
    "# input_tensor = Concatenate(axis=-1)([input_V_tensor, input_F_tensor])\n",
    "input_tensor = input_V_tensor\n",
    "\n",
    "conv_output_tensor = visual_conv_net(input_tensor)\n",
    "\n",
    "# fc_out = TimeDistributed(GlobalAveragePooling2D())(conv_output_tensor)\n",
    "fc_in = TimeDistributed(Flatten())(conv_output_tensor)\n",
    "fc_out = TimeDistributed(Dense(256, activation=\"relu\"))(fc_in)\n",
    "\n",
    "print(fc_out)\n",
    "\n",
    "encoder, encoder_state_h, encoder_state_c = LSTM(256, return_sequences=True, \n",
    "                                                 return_state=True)(fc_out)\n",
    "encoder_last = encoder[:,-1,:]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "decode_input_embedding = Embedding(num_tokens+1, 256, mask_zero=True)\n",
    "decoder = decode_input_embedding(decoder_inputs)\n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "decoder, _, _ =decoder_lstm(decoder, initial_state=[encoder_state_h, encoder_state_c])\n",
    "# Equation (7) with 'dot' score from Section 3.1 in the paper. \n",
    "# Note that we reuse Softmax-activation layer instead of writing tensor calculation\n",
    "attention = Dot(axes=[2, 2])([decoder, encoder])\n",
    "attention = Activation('softmax')(attention)\n",
    "\n",
    "context = Dot(axes=[2,1])([attention, encoder])\n",
    "decoder_combined_context = Concatenate()([context, decoder])\n",
    "\n",
    "# Has another weight + tanh layer as described in equation (5) of the paper\n",
    "decode_dense =  TimeDistributed(Dense(256, activation=\"tanh\"))\n",
    "decoded = decode_dense(decoder_combined_context)\n",
    "print(decode_dense, decoded)\n",
    "decode_output_dense =  Dense(num_tokens, activation='softmax', name='output_sequence')\n",
    "decoder_outputs = decode_output_dense(decoded)\n",
    "\n",
    "# decoder_outputs = TimeDistributed(Dense(n_classes+1, activation='softmax'), name='output_sequence')(decoded)\n",
    "\n",
    "loss = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([decoder_outputs, labels, label_length])\n",
    "\n",
    "ce_loss = Lambda(masked_crossentropy_func, output_shape=(1,), name='masked_ce')([labels, decoder_outputs])\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([input_V_tensor, input_F_tensor, decoder_inputs, labels, label_length], [ce_loss, decoder_outputs])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ##################\n",
    "# # Baseline Model #\n",
    "# ##################\n",
    "# input_V_tensor = Input(shape=video_tensor_size, name=\"V\")\n",
    "# input_F_tensor = Input(shape=(video_tensor_size[0], \n",
    "#                               keypoint_img_size[0], \n",
    "#                               keypoint_img_size[1], \n",
    "#                               keypoint_size), name=\"F\")\n",
    "\n",
    "# labels = Input(shape=(label_seq_size,), name=\"labels\")\n",
    "# label_length = Input(shape=(1,), name=\"label_length\")\n",
    "\n",
    "# # 224 x 224 x 23\n",
    "# # input_tensor = Concatenate(axis=-1)([input_V_tensor, input_F_tensor])\n",
    "# input_tensor = input_V_tensor\n",
    "\n",
    "# conv_output_tensor = visual_conv_net(input_tensor)\n",
    "\n",
    "# # fc_out = TimeDistributed(GlobalAveragePooling2D())(conv_output_tensor)\n",
    "# fc_in = TimeDistributed(Flatten())(conv_output_tensor)\n",
    "# fc_out = TimeDistributed(Dense(256, activation=\"relu\"))(fc_in)\n",
    "\n",
    "# print(conv_output_tensor)\n",
    "# print(fc_out)\n",
    "\n",
    "# # att_seq2seq = AttentionSeq2Seq(input_dim=128, input_length=video_tensor_size[0], \n",
    "# #                          hidden_dim=128, \n",
    "# #                          output_length=label_seq_size, \n",
    "# #                          output_dim=n_classes+1,\n",
    "# #                          depth=2)\n",
    "# # decoded = att_seq2seq(fc_out)\n",
    "\n",
    "# # # LSTM Encoder\n",
    "# # encoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "# # encoder_outputs, state_h, state_c = encoder_lstm(fc_out)\n",
    "# # encoder_states = [state_h, state_c]\n",
    "\n",
    "# # # Sequence Placeholder\n",
    "# # # decoder_inputs = Input(shape=(None, n_classes+2))\n",
    "\n",
    "# # # LSTM Decoder\n",
    "# # decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "# # decoded, _, _ = decoder_lstm(encoder_outputs, initial_state=encoder_states)\n",
    "\n",
    "# fc_out = Masking(mask_value=0.0)(fc_out)\n",
    "\n",
    "# seq2seq = SimpleSeq2Seq(output_dim=n_classes+1, output_length=label_seq_size,\n",
    "#             input_dim=256, input_length=video_tensor_size[0],\n",
    "#             hidden_dim=256, depth=2, unroll=False,\n",
    "#             stateful=False, dropout=0.3)\n",
    "\n",
    "# decoded = seq2seq(fc_out)\n",
    "\n",
    "# decoder_outputs = TimeDistributed(Dense(n_classes+1, activation='softmax'), name='output_sequence')(decoded)\n",
    "# # decoder_outputs = Lambda(lambda x:x, name='output_sequence')(decoded)\n",
    "\n",
    "# loss = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([decoder_outputs, labels, label_length])\n",
    "\n",
    "# ce_loss = Lambda(masked_crossentropy_func, output_shape=(1,), name='masked_ce')([labels, decoder_outputs])\n",
    "\n",
    "# model = Model(inputs=[input_V_tensor, input_F_tensor, labels, label_length], outputs=[ce_loss, decoder_outputs])\n",
    "\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./sessions/altseq2srq-mouthtrue-ce\"\n",
    "checkpoints_path = os.path.join(path, 'checkpoints')\n",
    "history_filename = 'history_' + path[path.rindex('/') + 1:] + '.csv'\n",
    "early_stopping_patience = 10\n",
    "\n",
    "if not os.path.exists(\"./sessions\"):\n",
    "    os.mkdir(\"./sessions\")\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)\n",
    "\n",
    "if not os.path.exists(checkpoints_path):\n",
    "    os.mkdir(checkpoints_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('./sessions/altseq2srq-mouthtrue-ce/checkpoints/checkpoint.00052-1.939.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('./sessions/lstm/checkpoints/checkpoint.00002-55.906.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(y_true, y_pred):\n",
    "    return y_pred\n",
    "\n",
    "def dummy_loss_func(y_true, y_pred):\n",
    "    return tf.fill([tf.shape(y_true)[0], 1], 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ctc_loss(y_true, y_pred):\n",
    "#     y_true_sparse = tf.argmax(y_true, axis=-1)\n",
    "#     y_true_valid = tf.cast(y_true_sparse < n_classes, tf.float32)\n",
    "#     print(y_true_valid)\n",
    "#     print(y_pred)\n",
    "#     label_length = tf.reduce_sum(y_true_valid, axis=-1, keepdims=True)\n",
    "#     print(label_length)\n",
    "#     input_length = label_seq_size * tf.ones((tf.shape(y_pred)[0], 1))\n",
    "#     print(input_length)\n",
    "#     loss_tensor = tf.keras.backend.ctc_batch_cost(\n",
    "#                         y_true_sparse,\n",
    "#                         y_pred,\n",
    "#                         input_length,\n",
    "#                         label_length\n",
    "#                     )\n",
    "#     return tf.reduce_mean(loss_tensor)\n",
    "\n",
    "# def ctc_loss_2(y_true, y_pred):\n",
    "#     sparse = tf.contrib.layers.dense_to_sparse(y_true)\n",
    "#     input_length = label_seq_size * tf.ones((tf.shape(y_pred)[0], 1))\n",
    "#     y_true_valid = tf.cast(y_true_sparse<n_classes, tf.float32)\n",
    "#     label_length = tf.reduce_sum(y_true_valid, axis=-1, keepdims=True)\n",
    "#     loss_tensor = tf.nn.ctc_loss(\n",
    "#             sparse,\n",
    "#             y_pred,\n",
    "#             label_length,\n",
    "#             preprocess_collapse_repeated=False,\n",
    "#             ctc_merge_repeated=True,\n",
    "#             ignore_longer_outputs_than_inputs=True,\n",
    "#             time_major=False\n",
    "#         )\n",
    "#     return tf.reduce_mean(loss_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_edit_distance(truth, hyp):\n",
    "    truth = tf.reshape(truth,(tf.shape(truth)[0], tf.shape(truth)[1]))\n",
    "    truth = tf.cast(truth, dtype=tf.int64)\n",
    "    truth_idx = tf.where(tf.not_equal(truth, num_tokens))\n",
    "    # Use tf.shape(a_t, out_type=tf.int64) instead of a_t.get_shape() if tensor shape is dynamic\n",
    "    truth_sparse = tf.SparseTensor(truth_idx, tf.gather_nd(truth, truth_idx), tf.shape(truth, out_type=tf.int64))\n",
    "    \n",
    "    hyp_dense = tf.argmax(hyp, axis=-1)\n",
    "    hyp_idx = tf.where(tf.not_equal(hyp_dense, num_tokens))\n",
    "    # Use tf.shape(a_t, out_type=tf.int64) instead of a_t.get_shape() if tensor shape is dynamic\n",
    "    hyp_sparse = tf.SparseTensor(hyp_idx, tf.gather_nd(hyp_dense, hyp_idx), tf.shape(hyp_dense, out_type=tf.int64))\n",
    "#     tf.contrib.layers.dense_to_sparse(\n",
    "#     tensor,\n",
    "#     eos_token=0,\n",
    "#     outputs_collections=None,\n",
    "#     scope=None\n",
    "# )\n",
    "    print(truth, hyp_dense)\n",
    "    editDist = tf.edit_distance(hyp_sparse, truth_sparse, normalize=True)\n",
    "    return editDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_accuracy(y_true, y_pred):\n",
    "    y_pred_sparse = tf.argmax(y_pred, axis=-1)\n",
    "    return tf.reduce_mean(tf.cast(y_pred_sparse==y_true, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor 'metrics/test_edit_distance/Cast:0' shape=(?, ?) dtype=int64>, <tf.Tensor 'metrics/test_edit_distance/ArgMax:0' shape=(?, ?) dtype=int64>)\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "# model.compile(optimizer=Adam(lr=0.001), \n",
    "#               loss='categorical_crossentropy', metrics=[ctc_loss])\n",
    "\n",
    "model.compile(optimizer=RMSprop(lr=0.001, clipnorm=200), loss={'masked_ce': loss_func, \n",
    "                                                               'output_sequence':'sparse_categorical_crossentropy'},\n",
    "                                          loss_weights={'masked_ce': 0.0, 'output_sequence':1.0},\n",
    "                                          metrics={'output_sequence':sparse_accuracy, 'output_sequence':test_edit_distance})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks():\n",
    "    return [\n",
    "        keras.callbacks.ReduceLROnPlateau(patience=early_stopping_patience / 2,\n",
    "                                              cooldown=early_stopping_patience / 4,\n",
    "                                              verbose=1),\n",
    "        keras.callbacks.EarlyStopping(patience=early_stopping_patience, verbose=1,\n",
    "                                          monitor='val_loss'),\n",
    "        keras.callbacks.ModelCheckpoint(os.path.join(checkpoints_path, 'checkpoint.{epoch:05d}-{val_loss:.3f}.hdf5')),\n",
    "        keras.callbacks.CSVLogger(os.path.join(path, history_filename), append=True)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "400/400 [==============================] - 271s 677ms/step - loss: 3.1480 - masked_ce_loss: 3.2339 - output_sequence_loss: 3.1480 - output_sequence_test_edit_distance: 0.8417 - val_loss: 2.8635 - val_masked_ce_loss: 2.9371 - val_output_sequence_loss: 2.8635 - val_output_sequence_test_edit_distance: 0.7875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/fs/daps/anaconda2/envs/conda-audio-3.6/lib/python2.7/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_10 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_9/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_9/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "400/400 [==============================] - 249s 623ms/step - loss: 2.7903 - masked_ce_loss: 2.8928 - output_sequence_loss: 2.7903 - output_sequence_test_edit_distance: 0.7796 - val_loss: 2.7550 - val_masked_ce_loss: 2.8614 - val_output_sequence_loss: 2.7550 - val_output_sequence_test_edit_distance: 0.7710\n",
      "Epoch 3/100\n",
      "400/400 [==============================] - 259s 648ms/step - loss: 2.7025 - masked_ce_loss: 2.8030 - output_sequence_loss: 2.7025 - output_sequence_test_edit_distance: 0.7521 - val_loss: 2.5931 - val_masked_ce_loss: 2.6814 - val_output_sequence_loss: 2.5931 - val_output_sequence_test_edit_distance: 0.7345\n",
      "Epoch 4/100\n",
      "400/400 [==============================] - 263s 658ms/step - loss: 2.6154 - masked_ce_loss: 2.7115 - output_sequence_loss: 2.6154 - output_sequence_test_edit_distance: 0.7300 - val_loss: 2.6167 - val_masked_ce_loss: 2.7018 - val_output_sequence_loss: 2.6167 - val_output_sequence_test_edit_distance: 0.7454\n",
      "Epoch 5/100\n",
      "400/400 [==============================] - 271s 678ms/step - loss: 2.5574 - masked_ce_loss: 2.6556 - output_sequence_loss: 2.5574 - output_sequence_test_edit_distance: 0.7179 - val_loss: 2.5829 - val_masked_ce_loss: 2.6802 - val_output_sequence_loss: 2.5829 - val_output_sequence_test_edit_distance: 0.7216\n",
      "Epoch 6/100\n",
      "400/400 [==============================] - 268s 670ms/step - loss: 2.4967 - masked_ce_loss: 2.6109 - output_sequence_loss: 2.4967 - output_sequence_test_edit_distance: 0.7086 - val_loss: 2.4252 - val_masked_ce_loss: 2.5135 - val_output_sequence_loss: 2.4252 - val_output_sequence_test_edit_distance: 0.6740\n",
      "Epoch 7/100\n",
      "400/400 [==============================] - 256s 639ms/step - loss: 2.4634 - masked_ce_loss: 2.5674 - output_sequence_loss: 2.4634 - output_sequence_test_edit_distance: 0.6909 - val_loss: 2.3265 - val_masked_ce_loss: 2.4150 - val_output_sequence_loss: 2.3265 - val_output_sequence_test_edit_distance: 0.6642\n",
      "Epoch 8/100\n",
      "400/400 [==============================] - 248s 619ms/step - loss: 2.4242 - masked_ce_loss: 2.5340 - output_sequence_loss: 2.4242 - output_sequence_test_edit_distance: 0.6759 - val_loss: 2.3400 - val_masked_ce_loss: 2.4713 - val_output_sequence_loss: 2.3400 - val_output_sequence_test_edit_distance: 0.6646\n",
      "Epoch 9/100\n",
      "400/400 [==============================] - 263s 658ms/step - loss: 2.3315 - masked_ce_loss: 2.4377 - output_sequence_loss: 2.3315 - output_sequence_test_edit_distance: 0.6583 - val_loss: 2.3293 - val_masked_ce_loss: 2.4200 - val_output_sequence_loss: 2.3293 - val_output_sequence_test_edit_distance: 0.6525\n",
      "Epoch 10/100\n",
      "400/400 [==============================] - 256s 639ms/step - loss: 2.3038 - masked_ce_loss: 2.4163 - output_sequence_loss: 2.3038 - output_sequence_test_edit_distance: 0.6532 - val_loss: 2.2075 - val_masked_ce_loss: 2.2875 - val_output_sequence_loss: 2.2075 - val_output_sequence_test_edit_distance: 0.6158\n",
      "Epoch 11/100\n",
      "400/400 [==============================] - 255s 637ms/step - loss: 2.2535 - masked_ce_loss: 2.3700 - output_sequence_loss: 2.2535 - output_sequence_test_edit_distance: 0.6421 - val_loss: 2.2076 - val_masked_ce_loss: 2.3201 - val_output_sequence_loss: 2.2076 - val_output_sequence_test_edit_distance: 0.6278\n",
      "Epoch 12/100\n",
      "400/400 [==============================] - 254s 635ms/step - loss: 2.2479 - masked_ce_loss: 2.3545 - output_sequence_loss: 2.2479 - output_sequence_test_edit_distance: 0.6317 - val_loss: 2.3386 - val_masked_ce_loss: 2.4730 - val_output_sequence_loss: 2.3386 - val_output_sequence_test_edit_distance: 0.6591\n",
      "Epoch 13/100\n",
      "400/400 [==============================] - 265s 662ms/step - loss: 2.1723 - masked_ce_loss: 2.2863 - output_sequence_loss: 2.1723 - output_sequence_test_edit_distance: 0.6208 - val_loss: 2.1731 - val_masked_ce_loss: 2.2845 - val_output_sequence_loss: 2.1731 - val_output_sequence_test_edit_distance: 0.6117\n",
      "Epoch 14/100\n",
      "400/400 [==============================] - 261s 653ms/step - loss: 2.2210 - masked_ce_loss: 2.3480 - output_sequence_loss: 2.2210 - output_sequence_test_edit_distance: 0.6303 - val_loss: 2.2641 - val_masked_ce_loss: 2.3735 - val_output_sequence_loss: 2.2641 - val_output_sequence_test_edit_distance: 0.6189\n",
      "Epoch 15/100\n",
      "400/400 [==============================] - 258s 646ms/step - loss: 2.2481 - masked_ce_loss: 2.3647 - output_sequence_loss: 2.2481 - output_sequence_test_edit_distance: 0.6282 - val_loss: 2.2422 - val_masked_ce_loss: 2.3877 - val_output_sequence_loss: 2.2422 - val_output_sequence_test_edit_distance: 0.6253\n",
      "Epoch 16/100\n",
      "400/400 [==============================] - 262s 655ms/step - loss: 2.1902 - masked_ce_loss: 2.3171 - output_sequence_loss: 2.1902 - output_sequence_test_edit_distance: 0.6151 - val_loss: 2.2624 - val_masked_ce_loss: 2.4297 - val_output_sequence_loss: 2.2624 - val_output_sequence_test_edit_distance: 0.6355\n",
      "Epoch 17/100\n",
      "400/400 [==============================] - 258s 645ms/step - loss: 2.1466 - masked_ce_loss: 2.2596 - output_sequence_loss: 2.1466 - output_sequence_test_edit_distance: 0.6113 - val_loss: 2.1380 - val_masked_ce_loss: 2.2929 - val_output_sequence_loss: 2.1380 - val_output_sequence_test_edit_distance: 0.5994\n",
      "Epoch 18/100\n",
      "400/400 [==============================] - 262s 654ms/step - loss: 2.0361 - masked_ce_loss: 2.1663 - output_sequence_loss: 2.0361 - output_sequence_test_edit_distance: 0.5885 - val_loss: 2.1055 - val_masked_ce_loss: 2.2340 - val_output_sequence_loss: 2.1055 - val_output_sequence_test_edit_distance: 0.5815\n",
      "Epoch 19/100\n",
      "400/400 [==============================] - 263s 656ms/step - loss: 2.0658 - masked_ce_loss: 2.1865 - output_sequence_loss: 2.0658 - output_sequence_test_edit_distance: 0.5874 - val_loss: 2.0049 - val_masked_ce_loss: 2.1254 - val_output_sequence_loss: 2.0049 - val_output_sequence_test_edit_distance: 0.5670\n",
      "Epoch 20/100\n",
      "400/400 [==============================] - 258s 644ms/step - loss: 2.0076 - masked_ce_loss: 2.1225 - output_sequence_loss: 2.0076 - output_sequence_test_edit_distance: 0.5780 - val_loss: 2.1282 - val_masked_ce_loss: 2.2693 - val_output_sequence_loss: 2.1282 - val_output_sequence_test_edit_distance: 0.6017\n",
      "Epoch 21/100\n",
      "400/400 [==============================] - 243s 608ms/step - loss: 2.0355 - masked_ce_loss: 2.1756 - output_sequence_loss: 2.0355 - output_sequence_test_edit_distance: 0.5823 - val_loss: 2.1743 - val_masked_ce_loss: 2.3346 - val_output_sequence_loss: 2.1743 - val_output_sequence_test_edit_distance: 0.6145\n",
      "Epoch 22/100\n",
      "400/400 [==============================] - 251s 627ms/step - loss: 2.0188 - masked_ce_loss: 2.1504 - output_sequence_loss: 2.0188 - output_sequence_test_edit_distance: 0.5775 - val_loss: 1.9797 - val_masked_ce_loss: 2.1433 - val_output_sequence_loss: 1.9797 - val_output_sequence_test_edit_distance: 0.5646\n",
      "Epoch 23/100\n",
      "400/400 [==============================] - 246s 614ms/step - loss: 2.0325 - masked_ce_loss: 2.1608 - output_sequence_loss: 2.0325 - output_sequence_test_edit_distance: 0.5813 - val_loss: 2.0133 - val_masked_ce_loss: 2.1197 - val_output_sequence_loss: 2.0133 - val_output_sequence_test_edit_distance: 0.5653\n",
      "Epoch 24/100\n",
      "400/400 [==============================] - 256s 640ms/step - loss: 2.0085 - masked_ce_loss: 2.1302 - output_sequence_loss: 2.0085 - output_sequence_test_edit_distance: 0.5807 - val_loss: 2.1410 - val_masked_ce_loss: 2.2699 - val_output_sequence_loss: 2.1410 - val_output_sequence_test_edit_distance: 0.6100\n",
      "Epoch 25/100\n",
      "400/400 [==============================] - 272s 680ms/step - loss: 2.0504 - masked_ce_loss: 2.1847 - output_sequence_loss: 2.0504 - output_sequence_test_edit_distance: 0.5828 - val_loss: 1.9549 - val_masked_ce_loss: 2.1373 - val_output_sequence_loss: 1.9549 - val_output_sequence_test_edit_distance: 0.5836\n",
      "Epoch 26/100\n",
      "400/400 [==============================] - 218s 546ms/step - loss: 1.8832 - masked_ce_loss: 2.0213 - output_sequence_loss: 1.8832 - output_sequence_test_edit_distance: 0.5506 - val_loss: 2.2246 - val_masked_ce_loss: 2.3485 - val_output_sequence_loss: 2.2246 - val_output_sequence_test_edit_distance: 0.6036\n",
      "Epoch 27/100\n",
      "400/400 [==============================] - 261s 653ms/step - loss: 1.8325 - masked_ce_loss: 1.9708 - output_sequence_loss: 1.8325 - output_sequence_test_edit_distance: 0.5389 - val_loss: 2.0683 - val_masked_ce_loss: 2.2060 - val_output_sequence_loss: 2.0683 - val_output_sequence_test_edit_distance: 0.5798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100\n",
      "400/400 [==============================] - 261s 652ms/step - loss: 1.8835 - masked_ce_loss: 2.0250 - output_sequence_loss: 1.8835 - output_sequence_test_edit_distance: 0.5466 - val_loss: 2.0489 - val_masked_ce_loss: 2.1797 - val_output_sequence_loss: 2.0489 - val_output_sequence_test_edit_distance: 0.5631\n",
      "Epoch 29/100\n",
      "400/400 [==============================] - 257s 643ms/step - loss: 1.8640 - masked_ce_loss: 1.9990 - output_sequence_loss: 1.8640 - output_sequence_test_edit_distance: 0.5378 - val_loss: 2.0224 - val_masked_ce_loss: 2.1892 - val_output_sequence_loss: 2.0224 - val_output_sequence_test_edit_distance: 0.5684\n",
      "Epoch 30/100\n",
      "400/400 [==============================] - 270s 676ms/step - loss: 1.9015 - masked_ce_loss: 2.0312 - output_sequence_loss: 1.9015 - output_sequence_test_edit_distance: 0.5517 - val_loss: 2.1078 - val_masked_ce_loss: 2.2523 - val_output_sequence_loss: 2.1078 - val_output_sequence_test_edit_distance: 0.5968\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 31/100\n",
      "400/400 [==============================] - 260s 649ms/step - loss: 1.8221 - masked_ce_loss: 1.9515 - output_sequence_loss: 1.8221 - output_sequence_test_edit_distance: 0.5380 - val_loss: 2.0263 - val_masked_ce_loss: 2.1473 - val_output_sequence_loss: 2.0263 - val_output_sequence_test_edit_distance: 0.5890\n",
      "Epoch 32/100\n",
      "400/400 [==============================] - 267s 668ms/step - loss: 1.7909 - masked_ce_loss: 1.9214 - output_sequence_loss: 1.7909 - output_sequence_test_edit_distance: 0.5197 - val_loss: 1.8958 - val_masked_ce_loss: 2.0380 - val_output_sequence_loss: 1.8958 - val_output_sequence_test_edit_distance: 0.5400\n",
      "Epoch 33/100\n",
      "400/400 [==============================] - 256s 639ms/step - loss: 1.7709 - masked_ce_loss: 1.8979 - output_sequence_loss: 1.7709 - output_sequence_test_edit_distance: 0.5123 - val_loss: 1.9869 - val_masked_ce_loss: 2.1095 - val_output_sequence_loss: 1.9869 - val_output_sequence_test_edit_distance: 0.5772\n",
      "Epoch 34/100\n",
      "400/400 [==============================] - 256s 639ms/step - loss: 1.7065 - masked_ce_loss: 1.8385 - output_sequence_loss: 1.7065 - output_sequence_test_edit_distance: 0.5066 - val_loss: 1.9005 - val_masked_ce_loss: 2.0485 - val_output_sequence_loss: 1.9005 - val_output_sequence_test_edit_distance: 0.5430\n",
      "Epoch 35/100\n",
      "400/400 [==============================] - 270s 676ms/step - loss: 1.5729 - masked_ce_loss: 1.7182 - output_sequence_loss: 1.5729 - output_sequence_test_edit_distance: 0.4626 - val_loss: 1.9836 - val_masked_ce_loss: 2.1395 - val_output_sequence_loss: 1.9836 - val_output_sequence_test_edit_distance: 0.5649\n",
      "Epoch 36/100\n",
      "400/400 [==============================] - 271s 679ms/step - loss: 1.5927 - masked_ce_loss: 1.7439 - output_sequence_loss: 1.5927 - output_sequence_test_edit_distance: 0.4679 - val_loss: 2.0666 - val_masked_ce_loss: 2.2026 - val_output_sequence_loss: 2.0666 - val_output_sequence_test_edit_distance: 0.5777\n",
      "Epoch 37/100\n",
      "400/400 [==============================] - 271s 678ms/step - loss: 1.5947 - masked_ce_loss: 1.7422 - output_sequence_loss: 1.5947 - output_sequence_test_edit_distance: 0.4618 - val_loss: 2.1246 - val_masked_ce_loss: 2.2842 - val_output_sequence_loss: 2.1246 - val_output_sequence_test_edit_distance: 0.5998\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 38/100\n",
      "400/400 [==============================] - 273s 684ms/step - loss: 1.5950 - masked_ce_loss: 1.7426 - output_sequence_loss: 1.5950 - output_sequence_test_edit_distance: 0.4681 - val_loss: 1.9539 - val_masked_ce_loss: 2.1015 - val_output_sequence_loss: 1.9539 - val_output_sequence_test_edit_distance: 0.5505\n",
      "Epoch 39/100\n",
      "400/400 [==============================] - 259s 648ms/step - loss: 1.5837 - masked_ce_loss: 1.7128 - output_sequence_loss: 1.5837 - output_sequence_test_edit_distance: 0.4726 - val_loss: 2.0006 - val_masked_ce_loss: 2.1164 - val_output_sequence_loss: 2.0006 - val_output_sequence_test_edit_distance: 0.5716\n",
      "Epoch 40/100\n",
      "400/400 [==============================] - 264s 660ms/step - loss: 1.5662 - masked_ce_loss: 1.7071 - output_sequence_loss: 1.5662 - output_sequence_test_edit_distance: 0.4589 - val_loss: 1.8988 - val_masked_ce_loss: 2.0080 - val_output_sequence_loss: 1.8988 - val_output_sequence_test_edit_distance: 0.5286\n",
      "Epoch 41/100\n",
      "400/400 [==============================] - 274s 684ms/step - loss: 1.5948 - masked_ce_loss: 1.7471 - output_sequence_loss: 1.5948 - output_sequence_test_edit_distance: 0.4736 - val_loss: 1.9963 - val_masked_ce_loss: 2.1432 - val_output_sequence_loss: 1.9963 - val_output_sequence_test_edit_distance: 0.5598\n",
      "Epoch 42/100\n",
      "400/400 [==============================] - 264s 660ms/step - loss: 1.6010 - masked_ce_loss: 1.7354 - output_sequence_loss: 1.6010 - output_sequence_test_edit_distance: 0.4680 - val_loss: 1.8471 - val_masked_ce_loss: 1.9825 - val_output_sequence_loss: 1.8471 - val_output_sequence_test_edit_distance: 0.5379\n",
      "Epoch 43/100\n",
      "400/400 [==============================] - 256s 640ms/step - loss: 1.5583 - masked_ce_loss: 1.6966 - output_sequence_loss: 1.5583 - output_sequence_test_edit_distance: 0.4601 - val_loss: 2.0452 - val_masked_ce_loss: 2.2033 - val_output_sequence_loss: 2.0452 - val_output_sequence_test_edit_distance: 0.5830\n",
      "Epoch 44/100\n",
      "400/400 [==============================] - 244s 610ms/step - loss: 1.5412 - masked_ce_loss: 1.6827 - output_sequence_loss: 1.5412 - output_sequence_test_edit_distance: 0.4522 - val_loss: 2.1184 - val_masked_ce_loss: 2.2785 - val_output_sequence_loss: 2.1184 - val_output_sequence_test_edit_distance: 0.5865\n",
      "Epoch 45/100\n",
      "400/400 [==============================] - 260s 651ms/step - loss: 1.5675 - masked_ce_loss: 1.7211 - output_sequence_loss: 1.5675 - output_sequence_test_edit_distance: 0.4541 - val_loss: 2.0666 - val_masked_ce_loss: 2.2151 - val_output_sequence_loss: 2.0666 - val_output_sequence_test_edit_distance: 0.5801\n",
      "Epoch 46/100\n",
      "400/400 [==============================] - 252s 631ms/step - loss: 1.5479 - masked_ce_loss: 1.6851 - output_sequence_loss: 1.5479 - output_sequence_test_edit_distance: 0.4564 - val_loss: 1.9970 - val_masked_ce_loss: 2.1219 - val_output_sequence_loss: 1.9970 - val_output_sequence_test_edit_distance: 0.5566\n",
      "Epoch 47/100\n",
      "400/400 [==============================] - 260s 649ms/step - loss: 1.5368 - masked_ce_loss: 1.6854 - output_sequence_loss: 1.5368 - output_sequence_test_edit_distance: 0.4559 - val_loss: 2.0346 - val_masked_ce_loss: 2.1762 - val_output_sequence_loss: 2.0346 - val_output_sequence_test_edit_distance: 0.5783\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 48/100\n",
      "400/400 [==============================] - 254s 636ms/step - loss: 1.5549 - masked_ce_loss: 1.7029 - output_sequence_loss: 1.5549 - output_sequence_test_edit_distance: 0.4568 - val_loss: 1.9501 - val_masked_ce_loss: 2.0753 - val_output_sequence_loss: 1.9501 - val_output_sequence_test_edit_distance: 0.5356\n",
      "Epoch 49/100\n",
      "400/400 [==============================] - 265s 663ms/step - loss: 1.5503 - masked_ce_loss: 1.6829 - output_sequence_loss: 1.5503 - output_sequence_test_edit_distance: 0.4564 - val_loss: 1.8905 - val_masked_ce_loss: 2.0367 - val_output_sequence_loss: 1.8905 - val_output_sequence_test_edit_distance: 0.5391\n",
      "Epoch 50/100\n",
      "400/400 [==============================] - 255s 637ms/step - loss: 1.5450 - masked_ce_loss: 1.6963 - output_sequence_loss: 1.5450 - output_sequence_test_edit_distance: 0.4575 - val_loss: 1.8602 - val_masked_ce_loss: 1.9993 - val_output_sequence_loss: 1.8602 - val_output_sequence_test_edit_distance: 0.5361\n",
      "Epoch 51/100\n",
      "400/400 [==============================] - 269s 672ms/step - loss: 1.5426 - masked_ce_loss: 1.6832 - output_sequence_loss: 1.5426 - output_sequence_test_edit_distance: 0.4515 - val_loss: 1.9900 - val_masked_ce_loss: 2.1257 - val_output_sequence_loss: 1.9900 - val_output_sequence_test_edit_distance: 0.5668\n",
      "Epoch 52/100\n",
      "400/400 [==============================] - 280s 699ms/step - loss: 1.5423 - masked_ce_loss: 1.6897 - output_sequence_loss: 1.5423 - output_sequence_test_edit_distance: 0.4585 - val_loss: 1.9391 - val_masked_ce_loss: 2.0841 - val_output_sequence_loss: 1.9391 - val_output_sequence_test_edit_distance: 0.5548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00052: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2b212eda90>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, \n",
    "          epochs=100, \n",
    "          verbose=1, \n",
    "          callbacks=get_callbacks(), \n",
    "          validation_data=val_generator, \n",
    "          shuffle=True, \n",
    "          initial_epoch=0, \n",
    "          steps_per_epoch=400, \n",
    "          validation_steps=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor 'concatenate_16/concat:0' shape=(?, ?, 512) dtype=float32>, <tf.Tensor 'dot_32/MatMul:0' shape=(?, ?, 256) dtype=float32>)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Graph disconnected: cannot obtain value for tensor Tensor(\"V_10:0\", shape=(?, ?, 120, 120, 3), dtype=float32) at layer \"V\". The following previous layers were accessed without issue: []",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-aba7385e8b28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m decoder_model = Model(\n\u001b[1;32m     37\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0mdecoder_inputs\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdecoder_states_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     [decoder_outputs] + decoder_states)\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# Reverse-lookup token index to decode sequences back to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/n/fs/daps/anaconda2/envs/conda-audio-3.6/lib/python2.7/site-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/n/fs/daps/anaconda2/envs/conda-audio-3.6/lib/python2.7/site-packages/keras/engine/network.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 'inputs' in kwargs and 'outputs' in kwargs):\n\u001b[1;32m     92\u001b[0m             \u001b[0;31m# Graph network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;31m# Subclassed network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/n/fs/daps/anaconda2/envs/conda-audio-3.6/lib/python2.7/site-packages/keras/engine/network.pyc\u001b[0m in \u001b[0;36m_init_graph_network\u001b[0;34m(self, inputs, outputs, name)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;31m# Keep track of the network's nodes and layers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         nodes, nodes_by_depth, layers, layers_by_depth = _map_graph_network(\n\u001b[0;32m--> 231\u001b[0;31m             self.inputs, self.outputs)\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_network_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nodes_by_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnodes_by_depth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/n/fs/daps/anaconda2/envs/conda-audio-3.6/lib/python2.7/site-packages/keras/engine/network.pyc\u001b[0m in \u001b[0;36m_map_graph_network\u001b[0;34m(inputs, outputs)\u001b[0m\n\u001b[1;32m   1441\u001b[0m                                          \u001b[0;34m'The following previous layers '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1442\u001b[0m                                          \u001b[0;34m'were accessed without issue: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                          str(layers_with_complete_input))\n\u001b[0m\u001b[1;32m   1444\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_tensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m                     \u001b[0mcomputable_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Graph disconnected: cannot obtain value for tensor Tensor(\"V_10:0\", shape=(?, ?, 120, 120, 3), dtype=float32) at layer \"V\". The following previous layers were accessed without issue: []"
     ]
    }
   ],
   "source": [
    "# Next: inference mode (sampling).\n",
    "# Here's the drill:\n",
    "# 1) encode input and retrieve initial decoder state\n",
    "# 2) run one step of decoder with this initial state\n",
    "# and a \"start of sequence\" token as target.\n",
    "# Output will be the next target token\n",
    "# 3) Repeat with the current target token and current states\n",
    "\n",
    "# Define sampling models\n",
    "latent_dim = 256\n",
    "\n",
    "encoder_model = Model([input_V_tensor, input_F_tensor, decoder_inputs, labels, label_length], \n",
    "                      [encoder, encoder_state_h, encoder_state_c])\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "\n",
    "embed_decoder_inputs = decode_input_embedding(decoder_inputs)\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    embed_decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "\n",
    "attention = Dot(axes=[2, 2])([decoder_outputs, encoder])\n",
    "attention = Activation('softmax')(attention)\n",
    "\n",
    "context = Dot(axes=[2, 1])([attention, encoder])\n",
    "decoder_combined_context = Concatenate()([context, decoder_outputs])\n",
    "\n",
    "print(decoder_combined_context, context)\n",
    "# Has another weight + tanh layer as described in equation (5) of the paper\n",
    "decoded = decode_dense(decoder_combined_context) # equation (5) of the paper\n",
    "decoder_outputs = decode_output_dense(decoded)\n",
    "\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (index, key) for key, index in phoneme_dict.iteritems())\n",
    "reverse_target_char_index = dict(\n",
    "    (index, key) for key, index in phoneme_dict.iteritems())\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, phoneme_dict['START']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == 'END' or\n",
    "           len(decoded_sentence) > label_seq_size):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "for seq_index in range(10):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    test_inputs, test_output = val_generator.data_generation(val_generator.list_IDs[seq_index:seq_index+1])\n",
    "    decoded_sentence = decode_sequence(test_inputs)\n",
    "    print('-')\n",
    "    print('Input sentence:', [reverse_input_char_index[ph_id] for ph_id in list(test_inputs[3][0])])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(encoder_input):\n",
    "    encoder_input = transform(input_encoding, [text.lower()], INPUT_LENGTH)\n",
    "    decoder_input = np.zeros(shape=(len(encoder_input), OUTPUT_LENGTH))\n",
    "    decoder_input[:,0] = START_CHAR_CODE\n",
    "    for i in range(1, OUTPUT_LENGTH):\n",
    "        output = model.predict([encoder_input, decoder_input]).argmax(axis=2)\n",
    "        decoder_input[:,i] = output[:,i]\n",
    "    return decoder_input[:,1:]\n",
    "\n",
    "def decode(decoding, sequence):\n",
    "    text = ''\n",
    "    for i in sequence:\n",
    "        if i == 0:\n",
    "            break\n",
    "        text += output_decoding[i]\n",
    "    return text\n",
    "\n",
    "def to_katakana(text):\n",
    "    decoder_output = generate(text)\n",
    "    return decode(output_decoding, decoder_output[0])\n",
    "\n",
    "...\n",
    "to_katakana('Banana')           # バナナ\n",
    "to_katakana('Peter Parker')     # ピーター・パーカー\n",
    "to_katakana('Jon Snow')         # ジョン・スノー"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-audio-3.6",
   "language": "python",
   "name": "conda-audio-3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
