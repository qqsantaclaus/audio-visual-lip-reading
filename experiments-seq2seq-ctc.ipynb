{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/fs/daps/anaconda2/envs/conda-audio-3.6/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/fs/daps/anaconda2/envs/conda-audio-3.6/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import pickle\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import sys\n",
    "import dlib\n",
    "# import skvideo.io\n",
    "import json\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import glob\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import cmudict\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue May  7 18:17:35 2019       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 410.48                 Driver Version: 410.48                    |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla K80           Off  | 00000000:04:00.0 Off |                    0 |\r\n",
      "| N/A   58C    P0   104W / 149W |  10961MiB / 11441MiB |     68%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  Tesla K80           Off  | 00000000:05:00.0 Off |                    0 |\r\n",
      "| N/A   43C    P0    77W / 149W |  10961MiB / 11441MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   2  Tesla K80           Off  | 00000000:83:00.0 Off |                    0 |\r\n",
      "| N/A   57C    P0    58W / 149W |  10961MiB / 11441MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   3  Tesla K80           Off  | 00000000:84:00.0 Off |                    0 |\r\n",
      "| N/A   43C    P0    73W / 149W |      0MiB / 11441MiB |     64%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0     25304      C   ...aconda2/envs/conda-audio-3.6/bin/python 10948MiB |\r\n",
      "|    1      1344      C   ...aconda2/envs/conda-audio-3.6/bin/python 10948MiB |\r\n",
      "|    2     32580      C   ...aconda2/envs/conda-audio-3.6/bin/python 10948MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES']='3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([('AA', 'vowel'), ('AE', 'vowel'), ('AH', 'vowel'), ('AO', 'vowel'), ('AW', 'vowel'), ('AY', 'vowel'), ('B', 'stop'), ('CH', 'affricate'), ('D', 'stop'), ('DH', 'fricative'), ('EH', 'vowel'), ('ER', 'vowel'), ('EY', 'vowel'), ('F', 'fricative'), ('G', 'stop'), ('HH', 'aspirate'), ('IH', 'vowel'), ('IY', 'vowel'), ('JH', 'affricate'), ('K', 'stop'), ('L', 'liquid'), ('M', 'nasal'), ('N', 'nasal'), ('NG', 'nasal'), ('OW', 'vowel'), ('OY', 'vowel'), ('P', 'stop'), ('R', 'liquid'), ('S', 'fricative'), ('SH', 'fricative'), ('T', 'stop'), ('TH', 'fricative'), ('UH', 'vowel'), ('UW', 'vowel'), ('V', 'fricative'), ('W', 'semivowel'), ('Y', 'semivowel'), ('Z', 'fricative'), ('ZH', 'fricative')], {'IY': 17, 'W': 35, 'DH': 9, 'Y': 36, 'HH': 15, 'CH': 7, 'JH': 18, 'ZH': 38, 'EH': 10, 'NG': 23, 'TH': 31, 'AA': 0, 'B': 6, 'AE': 1, 'D': 8, 'G': 14, 'F': 13, 'AH': 2, 'K': 19, 'M': 21, 'L': 20, 'AO': 3, 'N': 22, 'IH': 16, 'S': 28, 'R': 27, 'EY': 12, 'T': 30, 'AW': 4, 'V': 34, 'AY': 5, 'Z': 37, 'ER': 11, 'P': 26, 'UW': 33, 'SH': 29, 'UH': 32, 'OY': 25, 'OW': 24})\n"
     ]
    }
   ],
   "source": [
    "phoneme_list = [] \n",
    "phoneme_dict = {}\n",
    "\n",
    "with open(\"/n/fs/scratch/jiaqis/cmudict-master/cmudict.phones\", 'r') as fp:\n",
    "    i = 0\n",
    "    line = fp.readline()\n",
    "    while line:\n",
    "        phoneme = line.split()[0].strip()\n",
    "        phoneme_property = line.split()[1].strip()\n",
    "        phoneme_list.append((phoneme, phoneme_property))\n",
    "        phoneme_dict[phoneme] = i\n",
    "        line = fp.readline()\n",
    "        i=i+1\n",
    "\n",
    "print(phoneme_list, phoneme_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pron_dict = cmudict.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_pron(pron):\n",
    "    \"\"\"Remove stress from pronunciations.\"\"\"\n",
    "    return re.sub(r\"\\d\", \"\", pron)\n",
    "\n",
    "def make_triphones(pron):\n",
    "    \"\"\"Output triphones from a word's pronunciation.\"\"\"\n",
    "    if len(pron) < 3:\n",
    "        return []\n",
    "    # Junk on end is to make word boundaries work\n",
    "    return ([((pron[idx - 2], pron[idx - 1]), pron[idx])\n",
    "             for idx in range(2, len(pron))] + [(('#', '#'), pron[0])] +\n",
    "            [((pron[-2], pron[-1]), '#')])\n",
    "                                                \n",
    "def triphone_probs(prons):\n",
    "    \"\"\"Calculate triphone probabilities for pronunciations.\"\"\"\n",
    "    context_counts = defaultdict(lambda: defaultdict(int))\n",
    "    for pron in prons:\n",
    "        for (context, phoneme) in make_triphones(pron):\n",
    "            context_counts[context][phoneme] += 1\n",
    "            \n",
    "    for (context, outcomes) in context_counts.items():\n",
    "        total_outcomes = sum(outcomes.values())\n",
    "        for outcome, count in outcomes.items():\n",
    "            context_counts[context][outcome] = float(count) / total_outcomes\n",
    "        \n",
    "    return context_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Volume and Facial Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"/n/fs/scratch/jiaqis/LRS3-TED/\"\n",
    "SAVE_DIR = \"/n/fs/scratch/jiaqis/LRS3-TED-Extracted/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_list(dataDir, setName):\n",
    "    # Images, facial/mouth features, text-> phonetic\n",
    "    data_list = []\n",
    "    for urlDir in glob.glob(os.path.join(dataDir, setName, \"*/\")):\n",
    "        url = urlDir.split('/')[-2]\n",
    "        for idFilename in glob.glob(os.path.join(urlDir, '*.txt')):\n",
    "            index = idFilename.split('/')[-1].split('.')[0]\n",
    "            filepath = os.path.join(dataDir, setName, url, index)\n",
    "            \n",
    "            text = open(filepath+\".txt\", 'r').readline()\n",
    "            words = text[5:].lower().strip().split()\n",
    "            flag = False\n",
    "            for word in words:\n",
    "                if word not in pron_dict:\n",
    "                    flag=True\n",
    "                    break\n",
    "            if flag:\n",
    "                continue\n",
    "            imgfiles = sorted(glob.glob(filepath + \"_*.jpg\"))\n",
    "            if len(imgfiles) > 100:\n",
    "                continue\n",
    "            \n",
    "            ID = idFilename.split('/')[-1].split('.')[0]\n",
    "            data_list.append((url, ID))\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_ID_list = get_dataset_list(SAVE_DIR, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainval_ID_list = get_dataset_list(SAVE_DIR, \"trainval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json.dump(test_ID_list, open('test_ID_list.json', \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json.dump(trainval_ID_list, open('trainval_ID_list.json', \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ID_list = json.load(open('test_ID_list.json', \"r\"))\n",
    "trainval_ID_list = json.load(open('trainval_ID_list.json', \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(730, 3360)\n"
     ]
    }
   ],
   "source": [
    "print(len(test_ID_list), len(trainval_ID_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "FPS = 25\n",
    "FRAME_ROWS = 120\n",
    "FRAME_COLS = 120\n",
    "NFRAMES = 5 # size of input volume of frames\n",
    "MARGIN = NFRAMES/2\n",
    "COLORS = 1 # grayscale\n",
    "CHANNELS = COLORS*NFRAMES\n",
    "MAX_FRAMES_COUNT= 250 # corresponding to 10 seconds, 25Hz*10\n",
    "\n",
    "EXAMPLE_FILEPATH = \"/n/fs/scratch/jiaqis/LRS3-TED-Extracted/test/0Fi83BHQsMA/00002\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_tensor_size = (100, 224, 224, 3) \n",
    "keypoint_size=20\n",
    "label_seq_size=100\n",
    "n_classes=39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(filepath, img_size, keypoint_size, label_seq_size):\n",
    "    # images\n",
    "    # frames x rows x cols x channels\n",
    "    visual_cube = []\n",
    "    # keypoint features\n",
    "    feature_cube = []\n",
    "    features = json.load(open(filepath + \".json\", 'r'))\n",
    "    # Target Text/phonemes\n",
    "    labels = []\n",
    "    text = open(filepath+\".txt\", 'r').readline()\n",
    "    words = text[5:].lower().strip().split()\n",
    "    for word in words:\n",
    "        word_phonemes = pron_dict[word][0]\n",
    "        word_indices = [phoneme_dict[clean_pron(phon)] for phon in word_phonemes]\n",
    "        labels.extend(word_indices)\n",
    "            \n",
    "    acc = 0\n",
    "    for imgFilename in sorted(glob.glob(filepath + \"_*.jpg\")):\n",
    "        if 'mouth' in imgFilename:\n",
    "            continue\n",
    "        x = image.img_to_array(\n",
    "              image.load_img(imgFilename, target_size=img_size))/255.0\n",
    "#         x = np.expand_dims(x, axis=0)\n",
    "#         x = preprocess_input(x)\n",
    "        visual_cube.append(x)\n",
    "        \n",
    "        mask = np.zeros((img_size[0], img_size[1], keypoint_size))\n",
    "        framenum = str(int(imgFilename.split(\"_\")[-1].split(\".\")[0]))\n",
    "        f_feature = features[framenum]['mouthCoords']\n",
    "        for ft_index in range(keypoint_size):\n",
    "            # TODO: check range of outputs\n",
    "            keypoint_x = f_feature[ft_index][0] - 1\n",
    "            keypoint_y = f_feature[ft_index][1] - 1\n",
    "            mask[keypoint_y, keypoint_x, ft_index] = 1.0\n",
    "        feature_cube.append(mask)\n",
    "        acc+=1\n",
    "    return np.array(visual_cube), np.array(feature_cube), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_cube, feature_cube, labels = prepare_data(EXAMPLE_FILEPATH, (224, 224, 3), 20, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41, 224, 224, 3)\n",
      "(41, 224, 224, 20)\n",
      "(19,)\n"
     ]
    }
   ],
   "source": [
    "print(visual_cube[:, :, :, :].shape)\n",
    "print(feature_cube.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOAAAADgCAAAAAA/RjU9AAAAfUlEQVR4nO3XMQ4AIQgEQP7/aa67QpMr9YCZTis2S0yMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAovL2AMf0SpqfR8qq3mT1+YmItcbcr5qaErJdzndF2yUDLvKiNKVYfmfGUs75UQAAAAAAAAAAAAAAAAAAAAAAAAAAcMgDgjMT7fkqJMcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## TODO: Not working\n",
    "import PIL.Image\n",
    "from cStringIO import StringIO\n",
    "import IPython.display\n",
    "import numpy as np\n",
    "def showarray(a, fmt='png'):\n",
    "    a = np.uint8(a * 255.0)\n",
    "    f = StringIO()\n",
    "    PIL.Image.fromarray(a).save(f, fmt)\n",
    "    IPython.display.display(IPython.display.Image(data=f.getvalue()))\n",
    "\n",
    "sub_cube = np.sum(feature_cube[10, :, :, :], axis=-1)\n",
    "mix_cube = np.minimum((np.mean(visual_cube[10, :, :, :], axis=-1)) * 0.5\n",
    "                + np.sum(feature_cube[10, :, :, :], axis=-1), 1.0)\n",
    "showarray(mix_cube)\n",
    "showarray(sub_cube)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, data_dir, subset, list_IDs, prons, phonemes,\n",
    "                       video_tensor_size=(200, 224, 224, 3), keypoint_size=20, label_seq_size=90, \n",
    "                       batch_size=32,\n",
    "                       n_classes=39, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.data_dir = data_dir\n",
    "        self.subset = subset\n",
    "        self.video_tensor_size = video_tensor_size\n",
    "        self.img_size = (video_tensor_size[1], video_tensor_size[2], video_tensor_size[3])\n",
    "        self.keypoint_size =keypoint_size\n",
    "        self.label_seq_size = label_seq_size\n",
    "        self.batch_size = batch_size\n",
    "        self.list_IDs = list_IDs\n",
    "        self.prons = prons\n",
    "        self.phonemes = phonemes\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.ceil(float(len(self.list_IDs)) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        # Generate data\n",
    "        X, Y = self.data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, Y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        V = np.zeros((len(list_IDs_temp), \n",
    "                      self.video_tensor_size[0],\n",
    "                      self.video_tensor_size[1],\n",
    "                      self.video_tensor_size[2],\n",
    "                      self.video_tensor_size[3]))\n",
    "        F = np.zeros((len(list_IDs_temp), \n",
    "                      self.video_tensor_size[0], \n",
    "                      self.video_tensor_size[1],\n",
    "                      self.video_tensor_size[2],\n",
    "                      self.keypoint_size))\n",
    "        # null = self.n_classes\n",
    "        T = self.n_classes * np.ones((len(list_IDs_temp), self.label_seq_size))\n",
    "        T_LEN = np.ones((len(list_IDs_temp), 1))\n",
    "        \n",
    "        acc = 0\n",
    "        for it, v_ID in enumerate(list_IDs_temp):\n",
    "#             try:\n",
    "#                 print(v_ID)\n",
    "            v_url, v_index = v_ID\n",
    "            filepath = os.path.join(self.data_dir, self.subset, v_url, v_index)\n",
    "            v_V, v_F, v_T = prepare_data(filepath, self.img_size, \n",
    "                                           self.keypoint_size, self.label_seq_size)\n",
    "#                 if v_V.shape[0] > self.video_tensor_size[0]:\n",
    "#                     continue\n",
    "#             except Exception as e:\n",
    "# #                 print(e)\n",
    "#                 continue\n",
    "                \n",
    "            num_frames = v_V.shape[0]\n",
    "            V[acc, -1*num_frames:, :, :, :] = v_V \n",
    "            F[acc, -1*num_frames:, :, :, :] = v_F \n",
    "            T[acc, :v_T.shape[0]] = v_T\n",
    "            T_LEN[acc, 0] = len(v_T)\n",
    "            acc+=1\n",
    "        \n",
    "        return [V[:acc], F[:acc], T[:acc], T_LEN[:acc]], \\\n",
    "                [np.zeros_like(T_LEN[:acc]), T[:acc,:, np.newaxis]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator =  DataGenerator(SAVE_DIR, 'trainval', trainval_ID_list, pron_dict, phoneme_dict,\n",
    "                       video_tensor_size=video_tensor_size, keypoint_size=keypoint_size, \n",
    "                       label_seq_size=label_seq_size, batch_size=1,\n",
    "                       n_classes=n_classes, shuffle=True)\n",
    "\n",
    "val_generator = DataGenerator(SAVE_DIR, 'test', test_ID_list, pron_dict, phoneme_dict,\n",
    "                       video_tensor_size=video_tensor_size, keypoint_size=keypoint_size, \n",
    "                       label_seq_size=label_seq_size, batch_size=1,\n",
    "                       n_classes=n_classes, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "try_inputs, try_output = train_generator.data_generation(train_generator.list_IDs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "try_V = try_inputs[0]\n",
    "try_F = try_inputs[1]\n",
    "try_T = try_inputs[2]\n",
    "try_T_LEN = try_inputs[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.07843138 0.07450981 0.07450981 ... 0.63529414 0.6156863  0.60000002]\n",
      " [0.07843138 0.07843138 0.07450981 ... 0.63921571 0.61960787 0.60392159]\n",
      " [0.07843138 0.07843138 0.07450981 ... 0.63137257 0.6156863  0.60000002]\n",
      " ...\n",
      " [0.23921569 0.23921569 0.23921569 ... 0.07058824 0.07058824 0.07058824]\n",
      " [0.23921569 0.23921569 0.23921569 ... 0.07058824 0.07058824 0.07058824]\n",
      " [0.23921569 0.23921569 0.23921569 ... 0.07058824 0.07058824 0.07058824]]\n"
     ]
    }
   ],
   "source": [
    "print(try_V[2,90, :, :, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, BatchNormalization,ZeroPadding2D, Embedding, LSTM, Bidirectional, Add, Multiply, Activation, Masking, Concatenate\n",
    "from keras.layers import TimeDistributed, GlobalAveragePooling3D, Conv2D, Flatten, Permute, RepeatVector, Lambda, GlobalAveragePooling2D, MaxPooling2D\n",
    "import seq2seq\n",
    "from seq2seq.models import AttentionSeq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_block(inputs):\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    input_dim = int(inputs.shape[2])\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Dense(video_tensor_size[0], activation='softmax')(a)\n",
    "    a = Lambda(lambda x: keras.backend.mean(x, axis=1), name='dim_reduction')(a)\n",
    "    a = RepeatVector(input_dim)(a)\n",
    "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "    output_attention_mul = Multiply(name='attention_mul')([inputs, a_probs])\n",
    "    return output_attention_mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visual_conv_net(inputs, maxpool=True):\n",
    "    # 224 x 224 x 64\n",
    "    conv1 = TimeDistributed(Conv2D(32, kernel_size=(3,3), padding='same', \n",
    "                                   activation=\"relu\"))(inputs)\n",
    "    # 112 x 112 x 64\n",
    "    if maxpool:\n",
    "        down1 = TimeDistributed(MaxPooling2D(pool_size=(2, 2)))(conv1)\n",
    "    else:\n",
    "        down1 = TimeDistributed(Conv2D(32, kernel_size=(2,2), \n",
    "                                      strides=(2,2), \n",
    "                                      padding='same', \n",
    "                                      activation=None))(conv1)\n",
    "    # 112 x 112 x 128\n",
    "    conv2 = TimeDistributed(Conv2D(64, (3,3), padding='same', activation=\"relu\"))(down1)\n",
    "    # 56 x 56 x 128\n",
    "    if maxpool:\n",
    "        down2 = TimeDistributed(MaxPooling2D(pool_size=(2, 2)))(conv2)\n",
    "    else:\n",
    "        down2 = TimeDistributed(Conv2D(64, kernel_size=(2,2), \n",
    "                                      strides=(2,2), \n",
    "                                      padding='same', \n",
    "                                      activation=None))(conv2)\n",
    "    # 56 x 56 x 256\n",
    "    conv3 = TimeDistributed(Conv2D(128, (3,3), padding='same', activation=\"relu\"))(down2)\n",
    "    # 28 x 28 x 256\n",
    "    if maxpool:\n",
    "        down3 = TimeDistributed(MaxPooling2D(pool_size=(2, 2)))(conv3)\n",
    "    else:\n",
    "        down3 = TimeDistributed(Conv2D(128, kernel_size=(2, 2), \n",
    "                                      strides=(2,2), \n",
    "                                      padding='same', \n",
    "                                      activation=None))(conv3)\n",
    "    # 28 x 28 x 256\n",
    "    conv4 = TimeDistributed(Conv2D(128, (3,3), padding='same', activation=\"relu\"))(down3)\n",
    "    # 14 x 14 x 256\n",
    "    if maxpool:\n",
    "        down4 = TimeDistributed(MaxPooling2D(pool_size=(2, 2)))(conv4)\n",
    "    else:\n",
    "        down4 = TimeDistributed(Conv2D(128, kernel_size=(2,2), \n",
    "                                      strides=(2,2), \n",
    "                                      padding='same', \n",
    "                                      activation=None))(conv4)\n",
    "    # 14 x 14 x 256\n",
    "    conv5 = TimeDistributed(Conv2D(128, (3,3), padding='same', activation=\"relu\"))(down4)\n",
    "    # 7 x 7 x 256\n",
    "    if maxpool:\n",
    "        down5 = TimeDistributed(MaxPooling2D(pool_size=(2, 2)))(conv5)\n",
    "    else:\n",
    "        down5 = TimeDistributed(Conv2D(128, kernel_size=(2,2), \n",
    "                                      strides=(2,2), \n",
    "                                      padding='same', \n",
    "                                      activation=None))(conv5)\n",
    "    return down5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ctc_lambda_func(args):\n",
    "#     y_pred, labels, input_length, label_length = args\n",
    "#     # the 2 is critical here since the first couple outputs of the RNN\n",
    "#     # tend to be garbage:\n",
    "#     y_pred = y_pred[:, 2:, :]\n",
    "#     return keras.backend.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
    "\n",
    "def ctc_lambda_func(args):\n",
    "    base_output, labels, label_length = args \n",
    "    base_output_shape = tf.shape(base_output)\n",
    "    sequence_length = tf.fill([base_output_shape[0], 1], base_output_shape[1])\n",
    "    print(labels)\n",
    "    print(base_output)\n",
    "    print(sequence_length)\n",
    "    print(label_length)\n",
    "    \n",
    "    return keras.backend.ctc_batch_cost(labels, base_output, sequence_length, label_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_crossentropy_func(args):  \n",
    "    target, output = args\n",
    "    print(target)\n",
    "#     target_dense = keras.utils.to_categorical(\n",
    "#                         target,\n",
    "#                         num_classes=n_classes+1\n",
    "#                     )\n",
    "#     # Compute cross entropy for each frame.\n",
    "#     cross_entropy = target_dense * tf.log(output)\n",
    "#     cross_entropy = -tf.reduce_sum(cross_entropy, 2)\n",
    "    cross_entropy = keras.backend.sparse_categorical_crossentropy(\n",
    "                        target,\n",
    "                        output,\n",
    "                        from_logits=False,\n",
    "                        axis=-1\n",
    "                    )\n",
    "    print(cross_entropy)\n",
    "    mask = tf.cast(target < n_classes, dtype=tf.float32)\n",
    "    print(mask)\n",
    "    cross_entropy *= mask\n",
    "    # Average over actual sequence lengths.\n",
    "    cross_entropy = tf.reduce_sum(cross_entropy, 1)\n",
    "    cross_entropy /= tf.reduce_sum(mask, 1)\n",
    "    return cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"time_distributed_143/Reshape_1:0\", shape=(?, 100, 7, 7, 128), dtype=float32)\n",
      "Tensor(\"time_distributed_145/Reshape_1:0\", shape=(?, 100, 128), dtype=float32)\n",
      "Tensor(\"labels_12:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"output_sequence_10/Reshape_1:0\", shape=(?, 100, 40), dtype=float32)\n",
      "Tensor(\"ctc_10/Fill:0\", shape=(?, 1), dtype=int32)\n",
      "Tensor(\"label_length_12:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"labels_12:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"masked_ce_2/Reshape_2:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"masked_ce_2/Cast_1:0\", shape=(?, 100), dtype=float32)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "V (InputLayer)                  (None, 100, 224, 224 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "F (InputLayer)                  (None, 100, 224, 224 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_57 (Concatenate)    (None, 100, 224, 224 0           V[0][0]                          \n",
      "                                                                 F[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_134 (TimeDistr (None, 100, 224, 224 6656        concatenate_57[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_135 (TimeDistr (None, 100, 112, 112 0           time_distributed_134[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_136 (TimeDistr (None, 100, 112, 112 18496       time_distributed_135[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_137 (TimeDistr (None, 100, 56, 56,  0           time_distributed_136[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_138 (TimeDistr (None, 100, 56, 56,  73856       time_distributed_137[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_139 (TimeDistr (None, 100, 28, 28,  0           time_distributed_138[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_140 (TimeDistr (None, 100, 28, 28,  147584      time_distributed_139[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_141 (TimeDistr (None, 100, 14, 14,  0           time_distributed_140[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_142 (TimeDistr (None, 100, 14, 14,  147584      time_distributed_141[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_143 (TimeDistr (None, 100, 7, 7, 12 0           time_distributed_142[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_144 (TimeDistr (None, 100, 6272)    0           time_distributed_143[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_145 (TimeDistr (None, 100, 128)     802944      time_distributed_144[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "model_226 (Model)               (None, 100, 40)      400681      time_distributed_145[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "output_sequence (TimeDistribute (None, 100, 40)      1640        model_226[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "labels (InputLayer)             (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "label_length (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ctc (Lambda)                    (None, 1)            0           output_sequence[0][0]            \n",
      "                                                                 labels[0][0]                     \n",
      "                                                                 label_length[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,599,441\n",
      "Trainable params: 1,599,441\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "# Baseline Model #\n",
    "##################\n",
    "input_V_tensor = Input(shape=video_tensor_size, name=\"V\")\n",
    "input_F_tensor = Input(shape=(video_tensor_size[0], \n",
    "                              video_tensor_size[1], \n",
    "                              video_tensor_size[2], \n",
    "                              keypoint_size), name=\"F\")\n",
    "\n",
    "labels = Input(shape=(label_seq_size,), name=\"labels\")\n",
    "label_length = Input(shape=(1,), name=\"label_length\")\n",
    "\n",
    "# 224 x 224 x 23\n",
    "input_tensor = Concatenate(axis=-1)([input_V_tensor, input_F_tensor])\n",
    "\n",
    "conv_output_tensor = visual_conv_net(input_tensor)\n",
    "\n",
    "# fc_out = TimeDistributed(GlobalAveragePooling2D())(conv_output_tensor)\n",
    "fc_in = TimeDistributed(Flatten())(conv_output_tensor)\n",
    "fc_out = TimeDistributed(Dense(128, activation=\"relu\"))(fc_in)\n",
    "\n",
    "print(conv_output_tensor)\n",
    "print(fc_out)\n",
    "\n",
    "att_seq2seq = AttentionSeq2Seq(input_dim=128, input_length=video_tensor_size[0], \n",
    "                         hidden_dim=128, \n",
    "                         output_length=label_seq_size, \n",
    "                         output_dim=n_classes+1,\n",
    "                         depth=1, dropout=0.3)\n",
    "decoded = att_seq2seq(fc_out)\n",
    "\n",
    "# # LSTM Encoder\n",
    "# encoder_lstm = LSTM(128, return_sequences=True, return_state=True)\n",
    "# encoder_outputs, state_h, state_c = encoder_lstm(fc_out)\n",
    "# encoder_states = [state_h, state_c]\n",
    "\n",
    "# # Sequence Placeholder\n",
    "# # decoder_inputs = Input(shape=(None, n_classes+2))\n",
    "\n",
    "# # LSTM Decoder\n",
    "# decoder_lstm = LSTM(128, return_sequences=True, return_state=True)\n",
    "# decoded, _, _ = decoder_lstm(encoder_outputs, initial_state=encoder_states)\n",
    "\n",
    "decoder_outputs = TimeDistributed(Dense(n_classes+1, activation='softmax'), name='output_sequence')(decoded)\n",
    "\n",
    "# decoder_outputs = Lambda(lambda x:x, name='output_sequence')(decoded)\n",
    "\n",
    "loss = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([decoder_outputs, labels, label_length])\n",
    "\n",
    "ce_loss = Lambda(masked_crossentropy_func, output_shape=(1,), name='masked_ce')([labels, decoder_outputs])\n",
    "model = Model(inputs=[input_V_tensor, input_F_tensor, labels, label_length], outputs=[loss, decoder_outputs])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./sessions/seq2seq_ctc\"\n",
    "checkpoints_path = os.path.join(path, 'checkpoints')\n",
    "history_filename = 'history_' + path[path.rindex('/') + 1:] + '.csv'\n",
    "early_stopping_patience = 10\n",
    "\n",
    "if not os.path.exists(\"./sessions\"):\n",
    "    os.mkdir(\"./sessions\")\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)\n",
    "\n",
    "if not os.path.exists(checkpoints_path):\n",
    "    os.mkdir(checkpoints_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('./sessions/seq2seq_ctc/checkpoints/checkpoint.00003-57.170.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(y_true, y_pred):\n",
    "    return y_pred\n",
    "\n",
    "def dummy_loss_func(y_true, y_pred):\n",
    "    return tf.fill([tf.shape(y_true)[0], 1], 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ctc_loss(y_true, y_pred):\n",
    "#     y_true_sparse = tf.argmax(y_true, axis=-1)\n",
    "#     y_true_valid = tf.cast(y_true_sparse < n_classes, tf.float32)\n",
    "#     print(y_true_valid)\n",
    "#     print(y_pred)\n",
    "#     label_length = tf.reduce_sum(y_true_valid, axis=-1, keepdims=True)\n",
    "#     print(label_length)\n",
    "#     input_length = label_seq_size * tf.ones((tf.shape(y_pred)[0], 1))\n",
    "#     print(input_length)\n",
    "#     loss_tensor = tf.keras.backend.ctc_batch_cost(\n",
    "#                         y_true_sparse,\n",
    "#                         y_pred,\n",
    "#                         input_length,\n",
    "#                         label_length\n",
    "#                     )\n",
    "#     return tf.reduce_mean(loss_tensor)\n",
    "\n",
    "# def ctc_loss_2(y_true, y_pred):\n",
    "#     sparse = tf.contrib.layers.dense_to_sparse(y_true)\n",
    "#     input_length = label_seq_size * tf.ones((tf.shape(y_pred)[0], 1))\n",
    "#     y_true_valid = tf.cast(y_true_sparse<n_classes, tf.float32)\n",
    "#     label_length = tf.reduce_sum(y_true_valid, axis=-1, keepdims=True)\n",
    "#     loss_tensor = tf.nn.ctc_loss(\n",
    "#             sparse,\n",
    "#             y_pred,\n",
    "#             label_length,\n",
    "#             preprocess_collapse_repeated=False,\n",
    "#             ctc_merge_repeated=True,\n",
    "#             ignore_longer_outputs_than_inputs=True,\n",
    "#             time_major=False\n",
    "#         )\n",
    "#     return tf.reduce_mean(loss_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_edit_distance(truth, hyp):\n",
    "    truth = tf.reshape(truth, (-1, label_seq_size))\n",
    "    truth = tf.cast(truth, dtype=tf.int64)\n",
    "    truth_idx = tf.where(tf.not_equal(truth, n_classes))\n",
    "    # Use tf.shape(a_t, out_type=tf.int64) instead of a_t.get_shape() if tensor shape is dynamic\n",
    "    truth_sparse = tf.SparseTensor(truth_idx, tf.gather_nd(truth, truth_idx), tf.shape(truth, out_type=tf.int64))\n",
    "    \n",
    "    hyp_dense = tf.argmax(hyp, axis=-1)\n",
    "    hyp_idx = tf.where(tf.not_equal(hyp_dense, n_classes))\n",
    "    # Use tf.shape(a_t, out_type=tf.int64) instead of a_t.get_shape() if tensor shape is dynamic\n",
    "    hyp_sparse = tf.SparseTensor(hyp_idx, tf.gather_nd(hyp_dense, hyp_idx), tf.shape(hyp_dense, out_type=tf.int64))\n",
    "#     tf.contrib.layers.dense_to_sparse(\n",
    "#     tensor,\n",
    "#     eos_token=0,\n",
    "#     outputs_collections=None,\n",
    "#     scope=None\n",
    "# )\n",
    "    print(truth, hyp_dense)\n",
    "    editDist = tf.edit_distance(hyp_sparse, truth_sparse, normalize=True)\n",
    "    return editDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_accuracy(y_true, y_pred):\n",
    "    y_pred_sparse = tf.argmax(y_pred, axis=-1)\n",
    "    return tf.reduce_mean(tf.cast(y_pred_sparse==y_true, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor 'metrics_21/test_edit_distance/Cast:0' shape=(?, 100) dtype=int64>, <tf.Tensor 'metrics_21/test_edit_distance/ArgMax:0' shape=(?, 100) dtype=int64>)\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "# model.compile(optimizer=Adam(lr=0.001), \n",
    "#               loss='categorical_crossentropy', metrics=[ctc_loss])\n",
    "\n",
    "model.compile(optimizer=Adam(lr=0.001), loss={'ctc': loss_func, \n",
    "                                              'output_sequence':'sparse_categorical_crossentropy'},\n",
    "                                          loss_weights={'ctc': 1.0, 'output_sequence':0.0},\n",
    "                                          metrics={'output_sequence':sparse_accuracy, 'output_sequence':test_edit_distance})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks():\n",
    "    return [\n",
    "        keras.callbacks.ReduceLROnPlateau(patience=early_stopping_patience / 2,\n",
    "                                              cooldown=early_stopping_patience / 4,\n",
    "                                              verbose=1),\n",
    "        keras.callbacks.EarlyStopping(patience=early_stopping_patience, verbose=1,\n",
    "                                          monitor='val_loss'),\n",
    "        keras.callbacks.ModelCheckpoint(os.path.join(checkpoints_path, 'checkpoint.{epoch:05d}-{val_loss:.3f}.hdf5')),\n",
    "        keras.callbacks.CSVLogger(os.path.join(path, history_filename), append=True)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "400/400 [==============================] - 1161s 3s/step - loss: 62.2005 - ctc_loss: 62.2005 - output_sequence_loss: 1.0283 - output_sequence_test_edit_distance: 0.9940 - val_loss: 58.0722 - val_ctc_loss: 58.0722 - val_output_sequence_loss: 0.9752 - val_output_sequence_test_edit_distance: 1.0000\n",
      "Epoch 2/100\n",
      "400/400 [==============================] - 1141s 3s/step - loss: 61.0578 - ctc_loss: 61.0578 - output_sequence_loss: 1.0149 - output_sequence_test_edit_distance: 0.9746 - val_loss: 57.0247 - val_ctc_loss: 57.0247 - val_output_sequence_loss: 0.9589 - val_output_sequence_test_edit_distance: 0.9728\n",
      "Epoch 3/100\n",
      "400/400 [==============================] - 1146s 3s/step - loss: 60.8119 - ctc_loss: 60.8119 - output_sequence_loss: 1.0104 - output_sequence_test_edit_distance: 0.9722 - val_loss: 57.1481 - val_ctc_loss: 57.1481 - val_output_sequence_loss: 0.9497 - val_output_sequence_test_edit_distance: 0.9764\n",
      "Epoch 4/100\n",
      "400/400 [==============================] - 1149s 3s/step - loss: 62.6504 - ctc_loss: 62.6504 - output_sequence_loss: 1.0310 - output_sequence_test_edit_distance: 0.9720 - val_loss: 55.6284 - val_ctc_loss: 55.6284 - val_output_sequence_loss: 0.9230 - val_output_sequence_test_edit_distance: 0.9669\n",
      "Epoch 5/100\n",
      "400/400 [==============================] - 1154s 3s/step - loss: 60.7377 - ctc_loss: 60.7377 - output_sequence_loss: 0.9980 - output_sequence_test_edit_distance: 0.9711 - val_loss: 57.7816 - val_ctc_loss: 57.7816 - val_output_sequence_loss: 0.9505 - val_output_sequence_test_edit_distance: 0.9736\n",
      "Epoch 6/100\n",
      "400/400 [==============================] - 1150s 3s/step - loss: 60.9839 - ctc_loss: 60.9839 - output_sequence_loss: 0.9997 - output_sequence_test_edit_distance: 0.9709 - val_loss: 57.4878 - val_ctc_loss: 57.4878 - val_output_sequence_loss: 0.9491 - val_output_sequence_test_edit_distance: 0.9715\n",
      "Epoch 7/100\n",
      "400/400 [==============================] - 1160s 3s/step - loss: 61.9494 - ctc_loss: 61.9494 - output_sequence_loss: 1.0103 - output_sequence_test_edit_distance: 0.9735 - val_loss: 57.0633 - val_ctc_loss: 57.0633 - val_output_sequence_loss: 0.9302 - val_output_sequence_test_edit_distance: 0.9688\n",
      "Epoch 8/100\n",
      "400/400 [==============================] - 1159s 3s/step - loss: 61.5313 - ctc_loss: 61.5313 - output_sequence_loss: 0.9996 - output_sequence_test_edit_distance: 0.9720 - val_loss: 56.3194 - val_ctc_loss: 56.3194 - val_output_sequence_loss: 0.9200 - val_output_sequence_test_edit_distance: 0.9787\n",
      "Epoch 9/100\n",
      "400/400 [==============================] - 1148s 3s/step - loss: 62.0293 - ctc_loss: 62.0293 - output_sequence_loss: 1.0048 - output_sequence_test_edit_distance: 0.9509 - val_loss: 57.0562 - val_ctc_loss: 57.0562 - val_output_sequence_loss: 0.9295 - val_output_sequence_test_edit_distance: 0.9657\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00010000000475.\n",
      "Epoch 10/100\n",
      "400/400 [==============================] - 1149s 3s/step - loss: 60.4945 - ctc_loss: 60.4945 - output_sequence_loss: 0.9808 - output_sequence_test_edit_distance: 0.9732 - val_loss: 56.4070 - val_ctc_loss: 56.4070 - val_output_sequence_loss: 0.9193 - val_output_sequence_test_edit_distance: 0.9667\n",
      "Epoch 11/100\n",
      "400/400 [==============================] - 1157s 3s/step - loss: 60.3695 - ctc_loss: 60.3695 - output_sequence_loss: 0.9723 - output_sequence_test_edit_distance: 0.9689 - val_loss: 57.5835 - val_ctc_loss: 57.5835 - val_output_sequence_loss: 0.9333 - val_output_sequence_test_edit_distance: 0.9685\n",
      "Epoch 12/100\n",
      "400/400 [==============================] - 1158s 3s/step - loss: 60.8643 - ctc_loss: 60.8643 - output_sequence_loss: 0.9798 - output_sequence_test_edit_distance: 0.9742 - val_loss: 56.5850 - val_ctc_loss: 56.5850 - val_output_sequence_loss: 0.9087 - val_output_sequence_test_edit_distance: 0.9696\n",
      "Epoch 13/100\n",
      "400/400 [==============================] - 1156s 3s/step - loss: 61.1217 - ctc_loss: 61.1217 - output_sequence_loss: 0.9760 - output_sequence_test_edit_distance: 0.9670 - val_loss: 54.7543 - val_ctc_loss: 54.7543 - val_output_sequence_loss: 0.8794 - val_output_sequence_test_edit_distance: 0.9695\n",
      "Epoch 14/100\n",
      "400/400 [==============================] - 1154s 3s/step - loss: 60.2103 - ctc_loss: 60.2103 - output_sequence_loss: 0.9603 - output_sequence_test_edit_distance: 0.9679 - val_loss: 55.7344 - val_ctc_loss: 55.7344 - val_output_sequence_loss: 0.8911 - val_output_sequence_test_edit_distance: 0.9687\n",
      "Epoch 15/100\n",
      "400/400 [==============================] - 1156s 3s/step - loss: 62.4118 - ctc_loss: 62.4118 - output_sequence_loss: 0.9915 - output_sequence_test_edit_distance: 0.9679 - val_loss: 56.7055 - val_ctc_loss: 56.7055 - val_output_sequence_loss: 0.9079 - val_output_sequence_test_edit_distance: 0.9640\n",
      "Epoch 16/100\n",
      "400/400 [==============================] - 1166s 3s/step - loss: 59.2705 - ctc_loss: 59.2705 - output_sequence_loss: 0.9437 - output_sequence_test_edit_distance: 0.9572 - val_loss: 57.1427 - val_ctc_loss: 57.1427 - val_output_sequence_loss: 0.9175 - val_output_sequence_test_edit_distance: 0.9663\n",
      "Epoch 17/100\n",
      "400/400 [==============================] - 1162s 3s/step - loss: 61.2403 - ctc_loss: 61.2403 - output_sequence_loss: 0.9775 - output_sequence_test_edit_distance: 0.9519 - val_loss: 53.4297 - val_ctc_loss: 53.4297 - val_output_sequence_loss: 0.8622 - val_output_sequence_test_edit_distance: 0.9668\n",
      "Epoch 18/100\n",
      "400/400 [==============================] - 1172s 3s/step - loss: 59.5349 - ctc_loss: 59.5349 - output_sequence_loss: 0.9563 - output_sequence_test_edit_distance: 0.9384 - val_loss: 57.1360 - val_ctc_loss: 57.1360 - val_output_sequence_loss: 0.9090 - val_output_sequence_test_edit_distance: 0.9300\n",
      "Epoch 19/100\n",
      "400/400 [==============================] - 1173s 3s/step - loss: 61.3102 - ctc_loss: 61.3102 - output_sequence_loss: 0.9710 - output_sequence_test_edit_distance: 0.9279 - val_loss: 59.3564 - val_ctc_loss: 59.3564 - val_output_sequence_loss: 0.9474 - val_output_sequence_test_edit_distance: 0.9274\n",
      "Epoch 20/100\n",
      "400/400 [==============================] - 1169s 3s/step - loss: 60.2976 - ctc_loss: 60.2976 - output_sequence_loss: 0.9606 - output_sequence_test_edit_distance: 0.9294 - val_loss: 54.3129 - val_ctc_loss: 54.3129 - val_output_sequence_loss: 0.8692 - val_output_sequence_test_edit_distance: 0.9238\n",
      "Epoch 21/100\n",
      "400/400 [==============================] - 1136s 3s/step - loss: 60.4843 - ctc_loss: 60.4843 - output_sequence_loss: 0.9606 - output_sequence_test_edit_distance: 0.9265 - val_loss: 57.1755 - val_ctc_loss: 57.1755 - val_output_sequence_loss: 0.9073 - val_output_sequence_test_edit_distance: 0.9304\n",
      "Epoch 22/100\n",
      "400/400 [==============================] - 1028s 3s/step - loss: 61.7491 - ctc_loss: 61.7491 - output_sequence_loss: 0.9789 - output_sequence_test_edit_distance: 0.9293 - val_loss: 57.6392 - val_ctc_loss: 57.6392 - val_output_sequence_loss: 0.9249 - val_output_sequence_test_edit_distance: 0.9272\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 23/100\n",
      "400/400 [==============================] - 1029s 3s/step - loss: 61.9562 - ctc_loss: 61.9562 - output_sequence_loss: 0.9835 - output_sequence_test_edit_distance: 0.9289 - val_loss: 56.9586 - val_ctc_loss: 56.9586 - val_output_sequence_loss: 0.9129 - val_output_sequence_test_edit_distance: 0.9280\n",
      "Epoch 24/100\n",
      "400/400 [==============================] - 1022s 3s/step - loss: 60.0138 - ctc_loss: 60.0138 - output_sequence_loss: 0.9537 - output_sequence_test_edit_distance: 0.9274 - val_loss: 55.0286 - val_ctc_loss: 55.0286 - val_output_sequence_loss: 0.8879 - val_output_sequence_test_edit_distance: 0.9233\n",
      "Epoch 25/100\n",
      "400/400 [==============================] - 1028s 3s/step - loss: 60.2005 - ctc_loss: 60.2005 - output_sequence_loss: 0.9585 - output_sequence_test_edit_distance: 0.9261 - val_loss: 56.7571 - val_ctc_loss: 56.7571 - val_output_sequence_loss: 0.9048 - val_output_sequence_test_edit_distance: 0.9299\n",
      "Epoch 26/100\n",
      "400/400 [==============================] - 1036s 3s/step - loss: 61.3607 - ctc_loss: 61.3607 - output_sequence_loss: 0.9766 - output_sequence_test_edit_distance: 0.9264 - val_loss: 55.8532 - val_ctc_loss: 55.8532 - val_output_sequence_loss: 0.8933 - val_output_sequence_test_edit_distance: 0.9177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/100\n",
      "400/400 [==============================] - 1033s 3s/step - loss: 60.3361 - ctc_loss: 60.3361 - output_sequence_loss: 0.9598 - output_sequence_test_edit_distance: 0.9249 - val_loss: 52.7058 - val_ctc_loss: 52.7058 - val_output_sequence_loss: 0.8444 - val_output_sequence_test_edit_distance: 0.9249\n",
      "Epoch 28/100\n",
      "400/400 [==============================] - 1034s 3s/step - loss: 61.6264 - ctc_loss: 61.6264 - output_sequence_loss: 0.9783 - output_sequence_test_edit_distance: 0.9318 - val_loss: 59.8629 - val_ctc_loss: 59.8629 - val_output_sequence_loss: 0.9496 - val_output_sequence_test_edit_distance: 0.9288\n",
      "Epoch 29/100\n",
      "400/400 [==============================] - 1033s 3s/step - loss: 61.2051 - ctc_loss: 61.2051 - output_sequence_loss: 0.9705 - output_sequence_test_edit_distance: 0.9256 - val_loss: 56.4582 - val_ctc_loss: 56.4582 - val_output_sequence_loss: 0.9016 - val_output_sequence_test_edit_distance: 0.9290\n",
      "Epoch 30/100\n",
      "400/400 [==============================] - 958s 2s/step - loss: 60.6749 - ctc_loss: 60.6749 - output_sequence_loss: 0.9630 - output_sequence_test_edit_distance: 0.9257 - val_loss: 57.9209 - val_ctc_loss: 57.9209 - val_output_sequence_loss: 0.9236 - val_output_sequence_test_edit_distance: 0.9263\n",
      "Epoch 31/100\n",
      "400/400 [==============================] - 929s 2s/step - loss: 60.4406 - ctc_loss: 60.4406 - output_sequence_loss: 0.9633 - output_sequence_test_edit_distance: 0.9261 - val_loss: 56.0611 - val_ctc_loss: 56.0611 - val_output_sequence_loss: 0.8973 - val_output_sequence_test_edit_distance: 0.9258\n",
      "Epoch 32/100\n",
      "400/400 [==============================] - 931s 2s/step - loss: 58.7222 - ctc_loss: 58.7222 - output_sequence_loss: 0.9363 - output_sequence_test_edit_distance: 0.9267 - val_loss: 57.0399 - val_ctc_loss: 57.0399 - val_output_sequence_loss: 0.9137 - val_output_sequence_test_edit_distance: 0.9270\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 33/100\n",
      "400/400 [==============================] - 913s 2s/step - loss: 60.3188 - ctc_loss: 60.3188 - output_sequence_loss: 0.9594 - output_sequence_test_edit_distance: 0.9317 - val_loss: 54.1698 - val_ctc_loss: 54.1698 - val_output_sequence_loss: 0.8686 - val_output_sequence_test_edit_distance: 0.9230\n",
      "Epoch 34/100\n",
      "400/400 [==============================] - 899s 2s/step - loss: 61.3699 - ctc_loss: 61.3699 - output_sequence_loss: 0.9780 - output_sequence_test_edit_distance: 0.9278 - val_loss: 56.5650 - val_ctc_loss: 56.5650 - val_output_sequence_loss: 0.9067 - val_output_sequence_test_edit_distance: 0.9252\n",
      "Epoch 35/100\n",
      "400/400 [==============================] - 923s 2s/step - loss: 60.3492 - ctc_loss: 60.3492 - output_sequence_loss: 0.9608 - output_sequence_test_edit_distance: 0.9266 - val_loss: 58.7610 - val_ctc_loss: 58.7610 - val_output_sequence_loss: 0.9375 - val_output_sequence_test_edit_distance: 0.9247\n",
      "Epoch 36/100\n",
      "400/400 [==============================] - 927s 2s/step - loss: 60.1742 - ctc_loss: 60.1742 - output_sequence_loss: 0.9575 - output_sequence_test_edit_distance: 0.9282 - val_loss: 55.9247 - val_ctc_loss: 55.9247 - val_output_sequence_loss: 0.8938 - val_output_sequence_test_edit_distance: 0.9287\n",
      "Epoch 37/100\n",
      "400/400 [==============================] - 919s 2s/step - loss: 60.8237 - ctc_loss: 60.8237 - output_sequence_loss: 0.9684 - output_sequence_test_edit_distance: 0.9255 - val_loss: 58.3482 - val_ctc_loss: 58.3482 - val_output_sequence_loss: 0.9252 - val_output_sequence_test_edit_distance: 0.9305\n",
      "Epoch 00037: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe6a79a8910>"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, \n",
    "          epochs=100, \n",
    "          verbose=1, \n",
    "          callbacks=get_callbacks(), \n",
    "          validation_data=val_generator, \n",
    "          shuffle=True, \n",
    "          initial_epoch=0, \n",
    "          steps_per_epoch=400, \n",
    "          validation_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-audio-3.6",
   "language": "python",
   "name": "conda-audio-3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
